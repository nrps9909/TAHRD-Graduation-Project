import { logger } from './logger'

interface PerformanceMetrics {
  requestId: string
  userId: string
  npcId: string
  userInput: number
  socketReceived: number
  dbQueryStart: number
  dbQueryEnd: number
  aiCallStart: number
  aiCallEnd: number
  responseSent: number
  totalTime?: number
  aiTime?: number
  dbTime?: number
}

/**
 * æ•ˆèƒ½ç›£æŽ§å·¥å…·
 */
export class PerformanceMonitor {
  private metrics: Map<string, PerformanceMetrics> = new Map()
  private readonly SLOW_THRESHOLD = 2000 // 2ç§’ç‚ºæ…¢æŸ¥è©¢
  
  /**
   * é–‹å§‹è¿½è¹¤æ–°è«‹æ±‚
   */
  startTracking(requestId: string, userId: string, npcId: string): PerformanceMetrics {
    const metrics: PerformanceMetrics = {
      requestId,
      userId,
      npcId,
      userInput: Date.now(),
      socketReceived: 0,
      dbQueryStart: 0,
      dbQueryEnd: 0,
      aiCallStart: 0,
      aiCallEnd: 0,
      responseSent: 0
    }
    
    this.metrics.set(requestId, metrics)
    return metrics
  }
  
  /**
   * æ¨™è¨˜æ™‚é–“é»ž
   */
  mark(requestId: string, event: keyof PerformanceMetrics) {
    const metrics = this.metrics.get(requestId)
    if (metrics) {
      metrics[event] = Date.now() as any
    }
  }
  
  /**
   * å®Œæˆè¿½è¹¤ä¸¦è¨˜éŒ„
   */
  endTracking(requestId: string) {
    const metrics = this.metrics.get(requestId)
    if (!metrics) return
    
    // è¨ˆç®—å„éšŽæ®µè€—æ™‚
    metrics.totalTime = metrics.responseSent - metrics.userInput
    metrics.aiTime = metrics.aiCallEnd - metrics.aiCallStart
    metrics.dbTime = metrics.dbQueryEnd - metrics.dbQueryStart
    
    // è¨˜éŒ„æ•ˆèƒ½è³‡æ–™
    const logData = {
      requestId,
      npcId: metrics.npcId,
      totalTime: metrics.totalTime,
      aiTime: metrics.aiTime,
      dbTime: metrics.dbTime,
      socketLatency: metrics.socketReceived - metrics.userInput,
      processingTime: metrics.responseSent - metrics.socketReceived
    }
    
    // æª¢æŸ¥æ˜¯å¦ç‚ºæ…¢æŸ¥è©¢
    if (metrics.totalTime > this.SLOW_THRESHOLD) {
      logger.warn('âš ï¸ æ…¢æŸ¥è©¢æª¢æ¸¬', logData)
      this.analyzeBottleneck(metrics)
    } else {
      logger.info('âœ… æ•ˆèƒ½æŒ‡æ¨™', logData)
    }
    
    // æ¸…ç†è¨˜æ†¶é«”
    this.metrics.delete(requestId)
    
    return metrics
  }
  
  /**
   * åˆ†æžç“¶é ¸
   */
  private analyzeBottleneck(metrics: PerformanceMetrics) {
    const bottlenecks = []
    
    if (metrics.aiTime! > 1500) {
      bottlenecks.push(`AI å›žæ‡‰éŽæ…¢: ${metrics.aiTime}ms`)
    }
    
    if (metrics.dbTime! > 500) {
      bottlenecks.push(`è³‡æ–™åº«æŸ¥è©¢éŽæ…¢: ${metrics.dbTime}ms`)
    }
    
    const socketLatency = metrics.socketReceived - metrics.userInput
    if (socketLatency > 100) {
      bottlenecks.push(`Socket å»¶é²éŽé«˜: ${socketLatency}ms`)
    }
    
    if (bottlenecks.length > 0) {
      logger.error('ðŸ”´ æ•ˆèƒ½ç“¶é ¸åˆ†æž', {
        requestId: metrics.requestId,
        bottlenecks,
        suggestion: this.getSuggestion(metrics)
      })
    }
  }
  
  /**
   * æä¾›å„ªåŒ–å»ºè­°
   */
  private getSuggestion(metrics: PerformanceMetrics): string[] {
    const suggestions = []
    
    if (metrics.aiTime! > 1500) {
      suggestions.push('è€ƒæ…®ä½¿ç”¨å¿«å–æˆ–æ›´å¿«çš„ AI æ¨¡åž‹')
    }
    
    if (metrics.dbTime! > 500) {
      suggestions.push('å„ªåŒ–è³‡æ–™åº«æŸ¥è©¢æˆ–æ·»åŠ ç´¢å¼•')
    }
    
    return suggestions
  }
  
  /**
   * ç²å–çµ±è¨ˆè³‡æ–™
   */
  getStats() {
    const activeRequests = this.metrics.size
    const avgMetrics = this.calculateAverages()
    
    return {
      activeRequests,
      averageResponseTime: avgMetrics.avgTotal,
      averageAITime: avgMetrics.avgAI,
      averageDBTime: avgMetrics.avgDB
    }
  }
  
  private calculateAverages() {
    const completed = Array.from(this.metrics.values()).filter(m => m.responseSent > 0)
    
    if (completed.length === 0) {
      return { avgTotal: 0, avgAI: 0, avgDB: 0 }
    }
    
    const sum = completed.reduce((acc, m) => ({
      total: acc.total + (m.totalTime || 0),
      ai: acc.ai + (m.aiTime || 0),
      db: acc.db + (m.dbTime || 0)
    }), { total: 0, ai: 0, db: 0 })
    
    return {
      avgTotal: Math.round(sum.total / completed.length),
      avgAI: Math.round(sum.ai / completed.length),
      avgDB: Math.round(sum.db / completed.length)
    }
  }
}

// å–®ä¾‹æ¨¡å¼
export const performanceMonitor = new PerformanceMonitor()

// Express ä¸­é–“ä»¶
export function performanceMiddleware() {
  return (req: any, res: any, next: any) => {
    const requestId = `req_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`
    req.perfId = requestId
    
    // è¨˜éŒ„è«‹æ±‚é–‹å§‹æ™‚é–“
    const startTime = Date.now()
    
    // æ””æˆª response
    const originalSend = res.send
    res.send = function(data: any) {
      const duration = Date.now() - startTime
      
      // æ·»åŠ æ•ˆèƒ½æ¨™é ­
      res.setHeader('X-Response-Time', `${duration}ms`)
      res.setHeader('X-Request-ID', requestId)
      
      // è¨˜éŒ„æ…¢è«‹æ±‚
      if (duration > 1000) {
        logger.warn(`Slow HTTP request: ${req.method} ${req.path} - ${duration}ms`)
      }
      
      return originalSend.call(this, data)
    }
    
    next()
  }
}