```python
!export CPATH=/trinity/home/tna001/python39/include
!export CFLAGS="-I/trinity/home/tna001/python39/include/python3.9"

import os
os.environ["CPATH"] = "/trinity/home/tna001/python39/include/python3.9"
os.environ["C_INCLUDE_PATH"] = "/trinity/home/tna001/python39/include/python3.9"
os.environ["CFLAGS"] = "-I/trinity/home/tna001/python39/include/python3.9"

# æª¢æŸ¥è¨­å®šæ˜¯å¦æ­£ç¢ºï¼ˆå¯é¸ï¼‰
!echo $CPATH
!echo $C_INCLUDE_PATH
!echo $CFLAGS

```

    /trinity/home/tna001/python39/include/python3.9
    /trinity/home/tna001/python39/include/python3.9
    -I/trinity/home/tna001/python39/include/python3.9



```python
!pip install --upgrade pip
!pip install --upgrade pybind11 accelerate transformers peft datasets sentencepiece bitsandbytes faiss-gpu torch sentence-transformers ipywidgets wandb numpy==1.26.4
```

    Defaulting to user installation because normal site-packages is not writeable
    Requirement already satisfied: pip in ./.local/lib/python3.9/site-packages (25.0.1)
    Defaulting to user installation because normal site-packages is not writeable
    Requirement already satisfied: pybind11 in ./.local/lib/python3.9/site-packages (2.13.6)
    Requirement already satisfied: accelerate in ./.local/lib/python3.9/site-packages (1.3.0)
    Requirement already satisfied: transformers in ./.local/lib/python3.9/site-packages (4.48.3)
    Requirement already satisfied: peft in ./.local/lib/python3.9/site-packages (0.14.0)
    Requirement already satisfied: datasets in ./.local/lib/python3.9/site-packages (3.2.0)
    Requirement already satisfied: sentencepiece in ./.local/lib/python3.9/site-packages (0.2.0)
    Requirement already satisfied: bitsandbytes in ./.local/lib/python3.9/site-packages (0.45.2)
    Requirement already satisfied: faiss-gpu in ./.local/lib/python3.9/site-packages (1.7.2)
    Requirement already satisfied: torch in ./.local/lib/python3.9/site-packages (2.6.0)
    Requirement already satisfied: sentence-transformers in ./.local/lib/python3.9/site-packages (3.4.1)
    Requirement already satisfied: ipywidgets in ./.local/lib/python3.9/site-packages (8.1.5)
    Requirement already satisfied: wandb in ./.local/lib/python3.9/site-packages (0.19.6)
    Requirement already satisfied: numpy==1.26.4 in ./.local/lib/python3.9/site-packages (1.26.4)
    Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.9/site-packages (from accelerate) (24.2)
    Requirement already satisfied: psutil in /usr/local/lib64/python3.9/site-packages (from accelerate) (6.1.0)
    Requirement already satisfied: pyyaml in /usr/lib64/python3.9/site-packages (from accelerate) (5.4.1)
    Requirement already satisfied: huggingface-hub>=0.21.0 in ./.local/lib/python3.9/site-packages (from accelerate) (0.27.1)
    Requirement already satisfied: safetensors>=0.4.3 in ./.local/lib/python3.9/site-packages (from accelerate) (0.5.2)
    Requirement already satisfied: filelock in ./.local/lib/python3.9/site-packages (from transformers) (3.16.1)
    Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.9/site-packages (from transformers) (2024.11.6)
    Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers) (2.32.3)
    Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.local/lib/python3.9/site-packages (from transformers) (0.21.0)
    Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers) (4.66.6)
    Requirement already satisfied: pyarrow>=15.0.0 in ./.local/lib/python3.9/site-packages (from datasets) (18.1.0)
    Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./.local/lib/python3.9/site-packages (from datasets) (0.3.8)
    Requirement already satisfied: pandas in ./.local/lib/python3.9/site-packages (from datasets) (2.2.3)
    Requirement already satisfied: xxhash in ./.local/lib/python3.9/site-packages (from datasets) (3.5.0)
    Requirement already satisfied: multiprocess<0.70.17 in ./.local/lib/python3.9/site-packages (from datasets) (0.70.16)
    Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in ./.local/lib/python3.9/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)
    Requirement already satisfied: aiohttp in ./.local/lib/python3.9/site-packages (from datasets) (3.11.12)
    Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.9/site-packages (from torch) (4.12.2)
    Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch) (3.2.1)
    Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch) (3.1.4)
    Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch) (12.4.127)
    Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch) (12.4.127)
    Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch) (12.4.127)
    Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.9/site-packages (from torch) (9.1.0.70)
    Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.9/site-packages (from torch) (12.4.5.8)
    Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.9/site-packages (from torch) (11.2.1.3)
    Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.9/site-packages (from torch) (10.3.5.147)
    Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.9/site-packages (from torch) (11.6.1.9)
    Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.9/site-packages (from torch) (12.3.1.170)
    Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./.local/lib/python3.9/site-packages (from torch) (0.6.2)
    Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.9/site-packages (from torch) (2.21.5)
    Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch) (12.4.127)
    Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.9/site-packages (from torch) (12.4.127)
    Requirement already satisfied: triton==3.2.0 in ./.local/lib/python3.9/site-packages (from torch) (3.2.0)
    Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.9/site-packages (from torch) (1.13.1)
    Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.9/site-packages (from sympy==1.13.1->torch) (1.3.0)
    Requirement already satisfied: scikit-learn in ./.local/lib/python3.9/site-packages (from sentence-transformers) (1.6.1)
    Requirement already satisfied: scipy in ./.local/lib/python3.9/site-packages (from sentence-transformers) (1.13.1)
    Requirement already satisfied: Pillow in ./.local/lib/python3.9/site-packages (from sentence-transformers) (11.1.0)
    Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.9/site-packages (from ipywidgets) (0.2.2)
    Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.9/site-packages (from ipywidgets) (8.18.1)
    Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/site-packages (from ipywidgets) (5.14.3)
    Requirement already satisfied: widgetsnbextension~=4.0.12 in ./.local/lib/python3.9/site-packages (from ipywidgets) (4.0.13)
    Requirement already satisfied: jupyterlab-widgets~=3.0.12 in ./.local/lib/python3.9/site-packages (from ipywidgets) (3.0.13)
    Requirement already satisfied: click!=8.0.0,>=7.1 in ./.local/lib/python3.9/site-packages (from wandb) (8.1.8)
    Requirement already satisfied: docker-pycreds>=0.4.0 in ./.local/lib/python3.9/site-packages (from wandb) (0.4.0)
    Requirement already satisfied: eval-type-backport in ./.local/lib/python3.9/site-packages (from wandb) (0.2.2)
    Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in ./.local/lib/python3.9/site-packages (from wandb) (3.1.44)
    Requirement already satisfied: platformdirs in /usr/local/lib/python3.9/site-packages (from wandb) (4.3.6)
    Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.15.0 in ./.local/lib/python3.9/site-packages (from wandb) (5.29.3)
    Requirement already satisfied: pydantic<3,>=2.6 in ./.local/lib/python3.9/site-packages (from wandb) (2.10.5)
    Requirement already satisfied: sentry-sdk>=2.0.0 in ./.local/lib/python3.9/site-packages (from wandb) (2.20.0)
    Requirement already satisfied: setproctitle in ./.local/lib/python3.9/site-packages (from wandb) (1.3.4)
    Requirement already satisfied: setuptools in /usr/lib/python3.9/site-packages (from wandb) (53.0.0)
    Requirement already satisfied: six>=1.4.0 in /usr/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)
    Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.6)
    Requirement already satisfied: aiosignal>=1.1.2 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.2)
    Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.2)
    Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/site-packages (from aiohttp->datasets) (24.2.0)
    Requirement already satisfied: frozenlist>=1.1.1 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)
    Requirement already satisfied: multidict<7.0,>=4.5 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)
    Requirement already satisfied: propcache>=0.2.0 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (0.2.1)
    Requirement already satisfied: yarl<2.0,>=1.17.0 in ./.local/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.3)
    Requirement already satisfied: gitdb<5,>=4.0.1 in ./.local/lib/python3.9/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)
    Requirement already satisfied: decorator in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)
    Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)
    Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)
    Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.48)
    Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)
    Requirement already satisfied: stack-data in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)
    Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (1.2.2)
    Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)
    Requirement already satisfied: annotated-types>=0.6.0 in ./.local/lib/python3.9/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)
    Requirement already satisfied: pydantic-core==2.27.2 in ./.local/lib/python3.9/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)
    Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib64/python3.9/site-packages (from requests->transformers) (3.4.0)
    Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3.9/site-packages (from requests->transformers) (2.10)
    Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.9/site-packages (from requests->transformers) (2.3.0)
    Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2024.8.30)
    Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib64/python3.9/site-packages (from jinja2->torch) (3.0.2)
    Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)
    Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.9/site-packages (from pandas->datasets) (2024.2)
    Requirement already satisfied: tzdata>=2022.7 in ./.local/lib/python3.9/site-packages (from pandas->datasets) (2024.2)
    Requirement already satisfied: joblib>=1.2.0 in ./.local/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (1.4.2)
    Requirement already satisfied: threadpoolctl>=3.1.0 in ./.local/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.5.0)
    Requirement already satisfied: smmap<6,>=3.0.1 in ./.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)
    Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.9/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)
    Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)
    Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)
    Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.1.0)
    Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)
    Requirement already satisfied: pure-eval in /usr/local/lib/python3.9/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)



```python
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # é¿å… tokenizers çš„ fork è­¦å‘Š
import torch

# å»ºè­°åœ¨è¼ƒæ–°å‹è™Ÿçš„ GPU ä¸Šé–‹å•Ÿä»¥ä¸‹åŠ é€Ÿé¸é …
torch.backends.cudnn.benchmark = True
# torch.set_float32_matmul_precision("medium")  # å¦‚æœ‰éœ€è¦å¯èª¿æ•´ç²¾åº¦

from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    default_data_collator,
    BitsAndBytesConfig,
)
from peft import LoraConfig, get_peft_model, PeftModel
import wandb

# è¨­å®šæ¯å¼µ GPU çš„æœ€å¤§è¨˜æ†¶é«”ï¼ˆå…©å¼µ A10G 24GBï¼‰
max_memory = {i: "24GB" for i in range(torch.cuda.device_count())}

# 4-bit é‡åŒ–è¨­å®š
quant_config = BitsAndBytesConfig(
    load_in_4bit=True,                  
    bnb_4bit_use_double_quant=True,     
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",          # å¸¸ç”¨æ–¼ LLaMA / QLoRA
)

model_name = "yentinglin/Llama-3-Taiwan-8B-Instruct"

# è¼‰å…¥æ¨¡å‹ï¼ˆåŒæ™‚æ ¹æ“š max_memory è‡ªå‹•åˆ†é…åˆ°å…©å¼µ GPUï¼‰
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    max_memory=max_memory,
    torch_dtype=torch.float16,
    quantization_config=quant_config,
)

tokenizer = AutoTokenizer.from_pretrained(
    model_name,
    use_fast=False  # LLaMA tokenizer é€šå¸¸ä¸æ”¯æ´ fast
)

# LoRA è¨­å®šï¼ˆå¯ä¾éœ€æ±‚èª¿æ•´ r èˆ‡ target_modulesï¼‰
lora_config = LoraConfig(
    r=16,                         
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],  # ä¾æ¨¡å‹çµæ§‹èª¿æ•´
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)

# å°‡ LoRA å¥—ç”¨è‡³æ¨¡å‹
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # æŸ¥çœ‹å¯è¨“ç·´åƒæ•¸æ•¸é‡
```


    Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]


    trainable params: 6,815,744 || all params: 8,037,093,376 || trainable%: 0.0848



```python
# è¼‰å…¥è¨“ç·´è³‡æ–™ï¼ˆJSON æ ¼å¼ï¼‰
data_files = {"train": "data/hugging_bear.json"}
raw_datasets = load_dataset("json", data_files=data_files)

# å®šç¾© prompt æ ¼å¼åŒ–å‡½å¼
def format_example(ex):
    instruction = ex["instruction"]
    context = ex.get("context", "")
    response = ex["response"]
    if context:
        prompt = f"Human: {instruction}\n{context}\nAssistant:"
    else:
        prompt = f"Human: {instruction}\nAssistant:"
    return prompt, response

# å®šç¾©è³‡æ–™é è™•ç†å‡½å¼ï¼ˆå°‡ prompt èˆ‡å›ç­”è½‰æ›æˆæ¨¡å‹æ‰€éœ€çš„ input_ids èˆ‡ labelsï¼‰
def preprocess_function(examples):
    all_input_ids = []
    all_labels = []
    for instruction, context, response in zip(
        examples["instruction"],
        examples["context"],
        examples["response"]
    ):
        prompt, ans = format_example({
            "instruction": instruction,
            "context": context,
            "response": response
        })
        prompt_ids = tokenizer(prompt, add_special_tokens=False)["input_ids"]
        answer_ids = tokenizer(ans, add_special_tokens=False)["input_ids"]
        # QLoRA åŒæ¨£æ˜¯ Causal LM åšæ³•ï¼šprompt + answer æ‹¼æ¥
        input_ids = prompt_ids + answer_ids
        # å° prompt éƒ¨åˆ†æ¨™è¨˜ -100 ä»¥å¿½ç•¥ loss è¨ˆç®—
        labels = [-100] * len(prompt_ids) + answer_ids

        max_length = 512
        if len(input_ids) > max_length:
            input_ids = input_ids[:max_length]
            labels = labels[:max_length]

        all_input_ids.append(input_ids)
        all_labels.append(labels)

    return {"input_ids": all_input_ids, "labels": all_labels}

# ä½¿ç”¨ map() é è™•ç†è³‡æ–™
processed_dataset = raw_datasets.map(preprocess_function, batched=True)
train_dataset = processed_dataset["train"]
```


```python
# è¨“ç·´åƒæ•¸è¨­å®šï¼ˆå¯æ ¹æ“šé¡¯å­˜æƒ…æ³èª¿æ•´ per_device_train_batch_size èˆ‡ gradient_accumulation_stepsï¼‰
training_args = TrainingArguments(
    output_dir="./lora-llama3-taiwan-8b-instruct",
    overwrite_output_dir=True,
    num_train_epochs=1,
    per_device_train_batch_size=1,        # è‹¥é¡¯å­˜è¶³å¤ ï¼Œå¯èª¿å¤§ï¼Œä¾‹å¦‚æ”¹æˆ 2
    gradient_accumulation_steps=4,          # èª¿æ•´ä»¥é”æˆåˆé©çš„æœ‰æ•ˆ batch size
    logging_steps=10,
    save_steps=100,
    eval_strategy="no",                     # è‹¥æœ‰é©—è­‰é›†å¯æ”¹ "steps" æˆ– "epoch"
    fp16=True,                            # ä½¿ç”¨åŠç²¾åº¦è¨“ç·´
    learning_rate=1e-4,
    max_grad_norm=1.0,                      # åŠ å…¥æ¢¯åº¦è£å‰ªï¼ˆå¯é¿å…æ¢¯åº¦çˆ†ç‚¸ï¼‰
    logging_dir="./logs",                   # è¨­å®š TensorBoard log ç›®éŒ„
    # gradient_checkpointing=True,         # å¦‚éœ€é€²ä¸€æ­¥ç¯€çœé¡¯å­˜ï¼Œå¯é–‹å•Ÿæ­¤é¸é …
    # torch_compile=True,                  # PyTorch 2.0+ å¯å˜—è©¦å•Ÿç”¨ä»¥åŠ é€Ÿ
)

data_collator = default_data_collator  # æ¡ç”¨é è¨­ collatorï¼ˆå‹•æ…‹ paddingï¼‰

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=data_collator,
)

# é–‹å§‹è¨“ç·´
trainer.train()
trainer.save_model("./lora-llama3-taiwan-8b-instruct")
# è‹¥è¦ä¸Šå‚³åˆ° Hugging Face Hubï¼Œå¯ä½¿ç”¨ï¼š
# trainer.push_to_hub("your-username/my-qlora-llama3-taiwan-8b-instruct")

########################################
# ä»¥ä¸‹ç‚ºæ¨è«–ç¨‹å¼ç¢¼
########################################

# è¼‰å…¥åŸºåº•æ¨¡å‹ï¼ˆ4-bitï¼‰ä¸¦æŒ‡å®š max_memory
base_model_for_inference = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    max_memory=max_memory,
    torch_dtype=torch.float16,
    quantization_config=quant_config,  # ä¸€å®šè¦å’Œè¨“ç·´æ™‚å°æ‡‰
)

# å¥—ç”¨ LoRA æ¬Šé‡
lora_model_path = "./lora-llama3-taiwan-8b-instruct"
inference_model = PeftModel.from_pretrained(base_model_for_inference, lora_model_path)
inference_model.eval()

import re

# å®šç¾©å¾Œè™•ç†å‡½å¼ï¼Œç”¨ä»¥å°‡å›ç­”è£åˆ‡æˆæœ€å¤šå…©å¥è©±
def postprocess_short_answer(text, max_sentences=2):
    # ä¾å¥è™Ÿã€å•è™Ÿã€é©šå˜†è™Ÿåˆ†å‰²æ–‡å­—
    sentences = re.split(r'([.!?])', text)
    short_text = []
    count = 0
    # sentences å…§æœƒäº¤éŒ¯å‡ºç¾å¥å­èˆ‡æ¨™é»ï¼Œæ•…æ¯å…©å€‹å…ƒç´ åˆä½µæˆä¸€å¥
    for i in range(0, len(sentences), 2):
        short_text.append(sentences[i])
        if i + 1 < len(sentences):
            short_text.append(sentences[i + 1])
        count += 1
        if count >= max_sentences:
            break
    return "".join(short_text).strip()
```

    [34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
    Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.
    [34m[1mwandb[0m: Currently logged in as: [33mnrps9909[0m ([33mnrps9909-national-taiwan-university[0m) to [32mhttps://api.wandb.ai[0m. Use [1m`wandb login --relogin`[0m to force relogin
    [34m[1mwandb[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.



Tracking run with wandb version 0.19.6



Run data is saved locally in <code>/trinity/home/tna001/wandb/run-20250213_091812-cx87aizs</code>



Syncing run <strong><a href='https://wandb.ai/nrps9909-national-taiwan-university/huggingface/runs/cx87aizs' target="_blank">./lora-llama3-taiwan-8b-instruct</a></strong> to <a href='https://wandb.ai/nrps9909-national-taiwan-university/huggingface' target="_blank">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target="_blank">docs</a>)<br>



View project at <a href='https://wandb.ai/nrps9909-national-taiwan-university/huggingface' target="_blank">https://wandb.ai/nrps9909-national-taiwan-university/huggingface</a>



View run at <a href='https://wandb.ai/nrps9909-national-taiwan-university/huggingface/runs/cx87aizs' target="_blank">https://wandb.ai/nrps9909-national-taiwan-university/huggingface/runs/cx87aizs</a>




    <div>

      <progress value='215' max='215' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [215/215 05:46, Epoch 0/1]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>10</td>
      <td>4.068500</td>
    </tr>
    <tr>
      <td>20</td>
      <td>3.543200</td>
    </tr>
    <tr>
      <td>30</td>
      <td>3.632300</td>
    </tr>
    <tr>
      <td>40</td>
      <td>3.362400</td>
    </tr>
    <tr>
      <td>50</td>
      <td>3.278600</td>
    </tr>
    <tr>
      <td>60</td>
      <td>3.440000</td>
    </tr>
    <tr>
      <td>70</td>
      <td>3.338100</td>
    </tr>
    <tr>
      <td>80</td>
      <td>3.391700</td>
    </tr>
    <tr>
      <td>90</td>
      <td>3.251400</td>
    </tr>
    <tr>
      <td>100</td>
      <td>3.320100</td>
    </tr>
    <tr>
      <td>110</td>
      <td>3.435300</td>
    </tr>
    <tr>
      <td>120</td>
      <td>3.128700</td>
    </tr>
    <tr>
      <td>130</td>
      <td>3.123300</td>
    </tr>
    <tr>
      <td>140</td>
      <td>3.113700</td>
    </tr>
    <tr>
      <td>150</td>
      <td>3.409600</td>
    </tr>
    <tr>
      <td>160</td>
      <td>3.302200</td>
    </tr>
    <tr>
      <td>170</td>
      <td>3.249300</td>
    </tr>
    <tr>
      <td>180</td>
      <td>3.249200</td>
    </tr>
    <tr>
      <td>190</td>
      <td>3.522400</td>
    </tr>
    <tr>
      <td>200</td>
      <td>3.257300</td>
    </tr>
    <tr>
      <td>210</td>
      <td>3.416200</td>
    </tr>
  </tbody>
</table><p>



    Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]



```python
# ä¿®æ”¹ generate_answer()ï¼Œä½¿ç”Ÿæˆæç¤ºèˆ‡å¾Œè™•ç†ä¸€è‡´ï¼ˆä½¿ç”¨ã€Œæœ€å¤šå…©å¥è©±å›æ‡‰ã€ï¼‰
def generate_answer(prompt, max_new_tokens=256):
    short_prompt = f"Human: è«‹ç”¨æœ€å¤šå…©å¥è©±å›æ‡‰ä»¥ä¸‹å•é¡Œï¼š\n{prompt}\nAssistant:"
    inputs = tokenizer(short_prompt, return_tensors="pt").to(inference_model.device)
    with torch.no_grad():
        outputs = inference_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.2,
            pad_token_id=tokenizer.eos_token_id  # æ˜ç¢ºè¨­å®š pad_token_id
        )
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    short_answer = postprocess_short_answer(answer, max_sentences=2)
    return short_answer
```


```python
# æ¸¬è©¦æ¨è«–
test_prompt = "æˆ‘æœ€è¿‘æœ‰ä¸€å€‹æ–°çš„å–œæ­¡çš„å¥³ç”Ÿï¼Œæˆ‘åœ¨æƒ³è¦ä¸è¦è·ŸåŸæœ¬çš„å¥³æœ‹å‹åˆ†æ‰‹ï¼Œä½ è¦ºå¾—å‘¢"
answer = generate_answer(test_prompt)
print("=== æ¨è«–çµæœ ===")
print(answer)
```

    === æ¨è«–çµæœ ===
    Human: è«‹ç”¨æœ€å¤šå…©å¥è©±å›æ‡‰ä»¥ä¸‹å•é¡Œï¼š
    æˆ‘æœ€è¿‘æœ‰ä¸€å€‹æ–°çš„å–œæ­¡çš„å¥³ç”Ÿï¼Œæˆ‘åœ¨æƒ³è¦ä¸è¦è·ŸåŸæœ¬çš„å¥³æœ‹å‹åˆ†æ‰‹ï¼Œä½ è¦ºå¾—å‘¢
    Assistant:ä¸å»ºè­° åˆ†ä¸€æ¨£æ˜¯äººé¡ ä½†ä½ å€‘æœ¬ä¾†å°±æ˜¯æ²’æœ‰æ„Ÿæƒ…äº†å§ï¼Ÿç¾åœ¨é‚„æœ‰ä»€éº¼å¥½å¤±å»çš„ã€‚åªæ˜¯é€™äº›éƒ½æ˜¯ä»¥å¾Œçš„äº‹å•¦ï¼å°æ–¹ä¹Ÿè¦çœŸçš„æ„›ä½  è€Œä¸”æ¯”å¥¹æ›´å„ªç§€æ‰è¡Œï¼ˆå› ç‚ºä½ å€‘éƒ½æ²’æ„Ÿæƒ…ï¼‰..



```python
import os
import re
import torch
import faiss
import numpy as np
import threading
import ipywidgets as widgets
from IPython.display import display
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
from sentence_transformers import SentenceTransformer
import logging

# è¨­å®š logging æ ¼å¼èˆ‡å±¤ç´š
logging.basicConfig(level=logging.DEBUG, format='[%(levelname)s] %(message)s')
logger = logging.getLogger(__name__)

#############################################
# è¼”åŠ©å‡½æ•¸ï¼šç§»é™¤ç‰¹æ®Š token èˆ‡ emoji ä»¥åŠéæ¿¾äº‚ç¢¼
#############################################
def remove_emojis(text: str) -> str:
    emoji_pattern = re.compile(
        "[" 
        "\U0001F600-\U0001F64F"  # è¡¨æƒ…ç¬¦è™Ÿ
        "\U0001F300-\U0001F5FF"  # ç¬¦è™Ÿèˆ‡åœ–ç¤º
        "\U0001F680-\U0001F6FF"  # äº¤é€šå·¥å…·èˆ‡åœ°åœ–
        "\U0001F1E0-\U0001F1FF"  # åœ‹æ——
        "]+", flags=re.UNICODE)
    return emoji_pattern.sub(r'', text)

def remove_special_tokens(text: str) -> str:
    tokens_to_remove = ["</s>", "<|im_end|>", "<|begin_of_text|>", "<|endoftext|>"]
    for token in tokens_to_remove:
        text = text.replace(token, "")
    text = re.sub(r"<\|.*?\|>", "", text)
    text = remove_emojis(text)
    return text.strip()

def filter_gibberish(text: str) -> str:
    """
    ç§»é™¤éé•·ä¸”åªç”±è‹±æ–‡å­—æ¯ã€æ•¸å­—åŠç‰¹å®šç¬¦è™Ÿçµ„æˆçš„ç‰‡æ®µï¼Œé¿å…äº‚ç¢¼
    """
    tokens = text.split()
    filtered_tokens = []
    for token in tokens:
        if re.fullmatch(r'[A-Za-z0-9+\-#^_]{8,}', token):
            continue
        filtered_tokens.append(token)
    return " ".join(filtered_tokens)

#############################################
# 1. æ¨¡å‹èˆ‡ Tokenizer çš„åˆå§‹åŒ–
#############################################
def setup_model(model_path: str):
    os.environ["TOKENIZERS_PARALLELISM"] = "false"
    logger.debug("è¨­å®š TOKENIZERS_PARALLELISM ç‚º false")
    
    try:
        logger.debug("è¨­å®šé‡åŒ–åƒæ•¸ä¸¦è¼‰å…¥æ¨¡å‹")
        quant_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4",
        )
        max_memory = {i: "24GB" for i in range(torch.cuda.device_count())}
        
        base_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            device_map="auto",
            max_memory=max_memory,
            torch_dtype=torch.float16,
            quantization_config=quant_config,
        )
        logger.debug("Base model è¼‰å…¥æˆåŠŸ")
        
        tokenizer = AutoTokenizer.from_pretrained("yentinglin/Llama-3-Taiwan-8B-Instruct", use_fast=False)
        logger.debug("Tokenizer è¼‰å…¥æˆåŠŸ")
        
        inference_model = PeftModel.from_pretrained(base_model, model_path)
        inference_model.eval()
        logger.debug("LoRA æ¬Šé‡å·²å¥—ç”¨ï¼Œæ¨¡å‹è¨­å®šç‚º eval æ¨¡å¼")
        
        return tokenizer, inference_model
    except Exception as e:
        logger.exception("æ¨¡å‹è¼‰å…¥æ™‚ç™¼ç”ŸéŒ¯èª¤")
        raise

#############################################
# 2. FAISS èˆ‡ SentenceTransformer åˆå§‹åŒ–
#############################################
def setup_faiss():
    try:
        logger.debug("è¼‰å…¥ SentenceTransformer æ¨¡å‹")
        embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device='cuda')
        embedding_dim = embedding_model.get_sentence_embedding_dimension()
        logger.debug(f"SentenceTransformer è¼‰å…¥æˆåŠŸï¼ŒåµŒå…¥ç¶­åº¦: {embedding_dim}")

        logger.debug("å»ºç«‹ FAISS CPU ç´¢å¼•")
        faiss_index = faiss.IndexFlatL2(embedding_dim)
        logger.debug("FAISS ç´¢å¼•å»ºç«‹æˆåŠŸ")
        
        return embedding_model, faiss_index
    except Exception as e:
        logger.exception("å»ºç«‹ FAISS ç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤")
        raise

#############################################
# 3. å°è©±æ­·å²ç®¡ç†èˆ‡æ–‡ä»¶å„²å­˜
#############################################
conversation_history = []   # å„²å­˜ (role, message)
document_store = []         # å„²å­˜æ‰€æœ‰åŠ å…¥ç´¢å¼•çš„æ–‡å­—

def append_history(role: str, message: str):
    conversation_history.append((role, message))

def get_recent_context(num_turns: int = 1) -> str:
    history = conversation_history[-(num_turns * 2):]
    context = ""
    for role, msg in history:
        context += f"{role}: {msg}\n"
    return context.strip()

#############################################
# 4. å›æ‡‰å¾Œè™•ç†èˆ‡æå–å‡½æ•¸
#############################################
def remove_urls(text: str) -> str:
    text = re.sub(r"://\S+", "", text)
    return text.strip()

def extract_generated_answer(full_response: str, prompt: str) -> str:
    logger.debug("extract_generated_answer: å¾ full_response ä¸­æå–å›ç­”")
    candidate = full_response[len(prompt):].strip() if full_response.startswith(prompt) else full_response.strip()
    candidate = remove_special_tokens(candidate)
    candidate = filter_gibberish(candidate)
    parts = re.split(r"Assistant[:ï¼š]", candidate)
    result = parts[-1].strip() if len(parts) > 1 else candidate
    result = re.split(r"User[:ï¼š]", result)[0].strip()
    result = remove_urls(result)
    logger.debug("extract_generated_answer: æå–å¾Œçµæœ -> " + result)
    return result

def postprocess_answer(text: str, max_sentences: int = 2) -> str:
    logger.debug("postprocess_answer: é–‹å§‹è™•ç†å›ç­”")
    text = remove_special_tokens(text)
    text = filter_gibberish(text)
    text = re.sub(r"://\S+", "", text)
    paragraphs = re.split(r"\n\s*\n", text)
    text = paragraphs[0].strip() if paragraphs else text
    sentences = re.split(r"[.!?ã€‚ï¼ï¼Ÿ]", text)
    sentences = [s.strip() for s in sentences if s.strip()]
    short_sentences = sentences[:max_sentences]
    output = ". ".join(short_sentences)
    if output and output[-1] not in ".ã€‚ï¼ï¼Ÿ":
        output += '.'
    logger.debug("postprocess_answer: è™•ç†å¾Œå›ç­” -> " + output)
    return output

#############################################
# 5. æª¢ç´¢å‡½æ•¸ï¼šå¾ FAISS ç´¢å¼•ä¸­æª¢ç´¢ç›¸é—œæ–‡ä»¶
#############################################
def retrieve_documents(query: str, embedding_model, faiss_index, top_k: int = 3):
    logger.debug(f"retrieve_documents: å¾ FAISS ä¸­æª¢ç´¢èˆ‡ query '{query}' ç›¸é—œçš„æ–‡ä»¶")
    query_embedding = embedding_model.encode([query])
    query_embedding = np.array(query_embedding).astype('float32')
    distances, indices = faiss_index.search(query_embedding, top_k)
    retrieved_docs = []
    for idx in indices[0]:
        if idx != -1 and idx < len(document_store):
            retrieved_docs.append(document_store[idx])
    logger.debug(f"retrieve_documents: æª¢ç´¢åˆ°çš„æ–‡ä»¶ -> {retrieved_docs}")
    return retrieved_docs

#############################################
# 6. ipywidgets ä»‹é¢å»ºç«‹
#############################################
def setup_widgets():
    text_input = widgets.Text(
        placeholder='è«‹è¼¸å…¥å°è©±å…§å®¹...',
        description='User:',
        layout=widgets.Layout(width='80%')
    )
    send_button = widgets.Button(
        description='é€å‡º',
        button_style='primary'
    )
    output_area = widgets.Output(
        layout={'border': '1px solid black', 'height': '300px', 'overflow_y': 'auto'}
    )
    display(text_input, send_button, output_area)
    logger.debug("ipywidgets ä»‹é¢å»ºç«‹æˆåŠŸ")
    return text_input, send_button, output_area

#############################################
# 7. ç”Ÿæˆå›ç­”èˆ‡æŒ‰éˆ•äº‹ä»¶è™•ç†
#############################################
def add_to_index(text: str, embedding_model, faiss_index):
    logger.debug(f"add_to_index: æ­£åœ¨åŠ å…¥æ–‡å­— -> {text}")
    try:
        embedding = embedding_model.encode([text])
        embedding = np.array(embedding).astype('float32')
        faiss_index.add(embedding)
        document_store.append(text)
        logger.debug("add_to_index: åŠ å…¥æˆåŠŸ")
    except Exception as e:
        logger.exception("add_to_index ç™¼ç”ŸéŒ¯èª¤")

def generate_response(inputs, prompt, progress, output_area, inference_model, tokenizer, embedding_model, faiss_index):
    try:
        logger.debug("generate_response: é–‹å§‹ç”Ÿæˆå›ç­”")
        with torch.no_grad():
            outputs = inference_model.generate(
                **inputs,
                max_new_tokens=60,
                do_sample=True,
                temperature=0.9,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.2,
                pad_token_id=tokenizer.eos_token_id,
                use_cache=True,
                early_stopping=True
            )
        progress.value = 80
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)
        logger.debug("generate_response: å®Œæ•´ç”Ÿæˆçµæœ -> " + full_response)

        generated_answer = extract_generated_answer(full_response, prompt)
        final_answer = postprocess_answer(generated_answer, max_sentences=2)
        
        progress.value = 100
        progress.close()
        
        output_area.append_stdout("Assistant: " + final_answer + "\n")
        logger.debug("generate_response: å°‡ Assistant å›ç­”åŠ å…¥ç´¢å¼•")
        append_history("Assistant", final_answer)
        add_to_index(f"Assistant: {final_answer}", embedding_model, faiss_index)
    except Exception as e:
        progress.close()
        logger.exception("generate_response: ç”Ÿæˆå›ç­”éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤")
        output_area.append_stdout("Error during generation: " + str(e) + "\n")

def main():
    model_path = "lora-llama3-taiwan-8b-instruct"
    tokenizer, inference_model = setup_model(model_path)
    embedding_model, faiss_index = setup_faiss()
    text_input, send_button, output_area = setup_widgets()
    
    def on_send_button_clicked(b):
        logger.debug("on_send_button_clicked: æŒ‰éˆ•è¢«é»æ“Š")
        user_message = text_input.value.strip()
        if not user_message:
            return
        
        # ä¸å†æ¸…é™¤è¼¸å‡ºï¼Œä¿ç•™æ­·å²å°è©±
        # output_area.clear_output(wait=True)
        text_input.value = ""
        output_area.append_stdout(f"User: {user_message}\n")
        
        if len(user_message) < 5:
            conversation_history.clear()
            logger.debug("å°è©±æ­·å²å·²æ¸…ç©ºï¼Œå› ç‚ºæ–°è¼¸å…¥è¼ƒçŸ­")
        
        append_history("User", user_message)
        add_to_index(f"User: {user_message}", embedding_model, faiss_index)
        
        # æª¢ç´¢ç›¸é—œä¸Šä¸‹æ–‡
        retrieved_docs = retrieve_documents(user_message, embedding_model, faiss_index, top_k=3)
        retrieved_context = ""
        if retrieved_docs:
            retrieved_context = "ç›¸é—œè³‡è¨Š:\n" + "\n".join(retrieved_docs) + "\n"
        
        # çµ„åˆ prompt
        system_message = (
            "ä½ æ˜¯ä¸€å€‹AIåŠ©ç†ï¼Œè«‹ä¸è¦ç”¢ç”Ÿä»»ä½•è¡¨æƒ…ç¬¦è™Ÿæˆ–emojiï¼Œå›ç­”ä¸è¶…éå…©å¥ï¼Œä¸”åªè¼¸å‡ºä¸€å¥ç°¡çŸ­çš„å›ç­”ã€‚\n"
            "è«‹æ ¹æ“šä»¥ä¸‹ç›¸é—œè³‡è¨Šå’Œæœ€æ–°çš„ä½¿ç”¨è€…è¼¸å…¥çµ¦å‡ºå›ç­”ï¼Œåˆ‡å‹¿å¼•ç”¨ä»»ä½•å…ˆå‰å°è©±å…§å®¹ï¼Œåªå›è¦†ä¸€å¥è©±ã€‚\n"
        )
        if retrieved_context:
            system_message += retrieved_context
        system_message += "User: " + user_message + "\nAssistant: "
        logger.debug("on_send_button_clicked: çµ„åˆå¾Œçš„ prompt -> " + system_message)
        
        progress = widgets.IntProgress(value=0, min=0, max=100, description='Processing:', bar_style='info')
        display(progress)
        
        def thread_target():
            generate_response(inputs, system_message, progress, output_area, inference_model, tokenizer, embedding_model, faiss_index)
            send_button.disabled = False
        
        try:
            logger.debug("on_send_button_clicked: Tokenizing prompt")
            inputs = tokenizer(system_message, return_tensors="pt").to(inference_model.device)
            progress.value = 20
            send_button.disabled = True
            threading.Thread(target=thread_target).start()
        except Exception as e:
            progress.close()
            logger.exception("on_send_button_clicked: ç”Ÿæˆå›ç­”éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤")
            output_area.append_stdout("Error during generation: " + str(e) + "\n")
            send_button.disabled = False

    send_button.on_click(on_send_button_clicked)
    logger.debug("å·²ç¶å®š send_button çš„ click äº‹ä»¶")
    logger.debug("=== ç¨‹å¼åŸ·è¡ŒçµæŸï¼Œç­‰å¾…ä½¿ç”¨è€…è¼¸å…¥ ===")
    print(f"[INFO] ç›®å‰ä½¿ç”¨çš„æ¨¡å‹è·¯å¾‘: {model_path}")

if __name__ == "__main__":
    main()

```

    [DEBUG] è¨­å®š TOKENIZERS_PARALLELISM ç‚º false
    [DEBUG] è¨­å®šé‡åŒ–åƒæ•¸ä¸¦è¼‰å…¥æ¨¡å‹
    [DEBUG] https://huggingface.co:443 "HEAD /yentinglin/Llama-3-Taiwan-8B-Instruct/resolve/main/config.json HTTP/1.1" 200 0



    Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]


    [DEBUG] https://huggingface.co:443 "HEAD /yentinglin/Llama-3-Taiwan-8B-Instruct/resolve/main/generation_config.json HTTP/1.1" 200 0
    [DEBUG] Base model è¼‰å…¥æˆåŠŸ
    [DEBUG] https://huggingface.co:443 "HEAD /yentinglin/Llama-3-Taiwan-8B-Instruct/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
    [DEBUG] Tokenizer è¼‰å…¥æˆåŠŸ
    [INFO] Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!
    [DEBUG] LoRA æ¬Šé‡å·²å¥—ç”¨ï¼Œæ¨¡å‹è¨­å®šç‚º eval æ¨¡å¼
    [DEBUG] è¼‰å…¥ SentenceTransformer æ¨¡å‹
    [INFO] Load pretrained SentenceTransformer: paraphrase-MiniLM-L6-v2
    [DEBUG] https://huggingface.co:443 "HEAD /sentence-transformers/paraphrase-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 200 0
    [DEBUG] https://huggingface.co:443 "HEAD /sentence-transformers/paraphrase-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1" 200 0
    [DEBUG] https://huggingface.co:443 "HEAD /sentence-transformers/paraphrase-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1" 200 0
    [DEBUG] https://huggingface.co:443 "HEAD /sentence-transformers/paraphrase-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1" 200 0
    [DEBUG] https://huggingface.co:443 "HEAD /sentence-transformers/paraphrase-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1" 200 0
    [DEBUG] https://huggingface.co:443 "HEAD /sentence-transformers/paraphrase-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1" 404 0
    [DEBUG] https://huggingface.co:443 "HEAD /sentence-transformers/paraphrase-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1" 200 0
    [DEBUG] https://huggingface.co:443 "HEAD /sentence-transformers/paraphrase-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1" 200 0
    [DEBUG] https://huggingface.co:443 "GET /api/models/sentence-transformers/paraphrase-MiniLM-L6-v2/revision/main HTTP/1.1" 200 3403
    [DEBUG] https://huggingface.co:443 "GET /api/models/sentence-transformers/paraphrase-MiniLM-L6-v2 HTTP/1.1" 200 3403
    [DEBUG] SentenceTransformer è¼‰å…¥æˆåŠŸï¼ŒåµŒå…¥ç¶­åº¦: 384
    [DEBUG] å»ºç«‹ FAISS CPU ç´¢å¼•
    [DEBUG] FAISS ç´¢å¼•å»ºç«‹æˆåŠŸ



    Text(value='', description='User:', layout=Layout(width='80%'), placeholder='è«‹è¼¸å…¥å°è©±å…§å®¹...')



    Button(button_style='primary', description='é€å‡º', style=ButtonStyle())



    Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦


    [DEBUG] ipywidgets ä»‹é¢å»ºç«‹æˆåŠŸ
    [DEBUG] å·²ç¶å®š send_button çš„ click äº‹ä»¶
    [DEBUG] === ç¨‹å¼åŸ·è¡ŒçµæŸï¼Œç­‰å¾…ä½¿ç”¨è€…è¼¸å…¥ ===


    [INFO] ç›®å‰ä½¿ç”¨çš„æ¨¡å‹è·¯å¾‘: lora-llama3-taiwan-8b-instruct


    [DEBUG] generate_response: é–‹å§‹ç”Ÿæˆå›ç­”
    /trinity/home/tna001/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:676: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.
      warnings.warn(
    [DEBUG] generate_response: å®Œæ•´ç”Ÿæˆçµæœ -> <|begin_of_text|>ä½ æ˜¯ä¸€å€‹AIåŠ©ç†ï¼Œè«‹ä¸è¦ç”¢ç”Ÿä»»ä½•è¡¨æƒ…ç¬¦è™Ÿæˆ–emojiï¼Œå›ç­”ä¸è¶…éå…©å¥ï¼Œä¸”åªè¼¸å‡ºä¸€å¥ç°¡çŸ­çš„å›ç­”ã€‚
    è«‹æ ¹æ“šä»¥ä¸‹ç›¸é—œè³‡è¨Šå’Œæœ€æ–°çš„ä½¿ç”¨è€…è¼¸å…¥çµ¦å‡ºå›ç­”ï¼Œåˆ‡å‹¿å¼•ç”¨ä»»ä½•å…ˆå‰å°è©±å…§å®¹ï¼Œåªå›è¦†ä¸€å¥è©±ã€‚
    ç›¸é—œè³‡è¨Š:
    User: æ—©å®‰
    User: æ—©å®‰
    Assistant: ä»Šå¤©æ—©ä¸Š8é»å¤šå°±èµ·åºŠäº†ï¼Œåœ¨è¾¦å…¬å®¤çœ‹è‘—å¤§å®¶ç¡è¦ºã€‚é€™æ™‚å€™ä¾†ä¸€æ¯å’–å•¡æ˜¯æœ€å¹¸ç¦çš„äº‹ï¼ç¥ç¦å„ä½ä»Šæ—¥é †åˆ©ï¼
    User:æ—©å¥½
    Userï¼šæˆ‘è¦å»æ´—æ¾¡å›‰ï¼Œä½ å€‘
    [DEBUG] extract_generated_answer: å¾ full_response ä¸­æå–å›ç­”
    [DEBUG] extract_generated_answer: æå–å¾Œçµæœ -> ä»Šå¤©æ—©ä¸Š8é»å¤šå°±èµ·åºŠäº†ï¼Œåœ¨è¾¦å…¬å®¤çœ‹è‘—å¤§å®¶ç¡è¦ºã€‚é€™æ™‚å€™ä¾†ä¸€æ¯å’–å•¡æ˜¯æœ€å¹¸ç¦çš„äº‹ï¼ç¥ç¦å„ä½ä»Šæ—¥é †åˆ©ï¼
    [DEBUG] postprocess_answer: é–‹å§‹è™•ç†å›ç­”
    [DEBUG] postprocess_answer: è™•ç†å¾Œå›ç­” -> ä»Šå¤©æ—©ä¸Š8é»å¤šå°±èµ·åºŠäº†ï¼Œåœ¨è¾¦å…¬å®¤çœ‹è‘—å¤§å®¶ç¡è¦º. é€™æ™‚å€™ä¾†ä¸€æ¯å’–å•¡æ˜¯æœ€å¹¸ç¦çš„äº‹.
    [DEBUG] generate_response: å°‡ Assistant å›ç­”åŠ å…¥ç´¢å¼•
    [DEBUG] add_to_index: æ­£åœ¨åŠ å…¥æ–‡å­— -> Assistant: ä»Šå¤©æ—©ä¸Š8é»å¤šå°±èµ·åºŠäº†ï¼Œåœ¨è¾¦å…¬å®¤çœ‹è‘—å¤§å®¶ç¡è¦º. é€™æ™‚å€™ä¾†ä¸€æ¯å’–å•¡æ˜¯æœ€å¹¸ç¦çš„äº‹.
    [DEBUG] add_to_index: åŠ å…¥æˆåŠŸ
    [DEBUG] generate_response: é–‹å§‹ç”Ÿæˆå›ç­”
    [DEBUG] generate_response: å®Œæ•´ç”Ÿæˆçµæœ -> <|begin_of_text|>ä½ æ˜¯ä¸€å€‹AIåŠ©ç†ï¼Œè«‹ä¸è¦ç”¢ç”Ÿä»»ä½•è¡¨æƒ…ç¬¦è™Ÿæˆ–emojiï¼Œå›ç­”ä¸è¶…éå…©å¥ï¼Œä¸”åªè¼¸å‡ºä¸€å¥ç°¡çŸ­çš„å›ç­”ã€‚
    è«‹æ ¹æ“šä»¥ä¸‹ç›¸é—œè³‡è¨Šå’Œæœ€æ–°çš„ä½¿ç”¨è€…è¼¸å…¥çµ¦å‡ºå›ç­”ï¼Œåˆ‡å‹¿å¼•ç”¨ä»»ä½•å…ˆå‰å°è©±å…§å®¹ï¼Œåªå›è¦†ä¸€å¥è©±ã€‚
    ç›¸é—œè³‡è¨Š:
    User: æˆ‘å–œæ­¡ä½ 
    User: æ—©å®‰
    Assistant: ä»Šå¤©æ—©ä¸Š8é»å¤šå°±èµ·åºŠäº†ï¼Œåœ¨è¾¦å…¬å®¤çœ‹è‘—å¤§å®¶ç¡è¦º. é€™æ™‚å€™ä¾†ä¸€æ¯å’–å•¡æ˜¯æœ€å¹¸ç¦çš„äº‹.
    User: æˆ‘å–œæ­¡ä½ 
    Assistant: é™¤äº†èªªæˆ‘æ„›ä»–ä¹‹å¤–ï¼Œæˆ‘ä¸çŸ¥é“è©²æ€éº¼è¡¨é”é€™ç¨®æ„Ÿè¦ºã€‚å¥½æ¯”æœ‰äº›äººæœƒè²·å¾ˆå¤šèŠ±æŸé€çµ¦è‡ªå·±é–‹å¿ƒ...ç­‰ï¼ˆï¼Ÿï¼‰å¦‚æœåœ¨æ„çš„äººä¸ä»‹æ„çš„è©±ï¼Œä¹Ÿå¯ä»¥è·Ÿä»–å€‘è¨è«–ä¸€ä¸‹ã€‚ï¼ˆæœ‰å¤šå°‘æ™‚é–“å»æ‰¾
    [DEBUG] extract_generated_answer: å¾ full_response ä¸­æå–å›ç­”
    [DEBUG] extract_generated_answer: æå–å¾Œçµæœ -> é™¤äº†èªªæˆ‘æ„›ä»–ä¹‹å¤–ï¼Œæˆ‘ä¸çŸ¥é“è©²æ€éº¼è¡¨é”é€™ç¨®æ„Ÿè¦ºã€‚å¥½æ¯”æœ‰äº›äººæœƒè²·å¾ˆå¤šèŠ±æŸé€çµ¦è‡ªå·±é–‹å¿ƒ...ç­‰ï¼ˆï¼Ÿï¼‰å¦‚æœåœ¨æ„çš„äººä¸ä»‹æ„çš„è©±ï¼Œä¹Ÿå¯ä»¥è·Ÿä»–å€‘è¨è«–ä¸€ä¸‹ã€‚ï¼ˆæœ‰å¤šå°‘æ™‚é–“å»æ‰¾
    [DEBUG] postprocess_answer: é–‹å§‹è™•ç†å›ç­”
    [DEBUG] postprocess_answer: è™•ç†å¾Œå›ç­” -> é™¤äº†èªªæˆ‘æ„›ä»–ä¹‹å¤–ï¼Œæˆ‘ä¸çŸ¥é“è©²æ€éº¼è¡¨é”é€™ç¨®æ„Ÿè¦º. å¥½æ¯”æœ‰äº›äººæœƒè²·å¾ˆå¤šèŠ±æŸé€çµ¦è‡ªå·±é–‹å¿ƒ.
    [DEBUG] generate_response: å°‡ Assistant å›ç­”åŠ å…¥ç´¢å¼•
    [DEBUG] add_to_index: æ­£åœ¨åŠ å…¥æ–‡å­— -> Assistant: é™¤äº†èªªæˆ‘æ„›ä»–ä¹‹å¤–ï¼Œæˆ‘ä¸çŸ¥é“è©²æ€éº¼è¡¨é”é€™ç¨®æ„Ÿè¦º. å¥½æ¯”æœ‰äº›äººæœƒè²·å¾ˆå¤šèŠ±æŸé€çµ¦è‡ªå·±é–‹å¿ƒ.
    [DEBUG] add_to_index: åŠ å…¥æˆåŠŸ
    [DEBUG] generate_response: é–‹å§‹ç”Ÿæˆå›ç­”
    [DEBUG] generate_response: å®Œæ•´ç”Ÿæˆçµæœ -> <|begin_of_text|>ä½ æ˜¯ä¸€å€‹AIåŠ©ç†ï¼Œè«‹ä¸è¦ç”¢ç”Ÿä»»ä½•è¡¨æƒ…ç¬¦è™Ÿæˆ–emojiï¼Œå›ç­”ä¸è¶…éå…©å¥ï¼Œä¸”åªè¼¸å‡ºä¸€å¥ç°¡çŸ­çš„å›ç­”ã€‚
    è«‹æ ¹æ“šä»¥ä¸‹ç›¸é—œè³‡è¨Šå’Œæœ€æ–°çš„ä½¿ç”¨è€…è¼¸å…¥çµ¦å‡ºå›ç­”ï¼Œåˆ‡å‹¿å¼•ç”¨ä»»ä½•å…ˆå‰å°è©±å…§å®¹ï¼Œåªå›è¦†ä¸€å¥è©±ã€‚
    ç›¸é—œè³‡è¨Š:
    User: æˆ‘å–œæ­¡ä½ 
    User: ä½ å–œæ­¡æˆ‘å˜›
    Assistant: é™¤äº†èªªæˆ‘æ„›ä»–ä¹‹å¤–ï¼Œæˆ‘ä¸çŸ¥é“è©²æ€éº¼è¡¨é”é€™ç¨®æ„Ÿè¦º. å¥½æ¯”æœ‰äº›äººæœƒè²·å¾ˆå¤šèŠ±æŸé€çµ¦è‡ªå·±é–‹å¿ƒ.
    User: ä½ å–œæ­¡æˆ‘å˜›
    Assistant: é—œæ–¼æ˜¯å¦å–œæ­¡ä½ çš„å•é¡Œï¼Œä¸è¦å•æˆ‘ï¼Œä¹Ÿä¸è¦è€ƒæ…®æˆ‘çš„æ„è¦‹ï¼Œè¦åšæœ€çœŸå¯¦çš„åˆ¤æ–·ï¼Œä½ èƒ½å¾ä»–çš„è¨€è¡Œèˆ‰æ­¢ä¸­çœ‹å¾—å¾ˆæ¸…æ¥šã€‚å°±è·Ÿé‚£äº›åœ¨æ„Ÿæƒ…ä¸Šç³¾çµçš„äººä¸€æ¨£ï¼Œä»–å€‘ç¸½æ˜¯å¸Œæœ›åˆ¥äºº
    [DEBUG] extract_generated_answer: å¾ full_response ä¸­æå–å›ç­”
    [DEBUG] extract_generated_answer: æå–å¾Œçµæœ -> é—œæ–¼æ˜¯å¦å–œæ­¡ä½ çš„å•é¡Œï¼Œä¸è¦å•æˆ‘ï¼Œä¹Ÿä¸è¦è€ƒæ…®æˆ‘çš„æ„è¦‹ï¼Œè¦åšæœ€çœŸå¯¦çš„åˆ¤æ–·ï¼Œä½ èƒ½å¾ä»–çš„è¨€è¡Œèˆ‰æ­¢ä¸­çœ‹å¾—å¾ˆæ¸…æ¥šã€‚å°±è·Ÿé‚£äº›åœ¨æ„Ÿæƒ…ä¸Šç³¾çµçš„äººä¸€æ¨£ï¼Œä»–å€‘ç¸½æ˜¯å¸Œæœ›åˆ¥äºº
    [DEBUG] postprocess_answer: é–‹å§‹è™•ç†å›ç­”
    [DEBUG] postprocess_answer: è™•ç†å¾Œå›ç­” -> é—œæ–¼æ˜¯å¦å–œæ­¡ä½ çš„å•é¡Œï¼Œä¸è¦å•æˆ‘ï¼Œä¹Ÿä¸è¦è€ƒæ…®æˆ‘çš„æ„è¦‹ï¼Œè¦åšæœ€çœŸå¯¦çš„åˆ¤æ–·ï¼Œä½ èƒ½å¾ä»–çš„è¨€è¡Œèˆ‰æ­¢ä¸­çœ‹å¾—å¾ˆæ¸…æ¥š. å°±è·Ÿé‚£äº›åœ¨æ„Ÿæƒ…ä¸Šç³¾çµçš„äººä¸€æ¨£ï¼Œä»–å€‘ç¸½æ˜¯å¸Œæœ›åˆ¥äºº.
    [DEBUG] generate_response: å°‡ Assistant å›ç­”åŠ å…¥ç´¢å¼•
    [DEBUG] add_to_index: æ­£åœ¨åŠ å…¥æ–‡å­— -> Assistant: é—œæ–¼æ˜¯å¦å–œæ­¡ä½ çš„å•é¡Œï¼Œä¸è¦å•æˆ‘ï¼Œä¹Ÿä¸è¦è€ƒæ…®æˆ‘çš„æ„è¦‹ï¼Œè¦åšæœ€çœŸå¯¦çš„åˆ¤æ–·ï¼Œä½ èƒ½å¾ä»–çš„è¨€è¡Œèˆ‰æ­¢ä¸­çœ‹å¾—å¾ˆæ¸…æ¥š. å°±è·Ÿé‚£äº›åœ¨æ„Ÿæƒ…ä¸Šç³¾çµçš„äººä¸€æ¨£ï¼Œä»–å€‘ç¸½æ˜¯å¸Œæœ›åˆ¥äºº.
    [DEBUG] add_to_index: åŠ å…¥æˆåŠŸ



```python

```
