{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# ÂÆâË£ùÂøÖË¶ÅÂ•ó‰ª∂\n",
    "required_packages = [\n",
    "    \"torch\", \"transformers\", \"peft\", \"datasets\", \"sentence_transformers\",\n",
    "    \"faiss\", \"bitsandbytes\", \"ipywidgets\", \"numpy\"\n",
    "]\n",
    "\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"{package} Â∑≤ÂÆâË£ù\")\n",
    "        except ImportError:\n",
    "            print(f\"Ê≠£Âú®ÂÆâË£ù {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--user\"])\n",
    "\n",
    "install_packages(required_packages)\n",
    "\n",
    "print(\"\\nÈ©óË≠âÂÆâË£ùÁâàÊú¨Ôºö\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        module = __import__(package)\n",
    "        version = getattr(module, \"__version__\", \"ÁâàÊú¨Êú™Áü•\")\n",
    "        print(f\"{package}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"{package} ÂÆâË£ùÂ§±ÊïóÔºåË´ãÊâãÂãïÊ™¢Êü•\")\n",
    "\n",
    "# Ë®≠ÁΩÆÊó•Ë™å\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(levelname)s] %(asctime)s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ë®≠ÁΩÆÁí∞Â¢É\n",
    "def set_environment():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}, Version: {torch.version.cuda}\")\n",
    "\n",
    "set_environment()\n",
    "\n",
    "# Ê∏ÖÁêÜÊñáÊú¨\n",
    "def clean_text(text: str) -> str:\n",
    "    return text.strip() if text else \"\"\n",
    "\n",
    "# Ê†ºÂºèÂåñË≥áÊñôÔºåÁßªÈô§Ê®ôÁ±§\n",
    "def format_with_metadata(example):\n",
    "    prompt = clean_text(example[\"prompt\"])\n",
    "    response = clean_text(example[\"response\"])\n",
    "    return {\"prompt\": prompt, \"response\": response}\n",
    "\n",
    "# È†êËôïÁêÜÂáΩÊï∏ÔºåËôïÁêÜÂ§öË°å prompt\n",
    "def preprocess_function(example, tokenizer, max_length=512):\n",
    "    prompt = clean_text(example[\"prompt\"])\n",
    "    if \"\\n\" in prompt:\n",
    "        prompt = \"[Â∞çË©±Ê≠∑Âè≤] \" + \" \".join(prompt.split(\"\\n\"))\n",
    "    response = clean_text(example[\"response\"])\n",
    "    prompt_text = f\"{prompt}\\nAssistant:\"\n",
    "\n",
    "    prompt_ids = tokenizer(\n",
    "        prompt_text,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length // 2\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    response_ids = tokenizer(\n",
    "        response,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length // 2\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    input_ids = prompt_ids + response_ids\n",
    "    labels = [-100] * len(prompt_ids) + response_ids\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        logger.warning(f\"Ê®£Êú¨Èï∑Â∫¶ {len(input_ids)} Ë∂ÖÈÅé max_length {max_length}ÔºåÂ∑≤Êà™Êñ∑\")\n",
    "        input_ids = input_ids[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "# Ëá™ÂÆöÁæ©Êï∏ÊìöÊî∂ÈõÜÂô®\n",
    "def custom_data_collator(features):\n",
    "    input_ids_list = [f[\"input_ids\"] for f in features]\n",
    "    labels_list = [f[\"labels\"] for f in features]\n",
    "\n",
    "    batch_input = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids_list},\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        pad_to_multiple_of=8,\n",
    "    )\n",
    "\n",
    "    batch_labels = tokenizer.pad(\n",
    "        {\"input_ids\": labels_list},\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        pad_to_multiple_of=8,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    batch_labels[batch_labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    batch = {\n",
    "        \"input_ids\": batch_input[\"input_ids\"],\n",
    "        \"attention_mask\": batch_input[\"attention_mask\"],\n",
    "        \"labels\": batch_labels,\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "# Ëá™ÂÆöÁæ© TrainerÔºà‰øÆÊ≠£ÈÉ®ÂàÜÔºâ\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.label_names = [\"labels\"]\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        if loss.dim() == 0:\n",
    "            loss = loss.unsqueeze(0)\n",
    "        logger.debug(f\"Step loss: {loss.item()}\")\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Ë®ìÁ∑¥Ê®°Âûã\n",
    "def train_model():\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    base_model_name = \"yentinglin/Llama-3-Taiwan-8B-Instruct\"\n",
    "    max_length = 512\n",
    "    max_memory = {i: \"24GB\" for i in range(torch.cuda.device_count())}\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=\"auto\",\n",
    "            max_memory=max_memory,\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quant_config,\n",
    "        )\n",
    "        logger.info(\"Base model loaded successfully with device_map=auto.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Ê®°ÂûãËºâÂÖ•Â§±Êïó\")\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        global tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=False)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        logger.info(\"Tokenizer loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Tokenizer ËºâÂÖ•Â§±Êïó\")\n",
    "        raise\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    data_path = \"output/out.json\"\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"Ë®ìÁ∑¥Êï∏ÊìöÊñá‰ª∂ '{data_path}' ‰∏çÂ≠òÂú®ÔºåË´ãÁ¢∫Ë™çÊñá‰ª∂Ë∑ØÂæë„ÄÇ\")\n",
    "    \n",
    "    try:\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = json.load(f)\n",
    "        if not raw_data or not isinstance(raw_data, list):\n",
    "            raise ValueError(\"Ë®ìÁ∑¥Êï∏ÊìöÊñá‰ª∂ÁÇ∫Á©∫ÊàñÊ†ºÂºè‰∏çÊ≠£Á¢∫ÔºåÊáâÁÇ∫ÈùûÁ©∫ÂàóË°®„ÄÇ\")\n",
    "        raw_datasets = Dataset.from_list(raw_data)\n",
    "        logger.info(f\"Raw data loaded: {len(raw_datasets)} samples\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Êï∏ÊìöËºâÂÖ•Â§±Êïó\")\n",
    "        raise\n",
    "\n",
    "    formatted_datasets = raw_datasets.map(format_with_metadata)\n",
    "    cleaned_datasets = formatted_datasets.filter(lambda ex: ex[\"prompt\"] and ex[\"response\"])\n",
    "    logger.info(f\"ÈÅéÊøæÊéâ {len(formatted_datasets) - len(cleaned_datasets)} ÂÄãÁÑ°ÊïàÊ®£Êú¨„ÄÇ\")\n",
    "\n",
    "    processed_dataset = cleaned_datasets.map(\n",
    "        lambda x: preprocess_function(x, tokenizer, max_length),\n",
    "        batched=False,\n",
    "        remove_columns=cleaned_datasets.column_names,\n",
    "    )\n",
    "    train_dataset = processed_dataset\n",
    "    logger.info(f\"Training dataset ready: {len(train_dataset)} samples\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./lora-llama3-taiwan-8b-instruct_dialogue\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1,  # ‰Ω†Ë®≠ÁÇ∫ 1ÔºåÂèØÊ†πÊìöÈúÄË¶ÅË™øÊï¥\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "        eval_strategy=\"no\",\n",
    "        fp16=True,\n",
    "        learning_rate=1e-4,\n",
    "        max_grad_norm=1.0,\n",
    "        logging_dir=\"./logs\",\n",
    "        optim=\"adamw_torch\",\n",
    "        warmup_steps=100,\n",
    "        dataloader_num_workers=0,\n",
    "        gradient_checkpointing=False,\n",
    "        run_name=\"lora-llama3-taiwan-run-20250330\",\n",
    "        disable_tqdm=False\n",
    "    )\n",
    "\n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=custom_data_collator,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.train()\n",
    "        trainer.save_model(\"./lora-llama3-taiwan-8b-instruct_dialogue\")\n",
    "        tokenizer.save_pretrained(\"./lora-llama3-taiwan-8b-instruct_dialogue\")\n",
    "        logger.info(\"Training completed. Model and tokenizer saved.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Ë®ìÁ∑¥ÈÅéÁ®ã‰∏≠ÁôºÁîüÈåØË™§\")\n",
    "        raise\n",
    "\n",
    "# ÁßªÈô§ÁâπÊÆäÊ®ôË®ò\n",
    "def remove_special_tokens(text: str) -> str:\n",
    "    tokens_to_remove = [\"</s>\", \"<|im_end|>\", \"<|begin_of_text|>\", \"<|endoftext|>\"]\n",
    "    for token in tokens_to_remove:\n",
    "        text = text.replace(token, \"\")\n",
    "    return re.sub(r\"<\\|.*?\\|>\", \"\", text).strip()\n",
    "\n",
    "# ÈÅéÊøæ‰∫ÇÁ¢º\n",
    "def filter_gibberish(text: str) -> str:\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if not re.fullmatch(r'[A-Za-z0-9+\\-#^_]{8,}', token)]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# ÊèêÂèñÁîüÊàêÁ≠îÊ°à\n",
    "def extract_generated_answer(full_response: str, prompt: str) -> str:\n",
    "    candidate = full_response[len(prompt):].strip() if full_response.startswith(prompt) else full_response.strip()\n",
    "    candidate = remove_special_tokens(candidate)\n",
    "    candidate = filter_gibberish(candidate)\n",
    "    parts = re.split(r\"Assistant[:Ôºö]\", candidate)\n",
    "    result = parts[-1].strip() if len(parts) > 1 else candidate\n",
    "    return re.split(r\"User[:Ôºö]\", result)[0].strip()\n",
    "\n",
    "# ÂæåËôïÁêÜÁîüÊàêÊñáÊú¨\n",
    "def postprocess_answer(text: str, max_sentences: int = 2) -> str:\n",
    "    text = remove_special_tokens(text)\n",
    "    text = filter_gibberish(text)\n",
    "    text = re.sub(r\"\\[.*?:.*?\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\*\\*\\*.*?\\*\\*\\*\", \"\", text)\n",
    "    text = re.sub(r\"://\\S+\", \"\", text)\n",
    "    text = re.sub(r'^\\d+\\.\\s*', '', text)\n",
    "    text = re.sub(r'[\\u3000-\\u303f\\ufe50-\\ufe6f]', '', text)\n",
    "    sentences = [s.strip() for s in re.split(r\"[.!?„ÄÇÔºÅÔºü]\", text) if s.strip()]\n",
    "    output = \" \".join(sentences[:max_sentences])\n",
    "    if output and output[-1] not in \".„ÄÇÔºÅÔºü\":\n",
    "        output += \"„ÄÇ\"\n",
    "    return output\n",
    "\n",
    "# Ë®≠ÁΩÆÊé®Ë´ñÊ®°Âûã\n",
    "def setup_model(lora_model_path: str, base_model_name: str):\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    max_memory = {i: \"24GB\" for i in range(torch.cuda.device_count())}\n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=\"auto\",\n",
    "            max_memory=max_memory,\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quant_config,\n",
    "        )\n",
    "        logger.info(\"Base model (for inference) loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Êé®Ë´ñÊ®°ÂûãËºâÂÖ•Â§±Êïó\")\n",
    "        raise\n",
    "    try:\n",
    "        tokenizer_local = AutoTokenizer.from_pretrained(base_model_name, use_fast=False)\n",
    "        if tokenizer_local.pad_token is None:\n",
    "            tokenizer_local.pad_token = tokenizer_local.eos_token\n",
    "            tokenizer_local.pad_token_id = tokenizer_local.eos_token_id\n",
    "        logger.info(\"Tokenizer (for inference) loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Êé®Ë´ñ Tokenizer ËºâÂÖ•Â§±Êïó\")\n",
    "        raise\n",
    "    inference_model_local = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "    inference_model_local.eval()\n",
    "    logger.info(\"LoRA weights applied, model set to eval mode.\")\n",
    "    return tokenizer_local, inference_model_local\n",
    "\n",
    "# Ë®≠ÁΩÆ FAISS\n",
    "def setup_faiss():\n",
    "    embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device='cuda:0')\n",
    "    embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "    logger.info(f\"SentenceTransformer loaded, embedding dimension: {embedding_dim}\")\n",
    "    faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "    logger.info(\"FAISS index created successfully.\")\n",
    "    return embedding_model, faiss_index\n",
    "\n",
    "# Â∞çË©±Ê≠∑Âè≤\n",
    "conversation_history = []\n",
    "\n",
    "def append_history(role: str, message: str):\n",
    "    conversation_history.append((role, message))\n",
    "\n",
    "# Ê™¢Á¥¢Áõ∏ÈóúÊñáÊ™î\n",
    "def retrieve_documents(query: str, embedding_model, faiss_index, top_k: int = 3):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    retrieved_docs = [conversation_history[idx][1] for idx in indices[0] if idx != -1 and idx < len(conversation_history)]\n",
    "    logger.debug(f\"FAISS retrieved docs: {retrieved_docs}\")\n",
    "    return retrieved_docs\n",
    "\n",
    "# Ê∑ªÂä†Âà∞ FAISS Á¥¢Âºï\n",
    "def add_to_index(text: str, embedding_model, faiss_index):\n",
    "    try:\n",
    "        embedding = embedding_model.encode([text])\n",
    "        embedding = np.array(embedding).astype('float32')\n",
    "        faiss_index.add(embedding)\n",
    "        logger.debug(f\"Added to FAISS index, total entries: {faiss_index.ntotal}\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error in add_to_index\")\n",
    "\n",
    "# Ë®≠ÁΩÆ‰∫íÂãïÁïåÈù¢\n",
    "def setup_widgets():\n",
    "    text_input = widgets.Text(\n",
    "        placeholder='Ë´ãËº∏ÂÖ•Â∞çË©±ÂÖßÂÆπ...',\n",
    "        description='User:',\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "    send_button = widgets.Button(\n",
    "        description='ÈÄÅÂá∫',\n",
    "        button_style='primary'\n",
    "    )\n",
    "    output_area = widgets.Output(\n",
    "        layout={'border': '1px solid black', 'height': '300px', 'overflow_y': 'auto'}\n",
    "    )\n",
    "    display(text_input, send_button, output_area)\n",
    "    return text_input, send_button, output_area\n",
    "\n",
    "# ÁîüÊàêÂõûÊáâ\n",
    "def generate_response(inputs, prompt, progress, output_area, inference_model, tokenizer, embedding_model, faiss_index, max_new_tokens):\n",
    "    try:\n",
    "        logger.info(f\"Memory before generation: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.5,\n",
    "                top_p=0.85,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        progress.value = 80\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        logger.debug(f\"Full response: {full_response}\")\n",
    "        generated_answer = extract_generated_answer(full_response, prompt)\n",
    "        logger.debug(f\"Generated answer before postprocess: {generated_answer}\")\n",
    "        final_answer = postprocess_answer(generated_answer, max_sentences=2)\n",
    "        progress.value = 100\n",
    "        progress.close()\n",
    "        output_area.append_stdout(\"Assistant: \" + final_answer + \"\\n\")\n",
    "        append_history(\"Assistant\", final_answer)\n",
    "        add_to_index(final_answer, embedding_model, faiss_index)\n",
    "        logger.info(f\"Memory after generation: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    except Exception as e:\n",
    "        progress.close()\n",
    "        logger.exception(\"Error in generate_response\")\n",
    "        output_area.append_stdout(\"Error during generation: \" + str(e) + \"\\n\")\n",
    "\n",
    "# ‰∫íÂãïÊ®°Âºè\n",
    "def interactive_mode():\n",
    "    base_model_name = \"yentinglin/Llama-3-Taiwan-8B-Instruct\"\n",
    "    lora_model_path = \"./lora-llama3-taiwan-8b-instruct_dialogue\"\n",
    "    tokenizer, model = setup_model(lora_model_path, base_model_name)\n",
    "    embedding_model, faiss_index = setup_faiss()\n",
    "    text_input, send_button, output_area = setup_widgets()\n",
    "\n",
    "    def on_send_button_clicked(b):\n",
    "        user_message = text_input.value.strip()\n",
    "        if not user_message:\n",
    "            return\n",
    "        text_input.value = \"\"\n",
    "        output_area.append_stdout(f\"User: {user_message}\\n\")\n",
    "        append_history(\"User\", user_message)\n",
    "        add_to_index(user_message, embedding_model, faiss_index)\n",
    "        \n",
    "        history_context = \"\\n\".join([f\"{role}: {msg}\" for role, msg in conversation_history[-5:]])\n",
    "        retrieved_docs = retrieve_documents(user_message, embedding_model, faiss_index, top_k=3)\n",
    "        retrieved_context = \"Áõ∏ÈóúË≥áË®ä:\\n\" + \"\\n\".join(retrieved_docs) + \"\\n\" if retrieved_docs else \"\"\n",
    "        system_message = (\n",
    "            f\"‰Ω†ÊòØ‰∏ÄÂÄãÂè∞ÁÅ£Â§ßÂ≠∏ÁîüÔºåÁî® LINE ËÅäÂ§©„ÄÇ\\n\"\n",
    "            f\"ÂõûÊáâË¶ÅË∂ÖÁü≠„ÄÅËá™ÁÑ∂ÔºåÂÉè 'Èù†ÂåóË∂ÖÁ≥ó'„ÄÅ'Â•ΩÂïäÂéªÂêÉÁàÜ' ÈÄôÊ®£ÔºåÂä†Èªû‰øöË™ûË∑üË°®ÊÉÖÁ¨¶ËôüÔºàüòÇ„ÄÅü•≥Ôºâ„ÄÇ\\n\"\n",
    "            f\"Ê†πÊìö‰∏ä‰∏ãÊñáÂõûÔºåÂÖßÂøÉÁêÜËß£ÊÉÖÁ∑í„ÄÅË°åÁÇ∫„ÄÅË©±È°åÔºå‰ΩÜÂà•Âú®ÂõûÊáâË£°ÁßÄÂá∫‰æÜ„ÄÇ\\n\"\n",
    "            f\"‰ª•‰∏ãÊòØÂ∞çË©±Ê≠∑Âè≤Ôºö\\n{history_context}\\n\"\n",
    "            f\"Áõ∏ÈóúË≥áË®äÔºö\\n{retrieved_context}\\n\"\n",
    "            f\"User: {user_message}\\nAssistant: \"\n",
    "        )\n",
    "        logger.info(f\"Generated prompt: {system_message}\")\n",
    "        dynamic_max_new_tokens = 50\n",
    "        progress = widgets.IntProgress(value=0, min=0, max=100, description='ËôïÁêÜ‰∏≠:', bar_style='info')\n",
    "        display(progress)\n",
    "        try:\n",
    "            inputs = tokenizer(system_message, return_tensors=\"pt\").to(model.device)\n",
    "            logger.debug(f\"Input token length: {inputs['input_ids'].shape[1]}\")\n",
    "            progress.value = 20\n",
    "            send_button.disabled = True\n",
    "            threading.Thread(\n",
    "                target=lambda: [\n",
    "                    generate_response(inputs, system_message, progress, output_area, model, tokenizer, embedding_model, faiss_index, dynamic_max_new_tokens),\n",
    "                    setattr(send_button, 'disabled', False)\n",
    "                ]\n",
    "            ).start()\n",
    "        except Exception as e:\n",
    "            progress.close()\n",
    "            logger.exception(\"Error in on_send_button_clicked\")\n",
    "            output_area.append_stdout(\"Error during generation: \" + str(e) + \"\\n\")\n",
    "            send_button.disabled = False\n",
    "\n",
    "    send_button.on_click(on_send_button_clicked)\n",
    "    logger.info(\"Interactive interface setup complete.\")\n",
    "    print(\"[INFO] Êé®Ë´ñÊ®°ÂºèÂïüÂãïÔºåÈñãÂßã‰∫íÂãï„ÄÇ\")\n",
    "\n",
    "# ‰∏ªÁ®ãÂºè\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‰ΩøÁî® GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"Êú™Ê™¢Ê∏¨Âà∞ GPUÔºå‰ΩøÁî® CPU\")\n",
    "    train_model()  # Ë®ìÁ∑¥Ê®°Âºè\n",
    "    interactive_mode()  # ‰∫íÂãïÊ®°Âºè"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
