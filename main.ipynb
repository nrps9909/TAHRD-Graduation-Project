{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import threading\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# 安裝必要套件\n",
    "required_packages = [\n",
    "    \"torch\", \"transformers\", \"peft\", \"datasets\", \"sentence_transformers\",\n",
    "    \"faiss\", \"bitsandbytes\", \"ipywidgets\", \"numpy\"\n",
    "]\n",
    "\n",
    "def install_packages(packages):\n",
    "    for package in packages:\n",
    "        try:\n",
    "            __import__(package)\n",
    "            print(f\"{package} 已安裝\")\n",
    "        except ImportError:\n",
    "            print(f\"正在安裝 {package}...\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--user\"])\n",
    "\n",
    "install_packages(required_packages)\n",
    "\n",
    "print(\"\\n驗證安裝版本：\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        module = __import__(package)\n",
    "        version = getattr(module, \"__version__\", \"版本未知\")\n",
    "        print(f\"{package}: {version}\")\n",
    "    except ImportError:\n",
    "        print(f\"{package} 安裝失敗，請手動檢查\")\n",
    "\n",
    "# 設置日誌\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='[%(levelname)s] %(asctime)s %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 設置環境\n",
    "def set_environment():\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    logger.info(f\"CUDA available: {torch.cuda.is_available()}, Version: {torch.version.cuda}\")\n",
    "\n",
    "set_environment()\n",
    "\n",
    "# 清理文本\n",
    "def clean_text(text: str) -> str:\n",
    "    return text.strip() if text else \"\"\n",
    "\n",
    "# 格式化資料，移除標籤\n",
    "def format_with_metadata(example):\n",
    "    prompt = clean_text(example[\"prompt\"])\n",
    "    response = clean_text(example[\"response\"])\n",
    "    return {\"prompt\": prompt, \"response\": response}\n",
    "\n",
    "# 預處理函數，處理多行 prompt\n",
    "def preprocess_function(example, tokenizer, max_length=512):\n",
    "    prompt = clean_text(example[\"prompt\"])\n",
    "    if \"\\n\" in prompt:\n",
    "        prompt = \"[對話歷史] \" + \" \".join(prompt.split(\"\\n\"))\n",
    "    response = clean_text(example[\"response\"])\n",
    "    prompt_text = f\"{prompt}\\nAssistant:\"\n",
    "\n",
    "    prompt_ids = tokenizer(\n",
    "        prompt_text,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length // 2\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    response_ids = tokenizer(\n",
    "        response,\n",
    "        add_special_tokens=False,\n",
    "        truncation=True,\n",
    "        max_length=max_length // 2\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    input_ids = prompt_ids + response_ids\n",
    "    labels = [-100] * len(prompt_ids) + response_ids\n",
    "\n",
    "    if len(input_ids) > max_length:\n",
    "        logger.warning(f\"樣本長度 {len(input_ids)} 超過 max_length {max_length}，已截斷\")\n",
    "        input_ids = input_ids[:max_length]\n",
    "        labels = labels[:max_length]\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "# 自定義數據收集器\n",
    "def custom_data_collator(features):\n",
    "    input_ids_list = [f[\"input_ids\"] for f in features]\n",
    "    labels_list = [f[\"labels\"] for f in features]\n",
    "\n",
    "    batch_input = tokenizer.pad(\n",
    "        {\"input_ids\": input_ids_list},\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        pad_to_multiple_of=8,\n",
    "    )\n",
    "\n",
    "    batch_labels = tokenizer.pad(\n",
    "        {\"input_ids\": labels_list},\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        pad_to_multiple_of=8,\n",
    "    )[\"input_ids\"]\n",
    "\n",
    "    batch_labels[batch_labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    batch = {\n",
    "        \"input_ids\": batch_input[\"input_ids\"],\n",
    "        \"attention_mask\": batch_input[\"attention_mask\"],\n",
    "        \"labels\": batch_labels,\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "# 自定義 Trainer（修正部分）\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.label_names = [\"labels\"]\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        if loss.dim() == 0:\n",
    "            loss = loss.unsqueeze(0)\n",
    "        logger.debug(f\"Step loss: {loss.item()}\")\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# 訓練模型\n",
    "def train_model():\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    base_model_name = \"yentinglin/Llama-3-Taiwan-8B-Instruct\"\n",
    "    max_length = 512\n",
    "    max_memory = {i: \"24GB\" for i in range(torch.cuda.device_count())}\n",
    "\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=\"auto\",\n",
    "            max_memory=max_memory,\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quant_config,\n",
    "        )\n",
    "        logger.info(\"Base model loaded successfully with device_map=auto.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"模型載入失敗\")\n",
    "        raise\n",
    "    \n",
    "    try:\n",
    "        global tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=False)\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        logger.info(\"Tokenizer loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Tokenizer 載入失敗\")\n",
    "        raise\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    data_path = \"output/out.json\"\n",
    "    if not os.path.exists(data_path):\n",
    "        raise FileNotFoundError(f\"訓練數據文件 '{data_path}' 不存在，請確認文件路徑。\")\n",
    "    \n",
    "    try:\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_data = json.load(f)\n",
    "        if not raw_data or not isinstance(raw_data, list):\n",
    "            raise ValueError(\"訓練數據文件為空或格式不正確，應為非空列表。\")\n",
    "        raw_datasets = Dataset.from_list(raw_data)\n",
    "        logger.info(f\"Raw data loaded: {len(raw_datasets)} samples\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"數據載入失敗\")\n",
    "        raise\n",
    "\n",
    "    formatted_datasets = raw_datasets.map(format_with_metadata)\n",
    "    cleaned_datasets = formatted_datasets.filter(lambda ex: ex[\"prompt\"] and ex[\"response\"])\n",
    "    logger.info(f\"過濾掉 {len(formatted_datasets) - len(cleaned_datasets)} 個無效樣本。\")\n",
    "\n",
    "    processed_dataset = cleaned_datasets.map(\n",
    "        lambda x: preprocess_function(x, tokenizer, max_length),\n",
    "        batched=False,\n",
    "        remove_columns=cleaned_datasets.column_names,\n",
    "    )\n",
    "    train_dataset = processed_dataset\n",
    "    logger.info(f\"Training dataset ready: {len(train_dataset)} samples\")\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./lora-llama3-taiwan-8b-instruct_dialogue\",\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=1,  # 你設為 1，可根據需要調整\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        logging_steps=50,\n",
    "        save_steps=500,\n",
    "        eval_strategy=\"no\",\n",
    "        fp16=True,\n",
    "        learning_rate=1e-4,\n",
    "        max_grad_norm=1.0,\n",
    "        logging_dir=\"./logs\",\n",
    "        optim=\"adamw_torch\",\n",
    "        warmup_steps=100,\n",
    "        dataloader_num_workers=0,\n",
    "        gradient_checkpointing=False,\n",
    "        run_name=\"lora-llama3-taiwan-run-20250330\",\n",
    "        disable_tqdm=False\n",
    "    )\n",
    "\n",
    "    trainer = MyTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=custom_data_collator,\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        trainer.train()\n",
    "        trainer.save_model(\"./lora-llama3-taiwan-8b-instruct_dialogue\")\n",
    "        tokenizer.save_pretrained(\"./lora-llama3-taiwan-8b-instruct_dialogue\")\n",
    "        logger.info(\"Training completed. Model and tokenizer saved.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"訓練過程中發生錯誤\")\n",
    "        raise\n",
    "\n",
    "# 移除特殊標記\n",
    "def remove_special_tokens(text: str) -> str:\n",
    "    tokens_to_remove = [\"</s>\", \"<|im_end|>\", \"<|begin_of_text|>\", \"<|endoftext|>\"]\n",
    "    for token in tokens_to_remove:\n",
    "        text = text.replace(token, \"\")\n",
    "    return re.sub(r\"<\\|.*?\\|>\", \"\", text).strip()\n",
    "\n",
    "# 過濾亂碼\n",
    "def filter_gibberish(text: str) -> str:\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if not re.fullmatch(r'[A-Za-z0-9+\\-#^_]{8,}', token)]\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "# 提取生成答案\n",
    "def extract_generated_answer(full_response: str, prompt: str) -> str:\n",
    "    candidate = full_response[len(prompt):].strip() if full_response.startswith(prompt) else full_response.strip()\n",
    "    candidate = remove_special_tokens(candidate)\n",
    "    candidate = filter_gibberish(candidate)\n",
    "    parts = re.split(r\"Assistant[:：]\", candidate)\n",
    "    result = parts[-1].strip() if len(parts) > 1 else candidate\n",
    "    return re.split(r\"User[:：]\", result)[0].strip()\n",
    "\n",
    "# 後處理生成文本\n",
    "def postprocess_answer(text: str, max_sentences: int = 2) -> str:\n",
    "    text = remove_special_tokens(text)\n",
    "    text = filter_gibberish(text)\n",
    "    text = re.sub(r\"\\[.*?:.*?\\]\", \"\", text)\n",
    "    text = re.sub(r\"\\*\\*\\*.*?\\*\\*\\*\", \"\", text)\n",
    "    text = re.sub(r\"://\\S+\", \"\", text)\n",
    "    text = re.sub(r'^\\d+\\.\\s*', '', text)\n",
    "    text = re.sub(r'[\\u3000-\\u303f\\ufe50-\\ufe6f]', '', text)\n",
    "    sentences = [s.strip() for s in re.split(r\"[.!?。！？]\", text) if s.strip()]\n",
    "    output = \" \".join(sentences[:max_sentences])\n",
    "    if output and output[-1] not in \".。！？\":\n",
    "        output += \"。\"\n",
    "    return output\n",
    "\n",
    "# 設置推論模型\n",
    "def setup_model(lora_model_path: str, base_model_name: str):\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    max_memory = {i: \"24GB\" for i in range(torch.cuda.device_count())}\n",
    "    try:\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=\"auto\",\n",
    "            max_memory=max_memory,\n",
    "            torch_dtype=torch.float16,\n",
    "            quantization_config=quant_config,\n",
    "        )\n",
    "        logger.info(\"Base model (for inference) loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"推論模型載入失敗\")\n",
    "        raise\n",
    "    try:\n",
    "        tokenizer_local = AutoTokenizer.from_pretrained(base_model_name, use_fast=False)\n",
    "        if tokenizer_local.pad_token is None:\n",
    "            tokenizer_local.pad_token = tokenizer_local.eos_token\n",
    "            tokenizer_local.pad_token_id = tokenizer_local.eos_token_id\n",
    "        logger.info(\"Tokenizer (for inference) loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"推論 Tokenizer 載入失敗\")\n",
    "        raise\n",
    "    inference_model_local = PeftModel.from_pretrained(base_model, lora_model_path)\n",
    "    inference_model_local.eval()\n",
    "    logger.info(\"LoRA weights applied, model set to eval mode.\")\n",
    "    return tokenizer_local, inference_model_local\n",
    "\n",
    "# 設置 FAISS\n",
    "def setup_faiss():\n",
    "    embedding_model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device='cuda:0')\n",
    "    embedding_dim = embedding_model.get_sentence_embedding_dimension()\n",
    "    logger.info(f\"SentenceTransformer loaded, embedding dimension: {embedding_dim}\")\n",
    "    faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
    "    logger.info(\"FAISS index created successfully.\")\n",
    "    return embedding_model, faiss_index\n",
    "\n",
    "# 對話歷史\n",
    "conversation_history = []\n",
    "\n",
    "def append_history(role: str, message: str):\n",
    "    conversation_history.append((role, message))\n",
    "\n",
    "# 檢索相關文檔\n",
    "def retrieve_documents(query: str, embedding_model, faiss_index, top_k: int = 3):\n",
    "    query_embedding = embedding_model.encode([query])\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    distances, indices = faiss_index.search(query_embedding, top_k)\n",
    "    retrieved_docs = [conversation_history[idx][1] for idx in indices[0] if idx != -1 and idx < len(conversation_history)]\n",
    "    logger.debug(f\"FAISS retrieved docs: {retrieved_docs}\")\n",
    "    return retrieved_docs\n",
    "\n",
    "# 添加到 FAISS 索引\n",
    "def add_to_index(text: str, embedding_model, faiss_index):\n",
    "    try:\n",
    "        embedding = embedding_model.encode([text])\n",
    "        embedding = np.array(embedding).astype('float32')\n",
    "        faiss_index.add(embedding)\n",
    "        logger.debug(f\"Added to FAISS index, total entries: {faiss_index.ntotal}\")\n",
    "    except Exception as e:\n",
    "        logger.exception(\"Error in add_to_index\")\n",
    "\n",
    "# 設置互動界面\n",
    "def setup_widgets():\n",
    "    text_input = widgets.Text(\n",
    "        placeholder='請輸入對話內容...',\n",
    "        description='User:',\n",
    "        layout=widgets.Layout(width='80%')\n",
    "    )\n",
    "    send_button = widgets.Button(\n",
    "        description='送出',\n",
    "        button_style='primary'\n",
    "    )\n",
    "    output_area = widgets.Output(\n",
    "        layout={'border': '1px solid black', 'height': '300px', 'overflow_y': 'auto'}\n",
    "    )\n",
    "    display(text_input, send_button, output_area)\n",
    "    return text_input, send_button, output_area\n",
    "\n",
    "# 生成回應\n",
    "def generate_response(inputs, prompt, progress, output_area, inference_model, tokenizer, embedding_model, faiss_index, max_new_tokens):\n",
    "    try:\n",
    "        logger.info(f\"Memory before generation: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "        with torch.no_grad():\n",
    "            outputs = inference_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.5,\n",
    "                top_p=0.85,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.2,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                use_cache=True,\n",
    "            )\n",
    "        progress.value = 80\n",
    "        full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        logger.debug(f\"Full response: {full_response}\")\n",
    "        generated_answer = extract_generated_answer(full_response, prompt)\n",
    "        logger.debug(f\"Generated answer before postprocess: {generated_answer}\")\n",
    "        final_answer = postprocess_answer(generated_answer, max_sentences=2)\n",
    "        progress.value = 100\n",
    "        progress.close()\n",
    "        output_area.append_stdout(\"Assistant: \" + final_answer + \"\\n\")\n",
    "        append_history(\"Assistant\", final_answer)\n",
    "        add_to_index(final_answer, embedding_model, faiss_index)\n",
    "        logger.info(f\"Memory after generation: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    except Exception as e:\n",
    "        progress.close()\n",
    "        logger.exception(\"Error in generate_response\")\n",
    "        output_area.append_stdout(\"Error during generation: \" + str(e) + \"\\n\")\n",
    "\n",
    "# 互動模式\n",
    "def interactive_mode():\n",
    "    base_model_name = \"yentinglin/Llama-3-Taiwan-8B-Instruct\"\n",
    "    lora_model_path = \"./lora-llama3-taiwan-8b-instruct_dialogue\"\n",
    "    tokenizer, model = setup_model(lora_model_path, base_model_name)\n",
    "    embedding_model, faiss_index = setup_faiss()\n",
    "    text_input, send_button, output_area = setup_widgets()\n",
    "\n",
    "    def on_send_button_clicked(b):\n",
    "        user_message = text_input.value.strip()\n",
    "        if not user_message:\n",
    "            return\n",
    "        text_input.value = \"\"\n",
    "        output_area.append_stdout(f\"User: {user_message}\\n\")\n",
    "        append_history(\"User\", user_message)\n",
    "        add_to_index(user_message, embedding_model, faiss_index)\n",
    "        \n",
    "        history_context = \"\\n\".join([f\"{role}: {msg}\" for role, msg in conversation_history[-5:]])\n",
    "        retrieved_docs = retrieve_documents(user_message, embedding_model, faiss_index, top_k=3)\n",
    "        retrieved_context = \"相關資訊:\\n\" + \"\\n\".join(retrieved_docs) + \"\\n\" if retrieved_docs else \"\"\n",
    "        system_message = (\n",
    "            f\"你是一個台灣大學生，用 LINE 聊天。\\n\"\n",
    "            f\"回應要超短、自然，像 '靠北超糗'、'好啊去吃爆' 這樣，加點俚語跟表情符號（😂、🥳）。\\n\"\n",
    "            f\"根據上下文回，內心理解情緒、行為、話題，但別在回應裡秀出來。\\n\"\n",
    "            f\"以下是對話歷史：\\n{history_context}\\n\"\n",
    "            f\"相關資訊：\\n{retrieved_context}\\n\"\n",
    "            f\"User: {user_message}\\nAssistant: \"\n",
    "        )\n",
    "        logger.info(f\"Generated prompt: {system_message}\")\n",
    "        dynamic_max_new_tokens = 50\n",
    "        progress = widgets.IntProgress(value=0, min=0, max=100, description='處理中:', bar_style='info')\n",
    "        display(progress)\n",
    "        try:\n",
    "            inputs = tokenizer(system_message, return_tensors=\"pt\").to(model.device)\n",
    "            logger.debug(f\"Input token length: {inputs['input_ids'].shape[1]}\")\n",
    "            progress.value = 20\n",
    "            send_button.disabled = True\n",
    "            threading.Thread(\n",
    "                target=lambda: [\n",
    "                    generate_response(inputs, system_message, progress, output_area, model, tokenizer, embedding_model, faiss_index, dynamic_max_new_tokens),\n",
    "                    setattr(send_button, 'disabled', False)\n",
    "                ]\n",
    "            ).start()\n",
    "        except Exception as e:\n",
    "            progress.close()\n",
    "            logger.exception(\"Error in on_send_button_clicked\")\n",
    "            output_area.append_stdout(\"Error during generation: \" + str(e) + \"\\n\")\n",
    "            send_button.disabled = False\n",
    "\n",
    "    send_button.on_click(on_send_button_clicked)\n",
    "    logger.info(\"Interactive interface setup complete.\")\n",
    "    print(\"[INFO] 推論模式啟動，開始互動。\")\n",
    "\n",
    "# 主程式\n",
    "if __name__ == \"__main__\":\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"使用 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"未檢測到 GPU，使用 CPU\")\n",
    "    train_model()  # 訓練模式\n",
    "    interactive_mode()  # 互動模式"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
