{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6935937c-3b03-413d-9681-bb6e8f72180d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 15:39:09,872 - æ­£åœ¨åˆå§‹åŒ–æƒ…æ„Ÿåˆ†é¡å™¨ï¼ˆj-hartmann/emotion-english-distilroberta-baseï¼‰...\n",
      "Device set to use cuda:0\n",
      "2025-03-28 15:39:10,642 - æ­£åœ¨åˆå§‹åŒ–é›¶æ¨£æœ¬åˆ†é¡å™¨...\n",
      "Device set to use cuda:1\n",
      "è®€å–èŠå¤©æª”æ¡ˆ:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-28 15:39:12,517 - æˆåŠŸè®€å–æª”æ¡ˆ: data/claire.txt\n",
      "è®€å–èŠå¤©æª”æ¡ˆ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 284.92it/s]\n",
      "è§£æèŠå¤©æª”æ¡ˆ:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-28 15:39:12,524 - é–‹å§‹è§£æèŠå¤©è¨˜éŒ„ (User: 0, Friend: 2)...\n",
      "\n",
      "è§£æèŠå¤©è¡Œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2014/2014 [00:00<00:00, 200449.15it/s]\n",
      "2025-03-28 15:39:12,540 - è§£æå®Œæˆï¼Œç”Ÿæˆ 116 çµ„å°è©±\n",
      "è§£æèŠå¤©æª”æ¡ˆ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 47.89it/s]\n",
      "2025-03-28 15:39:12,543 - å¾å¿«å–æª”æ¡ˆ embeddings.pt è¼‰å…¥åµŒå…¥å‘é‡...\n",
      "2025-03-28 15:39:12,547 - é–‹å§‹æ·»åŠ æƒ…ç·’æ¨™ç±¤...\n",
      "2025-03-28 15:39:12,608 - é–‹å§‹å…¨æ‰¹é‡æƒ…æ„Ÿåˆ†é¡...\n",
      "æƒ…æ„Ÿåˆ†é¡æ‰¹æ¬¡è™•ç†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:02<00:00,  4.14it/s]\n",
      "çµ„è£æƒ…æ„Ÿæ¨™ç±¤: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:00<00:00, 7498.37it/s]\n",
      "2025-03-28 15:39:15,528 - æƒ…æ„Ÿæ¨™ç±¤æ·»åŠ å®Œæˆ\n",
      "2025-03-28 15:39:15,530 - é–‹å§‹æ·»åŠ è©±é¡Œæ¨™ç±¤...\n",
      "2025-03-28 15:39:15,545 - é–‹å§‹å…¨æ‰¹é‡è©±é¡Œåˆ†é¡...\n",
      "è©±é¡Œåˆ†é¡æ‰¹æ¬¡è™•ç†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:48<00:00,  4.07s/it]\n",
      "çµ„è£è©±é¡Œæ¨™ç±¤: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:00<00:00, 872.88it/s]\n",
      "2025-03-28 15:40:04,552 - è©±é¡Œæ¨™ç±¤æ·»åŠ å®Œæˆ\n",
      "2025-03-28 15:40:04,554 - é–‹å§‹æ··åˆå°è©±...\n",
      "æ··åˆå°è©±: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1486/1486 [00:00<00:00, 906975.52it/s]\n",
      "2025-03-28 15:40:04,560 - æ··åˆå®Œæˆï¼Œç”Ÿæˆ 385 çµ„å°è©±\n",
      "2025-03-28 15:40:04,561 - é–‹å§‹æ ¼å¼åŒ–è¨“ç·´è³‡æ–™...\n",
      "2025-03-28 15:40:04,563 - é å…ˆæ‰¹é‡è¨ˆç®—æ‰€æœ‰å°è©±è¡Œç‚º...\n",
      "2025-03-28 15:40:04,578 - æ­£åœ¨å° 1486 æ¢è¨Šæ¯é€²è¡Œå°è©±è¡Œç‚ºåˆ†é¡ï¼Œæ‰¹æ¬¡å¤§å°: 128\n",
      "è¡Œç‚ºåˆ†é¡æ‰¹æ¬¡è™•ç†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [01:15<00:00,  6.30s/it]\n",
      "å¾Œè™•ç†å°è©±è¡Œç‚º: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1486/1486 [00:00<00:00, 7969.99it/s]\n",
      "æ ¼å¼åŒ–å°è©±çµ„: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 385/385 [00:00<00:00, 7741.57it/s]\n",
      "2025-03-28 15:41:20,377 - æ ¼å¼åŒ–å®Œæˆï¼Œç”Ÿæˆ 645 ç­†è¨“ç·´è³‡æ–™\n",
      "2025-03-28 15:41:20,389 - è¨“ç·´è³‡æ–™å·²å„²å­˜è‡³ output/out.jsonï¼Œç¸½å…± 645 ç­†è³‡æ–™\n",
      "2025-03-28 15:41:20,393 - å‰ 3 ç­†è¨“ç·´è³‡æ–™ç¯„ä¾‹ï¼š\n",
      "2025-03-28 15:41:20,394 - æ¨£æœ¬ 1:\n",
      "2025-03-28 15:41:20,395 - Prompt: User0: å•Šä»–åˆ°åº•æœ‰æ²’æœ‰å¥³æœ‹å‹ [æƒ…ç·’: ç„¦æ…®, è¡Œç‚º: å‘Šåˆ¥, è©±é¡Œ: äººéš›+æ„Ÿæƒ…å…«å¦]\n",
      "2025-03-28 15:41:20,397 - Response: ä»–é¡å€¼çœŸçš„å¾ˆé«˜ğŸ¥ºğŸ¥ºğŸ¥º [æƒ…ç·’: å–œæ„›, è¡Œç‚º: è®šç¾, è©±é¡Œ: äººéš›+æ„Ÿæƒ…å…«å¦]\n",
      "2025-03-28 15:41:20,398 - æ¨£æœ¬ 2:\n",
      "2025-03-28 15:41:20,399 - Prompt: User0: å•Šä»–åˆ°åº•æœ‰æ²’æœ‰å¥³æœ‹å‹ [æƒ…ç·’: ç„¦æ…®, è¡Œç‚º: å‘Šåˆ¥, è©±é¡Œ: äººéš›+æ„Ÿæƒ…å…«å¦]\n",
      "User0: ä»–é¡å€¼çœŸçš„å¾ˆé«˜ğŸ¥ºğŸ¥ºğŸ¥º [æƒ…ç·’: å–œæ„›, è¡Œç‚º: è®šç¾, è©±é¡Œ: äººéš›+æ„Ÿæƒ…å…«å¦]\n",
      "2025-03-28 15:41:20,400 - Response: é›–ç„¶æˆ‘è¦ºå¾— [æƒ…ç·’: ä¸­æ€§, è¡Œç‚º: è£œå……, è©±é¡Œ: äººéš›+æ„Ÿæƒ…å…«å¦]\n",
      "2025-03-28 15:41:20,402 - æ¨£æœ¬ 3:\n",
      "2025-03-28 15:41:20,402 - Prompt: User0: å•Šä»–åˆ°åº•æœ‰æ²’æœ‰å¥³æœ‹å‹ [æƒ…ç·’: ç„¦æ…®, è¡Œç‚º: å‘Šåˆ¥, è©±é¡Œ: äººéš›+æ„Ÿæƒ…å…«å¦]\n",
      "User0: ä»–é¡å€¼çœŸçš„å¾ˆé«˜ğŸ¥ºğŸ¥ºğŸ¥º [æƒ…ç·’: å–œæ„›, è¡Œç‚º: è®šç¾, è©±é¡Œ: äººéš›+æ„Ÿæƒ…å…«å¦]\n",
      "User0: é›–ç„¶æˆ‘è¦ºå¾— [æƒ…ç·’: ä¸­æ€§, è¡Œç‚º: è£œå……, è©±é¡Œ: äººéš›+æ„Ÿæƒ…å…«å¦]\n",
      "User0: å¾ˆå†·æ·¡ [æƒ…ç·’: é›£é, è¡Œç‚º: è¡¨é”æƒ…æ„Ÿ, è©±é¡Œ: äººéš›+æ„Ÿæƒ…å…«å¦]\n",
      "2025-03-28 15:41:20,404 - Response: ä½†ä»–æœƒç†å¤§ä¸€çš„ç”·ç”Ÿ [æƒ…ç·’: ä¸­æ€§, è¡Œç‚º: å…¶ä»–, è©±é¡Œ: äººéš›]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import logging\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "import os\n",
    "import opencc\n",
    "from functools import lru_cache\n",
    "\n",
    "# è¨­ç½®è¨˜æ†¶é«”ç®¡ç†ç’°å¢ƒè®Šæ•¸\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# è¨­ç½®æ—¥èªŒ\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# åˆå§‹åŒ–å¤š GPU ç’°å¢ƒ\n",
    "device_ids = [0, 1]  # å…©å¼µ A10G\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# å®šç¾©åˆ†é¡æ¨™ç±¤\n",
    "act_labels = [\"æå•\", \"è£œå……\", \"å¼•å…¥\", \"è¡¨é”æƒ…æ„Ÿ\", \"è®šç¾\", \"é“æ­‰\", \"æ„Ÿè¬\", \"è«‹æ±‚\", \"æ‹’çµ•\", \"å»ºè­°\", \"å•å€™\", \"å‘Šåˆ¥\", \"ç¢ºèª\", \"è¡¨é”æ„åœ–\", \"åæ§½\", \"å‚¬ä¿ƒ\", \"å®‰æ…°\", \"æ¥æ¢—\", \"å›ç­”\", \"æåŠ\", \"å…¶ä»–\"]\n",
    "topic_labels = [\"æ´»å‹•\", \"æ„Ÿæƒ…\", \"å­¸æ¥­\", \"å¨›æ¨‚\", \"äººéš›\", \"èª²æ¥­å£“åŠ›\", \"ç¤¾åœ˜æ´»å‹•\", \"æ„Ÿæƒ…å…«å¦\", \"ç”Ÿæ´»ç‘£äº‹\", \"ç§‘æŠ€\", \"å¥åº·\", \"æ—…è¡Œ\"]\n",
    "\n",
    "# åˆå§‹åŒ–ç°¡ç¹è½‰æ›å™¨\n",
    "converter_to_simplified = opencc.OpenCC('t2s')\n",
    "converter_to_traditional = opencc.OpenCC('s2t')\n",
    "\n",
    "# å‹•æ…‹æ‰¹æ¬¡å¤§å°\n",
    "def get_dynamic_batch_size(messages: List[str], base_batch_size: int = 128) -> int:\n",
    "    gpu_memory = torch.cuda.memory_reserved(device) / 1024**3\n",
    "    max_memory = 24  # A10G 24GB\n",
    "    if gpu_memory > max_memory * 0.8:\n",
    "        return max(16, base_batch_size // 2)\n",
    "    return base_batch_size\n",
    "\n",
    "# å–®é€²ç¨‹åˆ†ç‰‡è¨ˆç®—åµŒå…¥\n",
    "@lru_cache(maxsize=1)\n",
    "def load_embedder():\n",
    "    return SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "def compute_chunk(chunk: List[str], gpu_id: int, batch_size: int, embedder: SentenceTransformer) -> torch.Tensor:\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "    embedder.to(f\"cuda:{gpu_id}\")\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            embeddings = embedder.encode(chunk, convert_to_tensor=True, batch_size=batch_size, show_progress_bar=False)\n",
    "            embeddings = embeddings.to(\"cuda:0\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            return embeddings\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            logger.warning(f\"GPU {gpu_id} è¨˜æ†¶é«”ä¸è¶³ï¼Œæ¸›åŠæ‰¹æ¬¡å¤§å°é‡è©¦...\")\n",
    "            embeddings = embedder.encode(chunk, convert_to_tensor=True, batch_size=batch_size // 2, show_progress_bar=False)\n",
    "            embeddings = embeddings.to(\"cuda:0\")\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            return embeddings\n",
    "\n",
    "def precompute_embeddings(messages: List[str], num_gpus: int = 2, chunk_size: int = 5000, cache_file: str = \"embeddings.pt\") -> torch.Tensor:\n",
    "    if os.path.exists(cache_file):\n",
    "        logger.info(f\"å¾å¿«å–æª”æ¡ˆ {cache_file} è¼‰å…¥åµŒå…¥å‘é‡...\")\n",
    "        return torch.load(cache_file)\n",
    "    \n",
    "    logger.info(f\"é–‹å§‹é è¨ˆç®— {len(messages)} æ¢è¨Šæ¯çš„åµŒå…¥å‘é‡...\")\n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in messages]\n",
    "    chunks = [simplified_messages[i:i + chunk_size] for i in range(0, len(messages), chunk_size)]\n",
    "    batch_size = get_dynamic_batch_size(messages)\n",
    "    \n",
    "    embedder = load_embedder()\n",
    "    results = []\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"è¨ˆç®—åµŒå…¥åˆ†ç‰‡\")):\n",
    "        gpu_id = device_ids[i % num_gpus]\n",
    "        logger.info(f\"è™•ç†åˆ†ç‰‡ {i+1}/{len(chunks)} on GPU {gpu_id}\")\n",
    "        chunk_embeddings = compute_chunk(chunk, gpu_id, batch_size, embedder)\n",
    "        results.append(chunk_embeddings)\n",
    "    \n",
    "    embeddings = torch.cat(results, dim=0)\n",
    "    embeddings = embeddings.cpu()\n",
    "    torch.save(embeddings, cache_file)\n",
    "    logger.info(f\"åµŒå…¥è¨ˆç®—å®Œæˆï¼Œå·²å„²å­˜è‡³ {cache_file}\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return embeddings\n",
    "\n",
    "# æ¸…æ´—æ–‡æœ¬\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text or len(text.strip()) < 3 or \"[èªéŸ³è¨Šæ¯]\" in text:\n",
    "        return \"\"\n",
    "    patterns = r'(ä¸Šåˆ|ä¸‹åˆ)\\d{1,2}:\\d{2}\\s*|\\[ç…§ç‰‡\\]|\\[å½±ç‰‡\\]|\\[è²¼åœ–\\]|.*å·²æ”¶å›è¨Šæ¯.*|â˜.*|^\\*+$'\n",
    "    return re.sub(patterns, '', text).strip()\n",
    "\n",
    "# é—œéµè©åˆ†é¡ï¼ˆæ“´å±•è¡Œç‚ºèˆ‡è©±é¡Œï¼‰\n",
    "def keyword_infer_dialogue_act(message: str, prev_act: str = None) -> str:\n",
    "    message_lower = message.lower()\n",
    "    keywords = {\n",
    "        \"æå•\": ['æœ‰æ²’æœ‰', 'ä»€éº¼', 'æœƒä¸æœƒ', 'ä½ è¦ºå¾—', 'æ€éº¼', 'ç‚ºä»€éº¼', 'å—', 'å“ªå€‹', 'å¯ä¸å¯ä»¥', 'åˆ°äº†æ²’', 'å¹¾é»', 'åˆ°åº•', 'çŸ¥é“', 'å¤šå°‘', 'èª°', 'å“ªè£¡', 'å’©', 'æ¬¸ï¼Ÿ'],\n",
    "        \"è£œå……\": ['å› ç‚º', 'æ‰€ä»¥', 'é›–ç„¶', 'ä¸é', 'ä½†æ˜¯', 'ç„¶å¾Œ', 'è€Œä¸”', 'çµæœ', 'é‚„æœ‰', 'å°±', 'å…¶å¯¦', 'å‰›å‰›', 'å‡ºä¾†', 'è½åˆ°', 'å¤©æ‰', 'ä¹‹å‰'],\n",
    "        \"å¼•å…¥\": ['é‡é»æ˜¯', 'ä¸»è¦æ˜¯', 'èªªåˆ°', 'è¬›åˆ°', 'æåˆ°', 'æˆ‘ä¹Ÿä¸çŸ¥é“', 'è©±èªª'],\n",
    "        \"è¡¨é”æƒ…æ„Ÿ\": ['é›£é', 'å°·å°¬', 'è‡ªè±ª', 'éº»ç…©', 'ç·Šå¼µ', 'é–‹å¿ƒ', 'ç¬‘æ­»', 'å¸Œæœ›', 'æ€•', 'è¦ºå¾—', 'ç´¯äº†', 'å†·æ·¡', 'å¥½ç¬‘', 'é åŒ—', 'ä½ å¨˜', 'è¶…è¶•', 'è¶…å¥½ç¬‘', 'æ¨', 'æƒ³å“­', 'å•Šå•Š', 'åš‡æ­»', 'èŠä¸èµ·ä¾†'],\n",
    "        \"è®šç¾\": ['æ¼‚äº®', 'å¸¥', 'å¾ˆæ£’', 'å¾ˆè®š', 'å¾ˆå¯æ„›', 'é©åˆ', 'å¥½æœƒ', 'å¥½ä¸€é»', 'é¡å€¼', 'å¾ˆé«˜'],\n",
    "        \"é“æ­‰\": ['å°ä¸èµ·', 'æŠ±æ­‰', 'ä¸å¥½æ„æ€', 'sorry'],\n",
    "        \"æ„Ÿè¬\": ['è¬è¬', 'æ„Ÿè¬', 'æ„Ÿæ¿€', 'å¤šè¬'],\n",
    "        \"è«‹æ±‚\": ['æ‹œè¨—', 'è«‹', 'å¹«æˆ‘', 'å¯ä»¥å—'],\n",
    "        \"æ‹’çµ•\": ['ä¸è¦', 'ä¸è¡Œ', 'ä¸å¯ä»¥', 'ä¸ok', 'æ²’è¾¦æ³•', 'ä¸æœƒ', 'æˆ‘æ²’'],\n",
    "        \"å»ºè­°\": ['å»ºè­°', 'ä¸å¦‚', 'è¦ä¸è¦', 'ä¸ç„¶', 'ç­‰ç­‰å†', 'æ‡‰è©²'],\n",
    "        \"å•å€™\": ['å—¨', 'å˜¿', 'ä½ å¥½', 'æ—©å®‰'],\n",
    "        \"å‘Šåˆ¥\": ['æ°æ°', 'æ‹œæ‹œ', 'å†è¦‹', 'æ™šå®‰', 'è§£æ•£', 'å‡ºé–€', 'ä½ åˆªæ‰å•Š'],\n",
    "        \"ç¢ºèª\": ['çœŸçš„å‡çš„', 'ç¢ºå®š', 'ç¢ºèª', 'å¥½å•¦', 'æ˜¯å—', 'ä¸ç¢ºå®š'],\n",
    "        \"è¡¨é”æ„åœ–\": ['æˆ‘è¦', 'æˆ‘æƒ³', 'æˆ‘æœƒ', 'å¯èƒ½', 'ä¸å¯èƒ½'],\n",
    "        \"åæ§½\": ['ç¬‘æ­»', 'æ€éº¼å¯èƒ½', 'è¶…é†œ', 'é è…°', 'ä½ å¿«é»', 'æ™ºéšœ', 'æ‰ä½ '],\n",
    "        \"å‚¬ä¿ƒ\": ['å¿«é»', 'æœ€å¥½å¿«é»', 'è¶•ç·Š', 'ä¾›ä¸‰ä¿¡'],\n",
    "        \"å®‰æ…°\": ['ä¸æœƒå§', 'åˆæ²’å·®', 'æ²’äº‹çš„'],\n",
    "        \"æ¥æ¢—\": ['è¶…å¥½ç¬‘', 'å“ˆå“ˆå“ˆ', 'æˆ‘ä¹Ÿæ˜¯', 'å¾Œä¾†ä¹Ÿæ›'],\n",
    "        \"å›ç­”\": ['æ˜¯', 'ä¸æ˜¯', 'å°', 'éŒ¯', 'å¥½', 'å—¯', 'å–”', 'æ‡‰è©²', 'æ²’æœ‰'],\n",
    "        \"æåŠ\": ['è¨±æ©é–‹', 'é™³å§¿ä½‘', 'å‘‚å¿ è¨€', 'å¼µå°‘æ¬Š', 'æå† æ¯…', 'å°¹ä¿Šç¿”', 'è”¡', 'è³€å“¥', 'é­æ°å‡±', 'ä½™è‘¶', 'ç†å·¥ç”·']\n",
    "    }\n",
    "    # å„ªå…ˆæª¢æŸ¥æå•\n",
    "    if any(kw in message_lower for kw in keywords[\"æå•\"]):\n",
    "        return \"æå•\"\n",
    "    for act, kws in keywords.items():\n",
    "        if act != \"æå•\" and any(kw in message_lower for kw in kws):\n",
    "            return act\n",
    "    # ä¸Šä¸‹æ–‡æª¢æŸ¥\n",
    "    if prev_act == \"æå•\" and not any(kw in message_lower for kw in keywords[\"æå•\"]):\n",
    "        return \"å…¶ä»–\"\n",
    "    return \"å…¶ä»–\"\n",
    "\n",
    "# åˆå§‹åŒ–åˆ†é¡å™¨\n",
    "logger.info(\"æ­£åœ¨åˆå§‹åŒ–æƒ…æ„Ÿåˆ†é¡å™¨ï¼ˆj-hartmann/emotion-english-distilroberta-baseï¼‰...\")\n",
    "sentiment_classifier = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=\"j-hartmann/emotion-english-distilroberta-base\",\n",
    "    device=device_ids[0],\n",
    "    truncation=True,\n",
    "    max_length=128,\n",
    "    top_k=None\n",
    ")\n",
    "\n",
    "logger.info(\"æ­£åœ¨åˆå§‹åŒ–é›¶æ¨£æœ¬åˆ†é¡å™¨...\")\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=device_ids[1],\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# å°è©±è¡Œç‚ºåˆ†é¡\n",
    "def infer_dialogue_act(messages: List[str], prev_acts: List[str] = None, embeddings: torch.Tensor = None) -> List[str]:\n",
    "    if prev_acts is None:\n",
    "        prev_acts = [None] * len(messages)\n",
    "    \n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in messages]\n",
    "    batch_size = get_dynamic_batch_size(messages)\n",
    "    logger.info(f\"æ­£åœ¨å° {len(messages)} æ¢è¨Šæ¯é€²è¡Œå°è©±è¡Œç‚ºåˆ†é¡ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
    "    \n",
    "    num_batches = (len(messages) + batch_size - 1) // batch_size\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(messages), batch_size), total=num_batches, desc=\"è¡Œç‚ºåˆ†é¡æ‰¹æ¬¡è™•ç†\"):\n",
    "            batch = simplified_messages[i:i + batch_size]\n",
    "            batch_results = zero_shot_classifier(batch, act_labels, multi_label=False, batch_size=len(batch))\n",
    "            results.extend(batch_results)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    predicted_acts = []\n",
    "    for i, (message, prev_act, result) in tqdm(enumerate(zip(messages, prev_acts, results)), total=len(messages), desc=\"å¾Œè™•ç†å°è©±è¡Œç‚º\"):\n",
    "        predicted_act = result['labels'][0]\n",
    "        confidence = result['scores'][0]\n",
    "        \n",
    "        if confidence < 0.3 and embeddings is not None and i > 0:\n",
    "            similarity = util.cos_sim(embeddings[i], embeddings[i-1]).item()\n",
    "            if similarity > 0.7 and prev_acts[i-1]:\n",
    "                predicted_act = prev_acts[i-1]\n",
    "            else:\n",
    "                predicted_act = keyword_infer_dialogue_act(message, prev_act)\n",
    "        elif prev_act == \"æå•\" and predicted_act not in [\"æå•\", \"è£œå……\", \"å›ç­”\"]:\n",
    "            predicted_act = \"å…¶ä»–\"\n",
    "        \n",
    "        predicted_acts.append(predicted_act if predicted_act else \"å…¶ä»–\")\n",
    "    return predicted_acts\n",
    "\n",
    "# è§£æèŠå¤©è¨˜éŒ„\n",
    "def parse_chat_log(chat_log: str, user_id: str, friend_id: str, time_threshold: int = 5) -> List[List[Tuple[str, str, int]]]:\n",
    "    lines = [line.strip() for line in chat_log.split('\\n') if line.strip() and \"å„²å­˜æ—¥æœŸ\" not in line and not re.match(r'\\d{4}/\\d{2}/\\d{2}', line)]\n",
    "    conversation_groups = []\n",
    "    current_group = []\n",
    "    prev_time = None\n",
    "    \n",
    "    logger.info(f\"é–‹å§‹è§£æèŠå¤©è¨˜éŒ„ (User: {user_id}, Friend: {friend_id})...\")\n",
    "    for line in tqdm(lines, desc=\"è§£æèŠå¤©è¡Œ\"):\n",
    "        match = re.match(r'(ä¸Šåˆ|ä¸‹åˆ)(\\d{1,2}):(\\d{2})\\s*([A-B])\\s+(.+)', line)\n",
    "        if match:\n",
    "            period, hour, minute, speaker, message = match.groups()\n",
    "            cleaned_message = clean_text(message)\n",
    "            if cleaned_message:\n",
    "                current_time = int(hour) * 60 + int(minute) + (12 * 60 if period == \"ä¸‹åˆ\" else 0)\n",
    "                if prev_time and abs(current_time - prev_time) > time_threshold:\n",
    "                    conversation_groups.append(current_group)\n",
    "                    current_group = []\n",
    "                role = f\"User{user_id}\" if speaker == \"B\" else f\"User{friend_id}\"\n",
    "                current_group.append((role, cleaned_message, current_time))\n",
    "                prev_time = current_time\n",
    "    \n",
    "    if current_group:\n",
    "        conversation_groups.append(current_group)\n",
    "    logger.info(f\"è§£æå®Œæˆï¼Œç”Ÿæˆ {len(conversation_groups)} çµ„å°è©±\")\n",
    "    return conversation_groups\n",
    "\n",
    "# æ·»åŠ æƒ…ç·’æ¨™ç±¤\n",
    "def add_emotion_labels(conversation_groups: List[List[Tuple[str, str, int]]], embeddings: torch.Tensor) -> List[List[Tuple[str, str, int, str]]]:\n",
    "    all_messages = [emoji.demojize(msg) for group in conversation_groups for _, msg, _ in group]\n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in all_messages]\n",
    "    batch_size = get_dynamic_batch_size(all_messages)\n",
    "    logger.info(\"é–‹å§‹å…¨æ‰¹é‡æƒ…æ„Ÿåˆ†é¡...\")\n",
    "    num_batches = (len(all_messages) + batch_size - 1) // batch_size\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(all_messages), batch_size), total=num_batches, desc=\"æƒ…æ„Ÿåˆ†é¡æ‰¹æ¬¡è™•ç†\"):\n",
    "            batch = simplified_messages[i:i + batch_size]\n",
    "            batch_results = sentiment_classifier(batch, batch_size=len(batch), truncation=True, max_length=128)\n",
    "            results.extend(batch_results)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    labeled_groups = [[] for _ in conversation_groups]\n",
    "    emotion_map = {\n",
    "        'joy': 'é–‹å¿ƒ', 'sadness': 'é›£é', 'anger': 'æ†¤æ€’', 'fear': 'å®³æ€•', 'love': 'å–œæ„›', \n",
    "        'surprise': 'é©šè¨', 'neutral': 'ä¸­æ€§', ':smiling_face_with_heart-eyes:': 'å–œæ„›', \n",
    "        ':pouting_face:': 'é›£é', ':laughing:': 'é–‹å¿ƒ', ':sob:': 'é›£é', ':angry_face:': 'æ†¤æ€’',\n",
    "        ':face_with_tears_of_joy:': 'é–‹å¿ƒ', ':pleading_face:': 'å–œæ„›', ':scream:': 'èˆˆå¥®', \n",
    "        ':thinking_face:': 'ç„¦æ…®', ':weary_face:': 'ç„¡å¥ˆ', ':sleeping_face:': 'æ‡¶æ•£'\n",
    "    }\n",
    "    msg_idx = 0\n",
    "    for group_idx, group in enumerate(tqdm(conversation_groups, desc=\"çµ„è£æƒ…æ„Ÿæ¨™ç±¤\")):\n",
    "        for speaker, message, time in group:\n",
    "            sentiment_scores = results[msg_idx]\n",
    "            top_emotion = max(sentiment_scores, key=lambda x: x['score'])['label']\n",
    "            confidence = max(sentiment_scores, key=lambda x: x['score'])['score']\n",
    "            emotion = emotion_map.get(top_emotion, 'ä¸­æ€§')\n",
    "            # æª¢æŸ¥è¡¨æƒ…ç¬¦è™Ÿ\n",
    "            for emoji_key, emo in emotion_map.items():\n",
    "                if emoji_key in message and emo != 'ä¸­æ€§':\n",
    "                    emotion = emo\n",
    "                    break\n",
    "            # å¢å¼·ä¸Šä¸‹æ–‡æ¨ç†\n",
    "            if confidence < 0.5 and msg_idx > 0:\n",
    "                similarity = util.cos_sim(embeddings[msg_idx], embeddings[msg_idx-1]).item()\n",
    "                if similarity > 0.7:\n",
    "                    emotion = labeled_groups[group_idx][-1][3] if labeled_groups[group_idx] else 'ä¸­æ€§'\n",
    "            # é—œéµè©å¾Œè™•ç†\n",
    "            message_lower = message.lower()\n",
    "            if 'ç¬‘æ­»' in message_lower or 'å¥½ç¬‘' in message_lower or 'å“ˆå“ˆ' in message_lower:\n",
    "                emotion = 'é–‹å¿ƒ' if 'è¶…' not in message_lower else 'èˆˆå¥®'\n",
    "            elif 'é›£é' in message_lower or 'å°·å°¬' in message_lower or 'æ€•' in message_lower or 'å†·æ·¡' in message_lower or 'æƒ³å“­' in message_lower:\n",
    "                emotion = 'é›£é'\n",
    "            elif 'åš‡æ­»' in message_lower:\n",
    "                emotion = 'å®³æ€•'\n",
    "            elif 'æ‰ä½ ' in message_lower:\n",
    "                emotion = 'æ†¤æ€’'\n",
    "            elif 'æ²’å®‰å…¨æ„Ÿ' in message_lower:\n",
    "                emotion = 'ç„¦æ…®'\n",
    "            elif 'å¾ˆé«˜' in message_lower and 'é¡å€¼' in message_lower:\n",
    "                emotion = 'å–œæ„›'\n",
    "            elif 'ä½ å¨˜' in message_lower or 'é åŒ—' in message_lower or 'è€–ä½ åª½' in message_lower:\n",
    "                emotion = 'é›£é' if 'ç¬‘' not in message_lower else 'é–‹å¿ƒ'\n",
    "            elif 'æ€éº¼è¾¦' in message_lower or 'åˆ°åº•' in message_lower or 'è¶…è¶•' in message_lower:\n",
    "                emotion = 'ç„¦æ…®'\n",
    "            elif 'æˆ‘è¦ç¡äº†' in message_lower or 'åˆæ²’å·®' in message_lower:\n",
    "                emotion = 'æ‡¶æ•£'\n",
    "            elif 'å»å®œè˜­ç©' in message_lower or 'è¶…å¥½ç¬‘' in message_lower:\n",
    "                emotion = 'èˆˆå¥®' if 'è¶…' in message_lower else 'é–‹å¿ƒ'\n",
    "            elif 'æˆ‘æ²’æº–å‚™' in message_lower or 'æˆ‘ä»–åª½' in message_lower or 'è¶…é†œ' in message_lower or 'æ¸›è‚¥' in message_lower:\n",
    "                emotion = 'ç„¦æ…®' if 'æ€éº¼è¾¦' in message_lower else 'ç„¡å¥ˆ'\n",
    "            elif 'ä¸çŸ¥é“' in message_lower or 'æä¸å¥½' in message_lower:\n",
    "                emotion = 'å›°æƒ‘'\n",
    "            elif 'è¶…ç´šå†·' in message_lower and 'å¥½' not in message_lower:\n",
    "                emotion = 'é›£é'\n",
    "            labeled_groups[group_idx].append((speaker, message, time, emotion))\n",
    "            msg_idx += 1\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(\"æƒ…æ„Ÿæ¨™ç±¤æ·»åŠ å®Œæˆ\")\n",
    "    return labeled_groups\n",
    "\n",
    "# æ·»åŠ è©±é¡Œæ¨™ç±¤\n",
    "def add_topic_labels(conversation_groups: List[List[Tuple[str, str, int, str]]], embeddings: torch.Tensor) -> List[List[Tuple[str, str, int, str, str]]]:\n",
    "    all_messages = [msg for group in conversation_groups for _, msg, _, _ in group]\n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in all_messages]\n",
    "    batch_size = get_dynamic_batch_size(all_messages)\n",
    "    logger.info(\"é–‹å§‹å…¨æ‰¹é‡è©±é¡Œåˆ†é¡...\")\n",
    "    num_batches = (len(all_messages) + batch_size - 1) // batch_size\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(all_messages), batch_size), total=num_batches, desc=\"è©±é¡Œåˆ†é¡æ‰¹æ¬¡è™•ç†\"):\n",
    "            batch = simplified_messages[i:i + batch_size]\n",
    "            batch_results = zero_shot_classifier(batch, topic_labels, multi_label=True, batch_size=len(batch))\n",
    "            results.extend(batch_results)\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "    \n",
    "    labeled_groups = [[] for _ in conversation_groups]\n",
    "    msg_idx = 0\n",
    "    for group_idx, group in enumerate(tqdm(conversation_groups, desc=\"çµ„è£è©±é¡Œæ¨™ç±¤\")):\n",
    "        for speaker, message, time, emotion in group:\n",
    "            result = results[msg_idx]\n",
    "            predicted_topics = sorted([(label, score) for label, score in zip(result['labels'], result['scores'])], key=lambda x: x[1], reverse=True)[:2]\n",
    "            predicted_topics = [label for label, score in predicted_topics if score > 0.4]\n",
    "            message_lower = message.lower()\n",
    "            if 'å¥³æœ‹å‹' in message_lower or 'å–œæ­¡' in message_lower or 'åœ¨ä¸€èµ·' in message_lower or 'ç´„æœƒ' in message_lower or 'fu' in message_lower or 'æ›–æ˜§' in message_lower:\n",
    "                predicted_topics.append('æ„Ÿæƒ…å…«å¦')\n",
    "            if 'ç¾¤çµ„' in message_lower or 'ç”·ç”Ÿ' in message_lower or 'å¥³ç”Ÿ' in message_lower or 'å®¤å‹' in message_lower or 'æœ‹å‹' in message_lower:\n",
    "                predicted_topics.append('äººéš›')\n",
    "            if 'ç¬‘æ­»' in message_lower or 'å¥½ç¬‘' in message_lower or 'ç…§ç‰‡' in message_lower or 'é›»å½±' in message_lower or 'è¶…å¥½ç¬‘' in message_lower:\n",
    "                predicted_topics.append('å¨›æ¨‚')\n",
    "            if 'éŠè¦½è»Š' in message_lower or 'å‡ºå»ç©' in message_lower or 'è¨ˆç•«' in message_lower or 'æ´»å‹•' in message_lower or 'åƒåŠ ' in message_lower:\n",
    "                predicted_topics.append('ç¤¾åœ˜æ´»å‹•')\n",
    "            if 'å¡«' in message_lower or 'åé¡' in message_lower or 'å®¶æ•™' in message_lower or 'è€ƒè©¦' in message_lower or 'é€²åº¦' in message_lower:\n",
    "                predicted_topics.append('èª²æ¥­å£“åŠ›')\n",
    "            if 'ç¡äº†' in message_lower or 'æ´—æ¾¡' in message_lower or 'è¡Œæ' in message_lower or 'å‡ºé–€' in message_lower or 'åƒé£¯' in message_lower or 'èµ·åºŠ' in message_lower:\n",
    "                predicted_topics.append('ç”Ÿæ´»ç‘£äº‹')\n",
    "            if 'æ‰‹æ©Ÿ' in message_lower or 'é›»è…¦' in message_lower or 'ç§‘æŠ€' in message_lower or 'ç‰ˆæœ¬' in message_lower:\n",
    "                predicted_topics.append('ç§‘æŠ€')\n",
    "            if 'å¥åº·' in message_lower or 'é‹å‹•' in message_lower or 'ç”Ÿç—…' in message_lower or 'æšˆè»Š' in message_lower:\n",
    "                predicted_topics.append('å¥åº·')\n",
    "            if 'æ¡ƒåœ’' in message_lower or 'å®œè˜­' in message_lower or 'èµ°è·¯' in message_lower or 'è»Šä¸Š' in message_lower or 'å°ä¸­' in message_lower or 'å°åŒ—' in message_lower:\n",
    "                predicted_topics.append('æ—…è¡Œ')\n",
    "            if not predicted_topics and msg_idx > 0:\n",
    "                similarity = util.cos_sim(embeddings[msg_idx], embeddings[msg_idx-1]).item()\n",
    "                if similarity > 0.7:\n",
    "                    topic_str = labeled_groups[group_idx][-1][4] if labeled_groups[group_idx] else \"æ—¥å¸¸\"\n",
    "                else:\n",
    "                    topic_str = \"æ—¥å¸¸\"\n",
    "            else:\n",
    "                topic_str = \"+\".join(sorted(set(predicted_topics[:2]))) if predicted_topics else \"æ—¥å¸¸\"\n",
    "            labeled_groups[group_idx].append((speaker, message, time, emotion, topic_str))\n",
    "            msg_idx += 1\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(\"è©±é¡Œæ¨™ç±¤æ·»åŠ å®Œæˆ\")\n",
    "    return labeled_groups\n",
    "\n",
    "# å„ªåŒ–æ ¼å¼åŒ–è¨“ç·´æ•¸æ“š\n",
    "def format_training_data(conversation_groups: List[List[Tuple[str, str, int, str, str]]], embeddings: torch.Tensor, max_turns: int = 10, similarity_threshold: float = 0.7) -> List[dict]:\n",
    "    training_data = []\n",
    "    msg_idx = 0\n",
    "    \n",
    "    logger.info(\"é–‹å§‹æ ¼å¼åŒ–è¨“ç·´è³‡æ–™...\")\n",
    "    all_messages = [item[1] for group in conversation_groups for item in group]\n",
    "    logger.info(\"é å…ˆæ‰¹é‡è¨ˆç®—æ‰€æœ‰å°è©±è¡Œç‚º...\")\n",
    "    all_dialogue_acts = infer_dialogue_act(all_messages, embeddings=embeddings)\n",
    "    act_idx = 0\n",
    "    \n",
    "    for group in tqdm(conversation_groups, desc=\"æ ¼å¼åŒ–å°è©±çµ„\"):\n",
    "        messages = [item[1] for item in group]\n",
    "        dialogue_acts = all_dialogue_acts[act_idx:act_idx + len(messages)]\n",
    "        act_idx += len(messages)\n",
    "        \n",
    "        if len(messages) > 1:\n",
    "            curr_embeddings = embeddings[msg_idx:msg_idx + len(messages) - 1]\n",
    "            next_embeddings = embeddings[msg_idx + 1:msg_idx + len(messages)]\n",
    "            similarities = util.cos_sim(curr_embeddings, next_embeddings).diagonal().cpu().numpy()\n",
    "        else:\n",
    "            similarities = np.array([])\n",
    "        \n",
    "        for i in range(len(group) - 1):\n",
    "            context = group[max(0, i - max_turns + 1):i + 1]\n",
    "            next_msg = group[i + 1]\n",
    "            similarity = similarities[i] if i < len(similarities) else 0.0\n",
    "            \n",
    "            # åŠ å¼·èªç¾©å®Œæ•´æ€§å’Œè©±é¡Œä¸€è‡´æ€§æª¢æŸ¥\n",
    "            context_topic = context[-1][4].split(\"+\")[0]  # å–ä¸»è¦è©±é¡Œ\n",
    "            response_topic = next_msg[4].split(\"+\")[0]\n",
    "            if (similarity >= similarity_threshold and len(next_msg[1]) >= 3 and \n",
    "                any(c in next_msg[1] for c in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'æ˜¯', 'ä¸', 'æˆ‘', 'ä½ ', 'ä»–', 'æœ‰', 'æ²’']) and\n",
    "                context_topic == response_topic):\n",
    "                prompt = \"\\n\".join(f\"{s}: {m} [æƒ…ç·’: {e}, è¡Œç‚º: {dialogue_acts[j]}, è©±é¡Œ: {t}]\" \n",
    "                                 for j, (s, m, _, e, t) in enumerate(context))\n",
    "                response = f\"{next_msg[1]} [æƒ…ç·’: {next_msg[3]}, è¡Œç‚º: {dialogue_acts[i + 1]}, è©±é¡Œ: {next_msg[4]}]\"\n",
    "                training_data.append({\"prompt\": prompt, \"response\": response})\n",
    "        msg_idx += len(group)\n",
    "    \n",
    "    logger.info(f\"æ ¼å¼åŒ–å®Œæˆï¼Œç”Ÿæˆ {len(training_data)} ç­†è¨“ç·´è³‡æ–™\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return training_data\n",
    "\n",
    "# æ··åˆå¤šäººç¾¤èŠ\n",
    "def mix_conversations(chat_logs: List[str], friend_ids: List[str], time_threshold: int = 5, max_turns: int = 10, similarity_threshold: float = 0.7) -> List[dict]:\n",
    "    all_groups = []\n",
    "    for user_id, (chat_log, friend_id) in enumerate(tqdm(zip(chat_logs, friend_ids), desc=\"è§£æèŠå¤©æª”æ¡ˆ\", total=len(chat_logs))):\n",
    "        groups = parse_chat_log(chat_log, str(user_id), friend_id, time_threshold)\n",
    "        all_groups.extend(groups)\n",
    "    \n",
    "    all_messages = [msg for group in all_groups for _, msg, _ in group]\n",
    "    embeddings = precompute_embeddings(all_messages, num_gpus=2, chunk_size=5000)\n",
    "    \n",
    "    logger.info(\"é–‹å§‹æ·»åŠ æƒ…ç·’æ¨™ç±¤...\")\n",
    "    all_groups = add_emotion_labels(all_groups, embeddings)\n",
    "    logger.info(\"é–‹å§‹æ·»åŠ è©±é¡Œæ¨™ç±¤...\")\n",
    "    all_groups = add_topic_labels(all_groups, embeddings)\n",
    "    \n",
    "    logger.info(\"é–‹å§‹æ··åˆå°è©±...\")\n",
    "    all_conversations = [item for group in all_groups for item in group]\n",
    "    all_conversations.sort(key=lambda x: x[2])\n",
    "    \n",
    "    mixed_groups = []\n",
    "    current_group = []\n",
    "    prev_time = None\n",
    "    prev_topic = None\n",
    "    for item in tqdm(all_conversations, desc=\"æ··åˆå°è©±\"):\n",
    "        speaker, message, time, emotion, topic = item\n",
    "        if prev_time and (abs(time - prev_time) > time_threshold or topic.split(\"+\")[0] != prev_topic):\n",
    "            if current_group:\n",
    "                mixed_groups.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append((speaker, message, time, emotion, topic))\n",
    "        prev_time = time\n",
    "        prev_topic = topic.split(\"+\")[0]\n",
    "    if current_group:\n",
    "        mixed_groups.append(current_group)\n",
    "    \n",
    "    logger.info(f\"æ··åˆå®Œæˆï¼Œç”Ÿæˆ {len(mixed_groups)} çµ„å°è©±\")\n",
    "    return format_training_data(mixed_groups, embeddings, max_turns, similarity_threshold)\n",
    "\n",
    "# ä¸»å‡½æ•¸\n",
    "def process_chat_to_training_data(chat_files: List[str], friend_ids: List[str], output_file: str, time_threshold: int = 5, max_turns: int = 10, similarity_threshold: float = 0.7):\n",
    "    chat_logs = []\n",
    "    for chat_file in tqdm(chat_files, desc=\"è®€å–èŠå¤©æª”æ¡ˆ\"):\n",
    "        try:\n",
    "            with open(chat_file, 'r', encoding='utf-8') as f:\n",
    "                chat_logs.append(f.read())\n",
    "            logger.info(f\"æˆåŠŸè®€å–æª”æ¡ˆ: {chat_file}\")\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"æ‰¾ä¸åˆ°æª”æ¡ˆ: {chat_file}\")\n",
    "            return\n",
    "    \n",
    "    training_data = mix_conversations(chat_logs, friend_ids, time_threshold, max_turns, similarity_threshold)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(training_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    logger.info(f\"è¨“ç·´è³‡æ–™å·²å„²å­˜è‡³ {output_file}ï¼Œç¸½å…± {len(training_data)} ç­†è³‡æ–™\")\n",
    "\n",
    "# æ¸¬è©¦ç”¨\n",
    "if __name__ == \"__main__\":\n",
    "    chat_files = [\"data/claire.txt\"]\n",
    "    friend_ids = [\"2\"]\n",
    "    output_file = \"output/out.json\"\n",
    "    \n",
    "    process_chat_to_training_data(chat_files, friend_ids, output_file)\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            logger.info(\"å‰ 3 ç­†è¨“ç·´è³‡æ–™ç¯„ä¾‹ï¼š\")\n",
    "            for i, sample in enumerate(data[:3]):\n",
    "                logger.info(f\"æ¨£æœ¬ {i + 1}:\")\n",
    "                logger.info(f\"Prompt: {sample['prompt']}\")\n",
    "                logger.info(f\"Response: {sample['response']}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"ç„¡æ³•è®€å–è¼¸å‡ºæª”æ¡ˆï¼Œè«‹æª¢æŸ¥è™•ç†éç¨‹æ˜¯å¦æˆåŠŸã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1eefe79b-12fe-4404-b379-ac31444cb0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 15:46:06,908 - æ­£åœ¨åˆå§‹åŒ–æƒ…æ„Ÿåˆ†é¡å™¨ï¼ˆErlangshen-Roberta-110M-Sentimentï¼‰...\n",
      "Device set to use cuda:0\n",
      "2025-03-28 15:46:07,990 - æ­£åœ¨åˆå§‹åŒ–é›¶æ¨£æœ¬åˆ†é¡å™¨...\n",
      "Device set to use cuda:0\n",
      "è®€å–èŠå¤©æª”æ¡ˆ:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-28 15:46:09,862 - æˆåŠŸè®€å–æª”æ¡ˆ: data/claire.txt\n",
      "è®€å–èŠå¤©æª”æ¡ˆ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 382.62it/s]\n",
      "è§£æèŠå¤©æª”æ¡ˆ:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-28 15:46:09,870 - é–‹å§‹è§£æèŠå¤©è¨˜éŒ„ (User: 0, Friend: 2)...\n",
      "\n",
      "è§£æèŠå¤©è¡Œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2014/2014 [00:00<00:00, 212201.77it/s]\n",
      "2025-03-28 15:46:09,885 - è§£æå®Œæˆï¼Œç”Ÿæˆ 116 çµ„å°è©±\n",
      "è§£æèŠå¤©æª”æ¡ˆ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 47.95it/s]\n",
      "2025-03-28 15:46:09,888 - é–‹å§‹é è¨ˆç®— 1499 æ¢è¨Šæ¯çš„åµŒå…¥å‘é‡...\n",
      "2025-03-28 15:46:09,910 - Use pytorch device_name: cuda\n",
      "2025-03-28 15:46:09,911 - Load pretrained SentenceTransformer: intfloat/multilingual-e5-large\n",
      "è¨ˆç®—åµŒå…¥åˆ†ç‰‡:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-28 15:46:13,783 - è™•ç†åˆ†ç‰‡ 1/1 on GPU 0\n",
      "è¨ˆç®—åµŒå…¥åˆ†ç‰‡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:02<00:00,  2.74s/it]\n",
      "2025-03-28 15:46:16,527 - åµŒå…¥è¨ˆç®—å®Œæˆ\n",
      "2025-03-28 15:46:16,527 - é–‹å§‹æ·»åŠ æƒ…ç·’æ¨™ç±¤...\n",
      "2025-03-28 15:46:16,587 - é–‹å§‹å…¨æ‰¹é‡æƒ…æ„Ÿåˆ†é¡...\n",
      "æƒ…æ„Ÿåˆ†é¡æ‰¹æ¬¡è™•ç†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:02<00:00,  2.31it/s]\n",
      "çµ„è£æƒ…æ„Ÿæ¨™ç±¤: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:00<00:00, 25638.37it/s]\n",
      "2025-03-28 15:46:19,206 - æƒ…æ„Ÿæ¨™ç±¤æ·»åŠ å®Œæˆ\n",
      "2025-03-28 15:46:19,207 - é–‹å§‹æ·»åŠ è©±é¡Œæ¨™ç±¤...\n",
      "2025-03-28 15:46:19,223 - é–‹å§‹å…¨æ‰¹é‡è©±é¡Œåˆ†é¡...\n",
      "è©±é¡Œåˆ†é¡æ‰¹æ¬¡è™•ç†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:44<00:00,  7.44s/it]\n",
      "çµ„è£è©±é¡Œæ¨™ç±¤: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 116/116 [00:00<00:00, 844.34it/s]\n",
      "2025-03-28 15:47:03,986 - è©±é¡Œæ¨™ç±¤æ·»åŠ å®Œæˆ\n",
      "2025-03-28 15:47:03,988 - é–‹å§‹æ··åˆå°è©±...\n",
      "æ··åˆå°è©±: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1499/1499 [00:00<00:00, 887279.38it/s]\n",
      "2025-03-28 15:47:03,994 - æ··åˆå®Œæˆï¼Œç”Ÿæˆ 268 çµ„å°è©±\n",
      "2025-03-28 15:47:03,995 - é–‹å§‹æ ¼å¼åŒ–è¨“ç·´è³‡æ–™...\n",
      "2025-03-28 15:47:03,996 - é å…ˆæ‰¹é‡è¨ˆç®—æ‰€æœ‰å°è©±è¡Œç‚º...\n",
      "2025-03-28 15:47:04,017 - æ­£åœ¨å° 1499 æ¢è¨Šæ¯é€²è¡Œå°è©±è¡Œç‚ºåˆ†é¡ï¼Œæ‰¹æ¬¡å¤§å°: 256\n",
      "è¡Œç‚ºåˆ†é¡æ‰¹æ¬¡è™•ç†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [01:13<00:00, 12.26s/it]\n",
      "å¾Œè™•ç†å°è©±è¡Œç‚º: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1499/1499 [00:00<00:00, 14577.94it/s]\n",
      "æ ¼å¼åŒ–å°è©±çµ„: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268/268 [00:00<00:00, 4650.22it/s]\n",
      "2025-03-28 15:48:17,729 - æ ¼å¼åŒ–å®Œæˆï¼Œç”Ÿæˆ 1201 ç­†è¨“ç·´è³‡æ–™\n",
      "2025-03-28 15:48:17,749 - è¨“ç·´è³‡æ–™å·²å„²å­˜è‡³ output/out.jsonï¼Œç¸½å…± 1201 ç­†è³‡æ–™\n",
      "2025-03-28 15:48:17,757 - å‰ 3 ç­†è¨“ç·´è³‡æ–™ç¯„ä¾‹ï¼š\n",
      "2025-03-28 15:48:17,757 - æ¨£æœ¬ 1:\n",
      "2025-03-28 15:48:17,759 - Prompt: User0: å•Šä»–åˆ°åº•æœ‰æ²’æœ‰å¥³æœ‹å‹ [æƒ…ç·’: ç„¦æ…®, è¡Œç‚º: å‘Šåˆ¥, è©±é¡Œ: æ„Ÿæƒ…å…«å¦]\n",
      "2025-03-28 15:48:17,760 - Response: ä»–é¡å€¼çœŸçš„å¾ˆé«˜ğŸ¥ºğŸ¥ºğŸ¥º [æƒ…ç·’: å–œæ„›, è¡Œç‚º: None, è©±é¡Œ: æ„Ÿæƒ…å…«å¦]\n",
      "2025-03-28 15:48:17,762 - æ¨£æœ¬ 2:\n",
      "2025-03-28 15:48:17,763 - Prompt: User0: å•Šä»–åˆ°åº•æœ‰æ²’æœ‰å¥³æœ‹å‹ [æƒ…ç·’: ç„¦æ…®, è¡Œç‚º: å‘Šåˆ¥, è©±é¡Œ: æ„Ÿæƒ…å…«å¦]\n",
      "User0: ä»–é¡å€¼çœŸçš„å¾ˆé«˜ğŸ¥ºğŸ¥ºğŸ¥º [æƒ…ç·’: å–œæ„›, è¡Œç‚º: None, è©±é¡Œ: æ„Ÿæƒ…å…«å¦]\n",
      "2025-03-28 15:48:17,764 - Response: é›–ç„¶æˆ‘è¦ºå¾— [æƒ…ç·’: é–‹å¿ƒ, è¡Œç‚º: None, è©±é¡Œ: æ„Ÿæƒ…å…«å¦]\n",
      "2025-03-28 15:48:17,765 - æ¨£æœ¬ 3:\n",
      "2025-03-28 15:48:17,766 - Prompt: User0: å•Šä»–åˆ°åº•æœ‰æ²’æœ‰å¥³æœ‹å‹ [æƒ…ç·’: ç„¦æ…®, è¡Œç‚º: å‘Šåˆ¥, è©±é¡Œ: æ„Ÿæƒ…å…«å¦]\n",
      "User0: ä»–é¡å€¼çœŸçš„å¾ˆé«˜ğŸ¥ºğŸ¥ºğŸ¥º [æƒ…ç·’: å–œæ„›, è¡Œç‚º: None, è©±é¡Œ: æ„Ÿæƒ…å…«å¦]\n",
      "User0: é›–ç„¶æˆ‘è¦ºå¾— [æƒ…ç·’: é–‹å¿ƒ, è¡Œç‚º: None, è©±é¡Œ: æ„Ÿæƒ…å…«å¦]\n",
      "2025-03-28 15:48:17,768 - Response: å¾ˆå†·æ·¡ [æƒ…ç·’: é›£é, è¡Œç‚º: å¼•å…¥, è©±é¡Œ: æ„Ÿæƒ…å…«å¦]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import logging\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "import os\n",
    "import opencc\n",
    "\n",
    "# è¨­ç½®è¨˜æ†¶é«”ç®¡ç†ç’°å¢ƒè®Šæ•¸\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# è¨­ç½®æ—¥èªŒ\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# åˆå§‹åŒ–å¤š GPU ç’°å¢ƒ\n",
    "device_ids = [0, 1]  # å…©å¼µ A10G\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# å®šç¾©åˆ†é¡æ¨™ç±¤\n",
    "act_labels = [\"æå•\", \"è£œå……\", \"å¼•å…¥\", \"è¡¨é”æƒ…æ„Ÿ\", \"è®šç¾\", \"é“æ­‰\", \"æ„Ÿè¬\", \"è«‹æ±‚\", \"æ‹’çµ•\", \"å»ºè­°\", \"å•å€™\", \"å‘Šåˆ¥\", \"ç¢ºèª\", \"è¡¨é”æ„åœ–\", \"åæ§½\", \"å‚¬ä¿ƒ\", \"å®‰æ…°\", \"æ¥æ¢—\"]\n",
    "topic_labels = [\"æ´»å‹•\", \"æ„Ÿæƒ…\", \"å­¸æ¥­\", \"å¨›æ¨‚\", \"äººéš›\", \"èª²æ¥­å£“åŠ›\", \"ç¤¾åœ˜æ´»å‹•\", \"æ„Ÿæƒ…å…«å¦\", \"ç”Ÿæ´»ç‘£äº‹\"]\n",
    "\n",
    "# åˆå§‹åŒ–ç°¡ç¹è½‰æ›å™¨\n",
    "converter_to_simplified = opencc.OpenCC('t2s')\n",
    "converter_to_traditional = opencc.OpenCC('s2t')\n",
    "\n",
    "# å‹•æ…‹æ‰¹æ¬¡å¤§å°\n",
    "def get_dynamic_batch_size(messages: List[str], base_batch_size: int = 256) -> int:\n",
    "    gpu_memory = torch.cuda.memory_reserved(device) / 1024**3\n",
    "    max_memory = 24  # A10G 24GB\n",
    "    if gpu_memory > max_memory * 0.8:\n",
    "        return max(16, base_batch_size // 2)\n",
    "    return base_batch_size\n",
    "\n",
    "# å–®é€²ç¨‹åˆ†ç‰‡è¨ˆç®—åµŒå…¥\n",
    "def compute_chunk(chunk: List[str], gpu_id: int, batch_size: int, embedder: SentenceTransformer) -> torch.Tensor:\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "    embedder.to(f\"cuda:{gpu_id}\")\n",
    "    try:\n",
    "        embeddings = embedder.encode(chunk, convert_to_tensor=True, batch_size=batch_size, show_progress_bar=False)\n",
    "        embeddings = embeddings.to(\"cuda:0\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return embeddings\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        logger.warning(f\"GPU {gpu_id} è¨˜æ†¶é«”ä¸è¶³ï¼Œæ¸›åŠæ‰¹æ¬¡å¤§å°é‡è©¦...\")\n",
    "        embeddings = embedder.encode(chunk, convert_to_tensor=True, batch_size=batch_size // 2, show_progress_bar=False)\n",
    "        embeddings = embeddings.to(\"cuda:0\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return embeddings\n",
    "\n",
    "def precompute_embeddings(messages: List[str], num_gpus: int = 2, chunk_size: int = 5000) -> torch.Tensor:\n",
    "    logger.info(f\"é–‹å§‹é è¨ˆç®— {len(messages)} æ¢è¨Šæ¯çš„åµŒå…¥å‘é‡...\")\n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in messages]\n",
    "    chunks = [simplified_messages[i:i + chunk_size] for i in range(0, len(messages), chunk_size)]\n",
    "    batch_size = get_dynamic_batch_size(messages)\n",
    "    \n",
    "    embedder = SentenceTransformer('intfloat/multilingual-e5-large')\n",
    "    results = []\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"è¨ˆç®—åµŒå…¥åˆ†ç‰‡\")):\n",
    "        gpu_id = device_ids[i % num_gpus]\n",
    "        logger.info(f\"è™•ç†åˆ†ç‰‡ {i+1}/{len(chunks)} on GPU {gpu_id}\")\n",
    "        chunk_embeddings = compute_chunk(chunk, gpu_id, batch_size, embedder)\n",
    "        results.append(chunk_embeddings)\n",
    "    \n",
    "    embeddings = torch.cat(results, dim=0)\n",
    "    embeddings = embeddings.cpu()\n",
    "    logger.info(\"åµŒå…¥è¨ˆç®—å®Œæˆ\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return embeddings\n",
    "\n",
    "# æ¸…æ´—æ–‡æœ¬\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    patterns = r'(ä¸Šåˆ|ä¸‹åˆ)\\d{1,2}:\\d{2}\\s*|\\[ç…§ç‰‡\\]|\\[å½±ç‰‡\\]|\\[è²¼åœ–\\]|.*å·²æ”¶å›è¨Šæ¯.*|â˜.*|^\\*+$'\n",
    "    return re.sub(patterns, '', text).strip()\n",
    "\n",
    "# é—œéµè©åˆ†é¡ï¼ˆæ“´å±•è¡Œç‚ºèˆ‡è©±é¡Œï¼‰\n",
    "def keyword_infer_dialogue_act(message: str, prev_act: str = None) -> str:\n",
    "    message_lower = message.lower()\n",
    "    keywords = {\n",
    "        \"æå•\": ['æœ‰æ²’æœ‰', 'ä»€éº¼', 'æœƒä¸æœƒ', 'ä½ è¦ºå¾—å‘¢', 'æ€éº¼', 'ç‚ºä»€éº¼', 'å—', 'å“ªå€‹', 'å¯ä¸å¯ä»¥', 'åˆ°äº†æ²’', 'å¹¾é»', 'åˆ°åº•', 'çŸ¥é“', 'å¤šå°‘'],\n",
    "        \"è£œå……\": ['å› ç‚º', 'æ‰€ä»¥', 'é›–ç„¶', 'ä¸é', 'ä½†æ˜¯', 'ç„¶å¾Œ', 'è€Œä¸”', 'çµæœ', 'é‚„æœ‰', 'å¹«', 'å°±'],\n",
    "        \"å¼•å…¥\": ['é‡é»æ˜¯', 'ä¸»è¦æ˜¯', 'èªªåˆ°', 'è¬›åˆ°', 'æåˆ°', 'æˆ‘ä¹Ÿä¸çŸ¥é“'],\n",
    "        \"è¡¨é”æƒ…æ„Ÿ\": ['é›£é', 'å°·å°¬', 'è‡ªè±ª', 'éº»ç…©', 'ç·Šå¼µ', 'é–‹å¿ƒ', 'ç¬‘æ­»', 'å¸Œæœ›', 'æ€•', 'è¦ºå¾—', 'ç´¯äº†', 'å†·æ·¡', 'å¥½ç¬‘', 'é åŒ—', 'ä½ å¨˜', 'è¶…è¶•'],\n",
    "        \"è®šç¾\": ['æ¼‚äº®', 'å¸¥', 'å¾ˆæ£’', 'å¾ˆè®š', 'å¾ˆå¯æ„›', 'é©åˆ', 'å¥½æœƒ', 'å¥½ä¸€é»', 'é¡å€¼', 'å¾ˆé«˜'],\n",
    "        \"é“æ­‰\": ['å°ä¸èµ·', 'æŠ±æ­‰', 'ä¸å¥½æ„æ€', 'sorry'],\n",
    "        \"æ„Ÿè¬\": ['è¬è¬', 'æ„Ÿè¬', 'æ„Ÿæ¿€', 'å¤šè¬'],\n",
    "        \"è«‹æ±‚\": ['æ‹œè¨—', 'è«‹', 'å¹«æˆ‘', 'å¯ä»¥å—'],\n",
    "        \"æ‹’çµ•\": ['ä¸è¦', 'ä¸è¡Œ', 'ä¸å¯ä»¥', 'ä¸ok', 'æ²’è¾¦æ³•', 'ä¸æœƒ', 'æˆ‘æ²’'],\n",
    "        \"å»ºè­°\": ['å»ºè­°', 'ä¸å¦‚', 'è¦ä¸è¦', 'ä¸ç„¶', 'ç­‰ç­‰å†'],\n",
    "        \"å•å€™\": ['å—¨', 'å˜¿', 'ä½ å¥½', 'æ—©å®‰'],\n",
    "        \"å‘Šåˆ¥\": ['æ°æ°', 'æ‹œæ‹œ', 'å†è¦‹', 'æ™šå®‰', 'è§£æ•£', 'å‡ºé–€', 'ä½ åˆªæ‰å•Š'],\n",
    "        \"ç¢ºèª\": ['çœŸçš„å‡çš„', 'ç¢ºå®š', 'ç¢ºèª', 'å¥½å•¦'],\n",
    "        \"è¡¨é”æ„åœ–\": ['æˆ‘è¦', 'æˆ‘æƒ³', 'æˆ‘æœƒ'],\n",
    "        \"åæ§½\": ['ç¬‘æ­»', 'æ€éº¼å¯èƒ½', 'è¶…é†œ', 'é è…°', 'ä½ å¿«é»'],\n",
    "        \"å‚¬ä¿ƒ\": ['å¿«é»', 'æœ€å¥½å¿«é»', 'è¶•ç·Š', 'ä¾›ä¸‰ä¿¡'],\n",
    "        \"å®‰æ…°\": ['ä¸æœƒå§', 'åˆæ²’å·®', 'æ²’äº‹çš„'],\n",
    "        \"æ¥æ¢—\": ['è¶…å¥½ç¬‘', 'å“ˆå“ˆå“ˆ', 'æˆ‘ä¹Ÿæ˜¯å§', 'å¾Œä¾†ä¹Ÿæ›']\n",
    "    }\n",
    "    for act, kws in keywords.items():\n",
    "        if any(kw in message_lower for kw in kws):\n",
    "            return act\n",
    "    return \"å›ç­”\" if prev_act == \"æå•\" else None\n",
    "\n",
    "# åˆå§‹åŒ–åˆ†é¡å™¨\n",
    "logger.info(\"æ­£åœ¨åˆå§‹åŒ–æƒ…æ„Ÿåˆ†é¡å™¨ï¼ˆErlangshen-Roberta-110M-Sentimentï¼‰...\")\n",
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\",\n",
    "    device=device_ids[0],\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "logger.info(\"æ­£åœ¨åˆå§‹åŒ–é›¶æ¨£æœ¬åˆ†é¡å™¨...\")\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=device_ids[0],\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# å°è©±è¡Œç‚ºåˆ†é¡\n",
    "def infer_dialogue_act(messages: List[str], prev_acts: List[str] = None, embeddings: torch.Tensor = None) -> List[str]:\n",
    "    if prev_acts is None:\n",
    "        prev_acts = [None] * len(messages)\n",
    "    \n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in messages]\n",
    "    dataset = Dataset.from_dict({\"text\": simplified_messages})\n",
    "    batch_size = get_dynamic_batch_size(messages)\n",
    "    logger.info(f\"æ­£åœ¨å° {len(messages)} æ¢è¨Šæ¯é€²è¡Œå°è©±è¡Œç‚ºåˆ†é¡ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
    "    \n",
    "    num_batches = (len(messages) + batch_size - 1) // batch_size\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(messages), batch_size), total=num_batches, desc=\"è¡Œç‚ºåˆ†é¡æ‰¹æ¬¡è™•ç†\"):\n",
    "        batch = simplified_messages[i:i + batch_size]\n",
    "        batch_results = zero_shot_classifier(batch, act_labels, multi_label=False, batch_size=len(batch))\n",
    "        results.extend(batch_results)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    predicted_acts = []\n",
    "    for i, (message, prev_act, result) in tqdm(enumerate(zip(messages, prev_acts, results)), total=len(messages), desc=\"å¾Œè™•ç†å°è©±è¡Œç‚º\"):\n",
    "        predicted_act = result['labels'][0]\n",
    "        confidence = result['scores'][0]\n",
    "        \n",
    "        if confidence < 0.1 and embeddings is not None and i > 0:  # é™ä½é–¾å€¼è‡³ 0.1\n",
    "            similarity = util.cos_sim(embeddings[i], embeddings[i-1]).item()\n",
    "            if similarity > 0.7:\n",
    "                predicted_act = prev_acts[i-1]\n",
    "            else:\n",
    "                predicted_act = keyword_infer_dialogue_act(message, prev_act)\n",
    "        elif prev_act == \"æå•\" and predicted_act not in [\"æå•\", \"è£œå……\", \"å›ç­”\"]:\n",
    "            predicted_act = \"å›ç­”\"\n",
    "        \n",
    "        predicted_acts.append(predicted_act)\n",
    "    return predicted_acts\n",
    "\n",
    "# è§£æèŠå¤©è¨˜éŒ„\n",
    "def parse_chat_log(chat_log: str, user_id: str, friend_id: str, time_threshold: int = 5) -> List[List[Tuple[str, str, int]]]:\n",
    "    lines = [line.strip() for line in chat_log.split('\\n') if line.strip() and \"å„²å­˜æ—¥æœŸ\" not in line and not re.match(r'\\d{4}/\\d{2}/\\d{2}', line)]\n",
    "    conversation_groups = []\n",
    "    current_group = []\n",
    "    prev_time = None\n",
    "    \n",
    "    logger.info(f\"é–‹å§‹è§£æèŠå¤©è¨˜éŒ„ (User: {user_id}, Friend: {friend_id})...\")\n",
    "    for line in tqdm(lines, desc=\"è§£æèŠå¤©è¡Œ\"):\n",
    "        match = re.match(r'(ä¸Šåˆ|ä¸‹åˆ)(\\d{1,2}):(\\d{2})\\s*([A-B])\\s+(.+)', line)\n",
    "        if match:\n",
    "            period, hour, minute, speaker, message = match.groups()\n",
    "            cleaned_message = clean_text(message)\n",
    "            if cleaned_message and len(cleaned_message) >= 3:\n",
    "                current_time = int(hour) * 60 + int(minute) + (12 * 60 if period == \"ä¸‹åˆ\" else 0)\n",
    "                if prev_time and abs(current_time - prev_time) > time_threshold:\n",
    "                    conversation_groups.append(current_group)\n",
    "                    current_group = []\n",
    "                role = f\"User{user_id}\" if speaker == \"B\" else f\"User{friend_id}\"\n",
    "                current_group.append((role, cleaned_message, current_time))\n",
    "                prev_time = current_time\n",
    "    \n",
    "    if current_group:\n",
    "        conversation_groups.append(current_group)\n",
    "    logger.info(f\"è§£æå®Œæˆï¼Œç”Ÿæˆ {len(conversation_groups)} çµ„å°è©±\")\n",
    "    return conversation_groups\n",
    "\n",
    "# æ·»åŠ æƒ…ç·’æ¨™ç±¤\n",
    "def add_emotion_labels(conversation_groups: List[List[Tuple[str, str, int]]], embeddings: torch.Tensor) -> List[List[Tuple[str, str, int, str]]]:\n",
    "    all_messages = [emoji.demojize(msg) for group in conversation_groups for _, msg, _ in group]\n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in all_messages]\n",
    "    batch_size = get_dynamic_batch_size(all_messages)\n",
    "    logger.info(\"é–‹å§‹å…¨æ‰¹é‡æƒ…æ„Ÿåˆ†é¡...\")\n",
    "    dataset = Dataset.from_dict({\"text\": simplified_messages})\n",
    "    num_batches = (len(all_messages) + batch_size - 1) // batch_size\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(all_messages), batch_size), total=num_batches, desc=\"æƒ…æ„Ÿåˆ†é¡æ‰¹æ¬¡è™•ç†\"):\n",
    "        batch = simplified_messages[i:i + batch_size]\n",
    "        batch_results = sentiment_classifier(batch, batch_size=len(batch), truncation=True, max_length=128)\n",
    "        results.extend(batch_results)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    labeled_groups = [[] for _ in conversation_groups]\n",
    "    emotion_map = {\n",
    "        'Positive': 'é–‹å¿ƒ', 'Negative': 'é›£é', 'Neutral': 'ä¸­æ€§',\n",
    "        ':smiling_face_with_heart-eyes:': 'å–œæ„›', ':pouting_face:': 'é›£é',\n",
    "        ':laughing:': 'é–‹å¿ƒ', ':sob:': 'é›£é', ':angry_face:': 'æ†¤æ€’',\n",
    "        ':face_with_tears_of_joy:': 'é–‹å¿ƒ', ':pleading_face:': 'å–œæ„›',\n",
    "        ':scream:': 'èˆˆå¥®', ':thinking_face:': 'ç„¦æ…®', ':weary_face:': 'ç„¡å¥ˆ',\n",
    "        ':sleeping_face:': 'æ‡¶æ•£'\n",
    "    }\n",
    "    msg_idx = 0\n",
    "    for group_idx, group in enumerate(tqdm(conversation_groups, desc=\"çµ„è£æƒ…æ„Ÿæ¨™ç±¤\")):\n",
    "        for speaker, message, time in group:\n",
    "            sentiment = results[msg_idx]['label']\n",
    "            confidence = results[msg_idx]['score']\n",
    "            emotion = emotion_map.get(sentiment, 'ä¸­æ€§')\n",
    "            # æª¢æŸ¥è¡¨æƒ…ç¬¦è™Ÿ\n",
    "            for emoji_key, emo in emotion_map.items():\n",
    "                if emoji_key in message and emo != 'ä¸­æ€§':\n",
    "                    emotion = emo\n",
    "                    break\n",
    "            # é™ä½é–¾å€¼ä¸¦å¢å¼·ä¸Šä¸‹æ–‡æ¨ç†\n",
    "            if confidence < 0.5 and msg_idx > 0:\n",
    "                similarity = util.cos_sim(embeddings[msg_idx], embeddings[msg_idx-1]).item()\n",
    "                if similarity > 0.7:\n",
    "                    emotion = labeled_groups[group_idx][-1][3] if labeled_groups[group_idx] else 'ä¸­æ€§'\n",
    "            # é—œéµè©å¾Œè™•ç†\n",
    "            message_lower = message.lower()\n",
    "            if 'ç¬‘æ­»' in message_lower or 'å¥½ç¬‘' in message_lower or 'å“ˆå“ˆ' in message_lower:\n",
    "                emotion = 'é–‹å¿ƒ' if 'è¶…' not in message_lower else 'èˆˆå¥®'\n",
    "            elif 'é›£é' in message_lower or 'å°·å°¬' in message_lower or 'æ€•' in message_lower or 'å†·æ·¡' in message_lower:\n",
    "                emotion = 'é›£é'\n",
    "            elif 'å¾ˆé«˜' in message_lower and 'é¡å€¼' in message_lower:\n",
    "                emotion = 'å–œæ„›'\n",
    "            elif 'ä½ å¨˜' in message_lower or 'é åŒ—' in message_lower or 'è€–ä½ åª½' in message_lower:\n",
    "                emotion = 'é›£é' if 'ç¬‘' not in message_lower else 'é–‹å¿ƒ'\n",
    "            elif 'æ€éº¼è¾¦' in message_lower or 'åˆ°åº•' in message_lower or 'è¶…è¶•' in message_lower:\n",
    "                emotion = 'ç„¦æ…®'\n",
    "            elif 'æˆ‘è¦ç¡äº†' in message_lower or 'åˆæ²’å·®' in message_lower:\n",
    "                emotion = 'æ‡¶æ•£'\n",
    "            elif 'å»å®œè˜­ç©' in message_lower or 'è¶…å¥½ç¬‘' in message_lower:\n",
    "                emotion = 'èˆˆå¥®' if 'è¶…' in message_lower else 'é–‹å¿ƒ'\n",
    "            elif 'æˆ‘æ²’æº–å‚™' in message_lower or 'æˆ‘ä»–åª½' in message_lower:\n",
    "                emotion = 'ç„¦æ…®' if 'æ€éº¼è¾¦' in message_lower else 'ç„¡å¥ˆ'\n",
    "            labeled_groups[group_idx].append((speaker, message, time, emotion))\n",
    "            msg_idx += 1\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(\"æƒ…æ„Ÿæ¨™ç±¤æ·»åŠ å®Œæˆ\")\n",
    "    return labeled_groups\n",
    "\n",
    "# æ·»åŠ è©±é¡Œæ¨™ç±¤\n",
    "def add_topic_labels(conversation_groups: List[List[Tuple[str, str, int, str]]], embeddings: torch.Tensor) -> List[List[Tuple[str, str, int, str, str]]]:\n",
    "    all_messages = [msg for group in conversation_groups for _, msg, _, _ in group]\n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in all_messages]\n",
    "    batch_size = get_dynamic_batch_size(all_messages)\n",
    "    logger.info(\"é–‹å§‹å…¨æ‰¹é‡è©±é¡Œåˆ†é¡...\")\n",
    "    dataset = Dataset.from_dict({\"text\": simplified_messages})\n",
    "    num_batches = (len(all_messages) + batch_size - 1) // batch_size\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(all_messages), batch_size), total=num_batches, desc=\"è©±é¡Œåˆ†é¡æ‰¹æ¬¡è™•ç†\"):\n",
    "        batch = simplified_messages[i:i + batch_size]\n",
    "        batch_results = zero_shot_classifier(batch, topic_labels, multi_label=True, batch_size=len(batch))\n",
    "        results.extend(batch_results)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    labeled_groups = [[] for _ in conversation_groups]\n",
    "    msg_idx = 0\n",
    "    for group_idx, group in enumerate(tqdm(conversation_groups, desc=\"çµ„è£è©±é¡Œæ¨™ç±¤\")):\n",
    "        for speaker, message, time, emotion in group:\n",
    "            result = results[msg_idx]\n",
    "            predicted_topics = sorted([(label, score) for label, score in zip(result['labels'], result['scores'])], key=lambda x: x[1], reverse=True)[:3]\n",
    "            predicted_topics = [label for label, score in predicted_topics if score > 0.4]\n",
    "            # é—œéµè©å¾Œè™•ç†\n",
    "            message_lower = message.lower()\n",
    "            if 'å¥³æœ‹å‹' in message_lower or 'å–œæ­¡' in message_lower or 'åœ¨ä¸€èµ·' in message_lower:\n",
    "                predicted_topics.append('æ„Ÿæƒ…å…«å¦')\n",
    "            if 'ç¾¤çµ„' in message_lower or 'ç”·ç”Ÿ' in message_lower or 'å¥³ç”Ÿ' in message_lower or 'å®¤å‹' in message_lower:\n",
    "                predicted_topics.append('äººéš›')\n",
    "            if 'ç¬‘æ­»' in message_lower or 'å¥½ç¬‘' in message_lower or 'ç…§ç‰‡' in message_lower:\n",
    "                predicted_topics.append('å¨›æ¨‚')\n",
    "            if 'éŠè¦½è»Š' in message_lower or 'å‡ºå»ç©' in message_lower or 'è¨ˆç•«' in message_lower or 'è‚¡ç·´' in message_lower:\n",
    "                predicted_topics.append('ç¤¾åœ˜æ´»å‹•')\n",
    "            if 'å¡«' in message_lower or 'åé¡' in message_lower or 'å®¶æ•™' in message_lower:\n",
    "                predicted_topics.append('èª²æ¥­å£“åŠ›')\n",
    "            if 'ç¡äº†' in message_lower or 'æ´—æ¾¡' in message_lower or 'è¡Œæ' in message_lower or 'å‡ºé–€' in message_lower:\n",
    "                predicted_topics.append('ç”Ÿæ´»ç‘£äº‹')\n",
    "            if not predicted_topics and msg_idx > 0:\n",
    "                similarity = util.cos_sim(embeddings[msg_idx], embeddings[msg_idx-1]).item()\n",
    "                if similarity > 0.7:\n",
    "                    topic_str = labeled_groups[group_idx][-1][4] if labeled_groups[group_idx] else \"æ—¥å¸¸\"\n",
    "                else:\n",
    "                    topic_str = \"æ—¥å¸¸\"\n",
    "            else:\n",
    "                topic_str = \"+\".join(sorted(set(predicted_topics))) if predicted_topics else \"æ—¥å¸¸\"\n",
    "            labeled_groups[group_idx].append((speaker, message, time, emotion, topic_str))\n",
    "            msg_idx += 1\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(\"è©±é¡Œæ¨™ç±¤æ·»åŠ å®Œæˆ\")\n",
    "    return labeled_groups\n",
    "\n",
    "# å„ªåŒ–æ ¼å¼åŒ–è¨“ç·´æ•¸æ“š\n",
    "def format_training_data(conversation_groups: List[List[Tuple[str, str, int, str, str]]], embeddings: torch.Tensor, max_turns: int = 10, similarity_threshold: float = 0.8) -> List[dict]:\n",
    "    training_data = []\n",
    "    msg_idx = 0\n",
    "    \n",
    "    logger.info(\"é–‹å§‹æ ¼å¼åŒ–è¨“ç·´è³‡æ–™...\")\n",
    "    all_messages = [item[1] for group in conversation_groups for item in group]\n",
    "    logger.info(\"é å…ˆæ‰¹é‡è¨ˆç®—æ‰€æœ‰å°è©±è¡Œç‚º...\")\n",
    "    all_dialogue_acts = infer_dialogue_act(all_messages, embeddings=embeddings)\n",
    "    act_idx = 0\n",
    "    \n",
    "    for group in tqdm(conversation_groups, desc=\"æ ¼å¼åŒ–å°è©±çµ„\"):\n",
    "        messages = [item[1] for item in group]\n",
    "        dialogue_acts = all_dialogue_acts[act_idx:act_idx + len(messages)]\n",
    "        act_idx += len(messages)\n",
    "        \n",
    "        if len(messages) > 1:\n",
    "            curr_embeddings = embeddings[msg_idx:msg_idx + len(messages) - 1]\n",
    "            next_embeddings = embeddings[msg_idx + 1:msg_idx + len(messages)]\n",
    "            similarities = util.cos_sim(curr_embeddings, next_embeddings).diagonal().cpu().numpy()\n",
    "        else:\n",
    "            similarities = np.array([])\n",
    "        \n",
    "        for i in range(len(group) - 1):\n",
    "            context = group[max(0, i - max_turns + 1):i + 1]\n",
    "            next_msg = group[i + 1]\n",
    "            similarity = similarities[i] if i < len(similarities) else 0.0\n",
    "            \n",
    "            if similarity >= similarity_threshold and len(next_msg[1]) >= 3:\n",
    "                prompt = \"\\n\".join(f\"{s}: {m} [æƒ…ç·’: {e}, è¡Œç‚º: {dialogue_acts[j]}, è©±é¡Œ: {t}]\" \n",
    "                                 for j, (s, m, _, e, t) in enumerate(context))\n",
    "                response = f\"{next_msg[1]} [æƒ…ç·’: {next_msg[3]}, è¡Œç‚º: {dialogue_acts[i + 1]}, è©±é¡Œ: {next_msg[4]}]\"\n",
    "                training_data.append({\"prompt\": prompt, \"response\": response})\n",
    "        msg_idx += len(group)\n",
    "    \n",
    "    logger.info(f\"æ ¼å¼åŒ–å®Œæˆï¼Œç”Ÿæˆ {len(training_data)} ç­†è¨“ç·´è³‡æ–™\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return training_data\n",
    "\n",
    "# æ··åˆå¤šäººç¾¤èŠ\n",
    "def mix_conversations(chat_logs: List[str], friend_ids: List[str], time_threshold: int = 5, max_turns: int = 10, similarity_threshold: float = 0.8) -> List[dict]:\n",
    "    all_groups = []\n",
    "    for user_id, (chat_log, friend_id) in enumerate(tqdm(zip(chat_logs, friend_ids), desc=\"è§£æèŠå¤©æª”æ¡ˆ\", total=len(chat_logs))):\n",
    "        groups = parse_chat_log(chat_log, str(user_id), friend_id, time_threshold)\n",
    "        all_groups.extend(groups)\n",
    "    \n",
    "    all_messages = [msg for group in all_groups for _, msg, _ in group]\n",
    "    embeddings = precompute_embeddings(all_messages, num_gpus=2, chunk_size=5000)\n",
    "    \n",
    "    logger.info(\"é–‹å§‹æ·»åŠ æƒ…ç·’æ¨™ç±¤...\")\n",
    "    all_groups = add_emotion_labels(all_groups, embeddings)\n",
    "    logger.info(\"é–‹å§‹æ·»åŠ è©±é¡Œæ¨™ç±¤...\")\n",
    "    all_groups = add_topic_labels(all_groups, embeddings)\n",
    "    \n",
    "    logger.info(\"é–‹å§‹æ··åˆå°è©±...\")\n",
    "    all_conversations = [item for group in all_groups for item in group]\n",
    "    all_conversations.sort(key=lambda x: x[2])\n",
    "    \n",
    "    mixed_groups = []\n",
    "    current_group = []\n",
    "    prev_time = None\n",
    "    prev_topic = None\n",
    "    for item in tqdm(all_conversations, desc=\"æ··åˆå°è©±\"):\n",
    "        speaker, message, time, emotion, topic = item\n",
    "        if prev_time and (abs(time - prev_time) > time_threshold or topic.split(\"+\")[0] != prev_topic):\n",
    "            if current_group:\n",
    "                mixed_groups.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append((speaker, message, time, emotion, topic))\n",
    "        prev_time = time\n",
    "        prev_topic = topic.split(\"+\")[0]\n",
    "    if current_group:\n",
    "        mixed_groups.append(current_group)\n",
    "    \n",
    "    logger.info(f\"æ··åˆå®Œæˆï¼Œç”Ÿæˆ {len(mixed_groups)} çµ„å°è©±\")\n",
    "    return format_training_data(mixed_groups, embeddings, max_turns, similarity_threshold)\n",
    "\n",
    "# ä¸»å‡½æ•¸\n",
    "def process_chat_to_training_data(chat_files: List[str], friend_ids: List[str], output_file: str, time_threshold: int = 5, max_turns: int = 10, similarity_threshold: float = 0.8):\n",
    "    chat_logs = []\n",
    "    for chat_file in tqdm(chat_files, desc=\"è®€å–èŠå¤©æª”æ¡ˆ\"):\n",
    "        try:\n",
    "            with open(chat_file, 'r', encoding='utf-8') as f:\n",
    "                chat_logs.append(f.read())\n",
    "            logger.info(f\"æˆåŠŸè®€å–æª”æ¡ˆ: {chat_file}\")\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"æ‰¾ä¸åˆ°æª”æ¡ˆ: {chat_file}\")\n",
    "            return\n",
    "    \n",
    "    training_data = mix_conversations(chat_logs, friend_ids, time_threshold, max_turns, similarity_threshold)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(training_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    logger.info(f\"è¨“ç·´è³‡æ–™å·²å„²å­˜è‡³ {output_file}ï¼Œç¸½å…± {len(training_data)} ç­†è³‡æ–™\")\n",
    "\n",
    "# æ¸¬è©¦ç”¨\n",
    "if __name__ == \"__main__\":\n",
    "    chat_files = [\"data/claire.txt\"]\n",
    "    friend_ids = [\"2\"]\n",
    "    output_file = \"output/out.json\"\n",
    "    \n",
    "    process_chat_to_training_data(chat_files, friend_ids, output_file)\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            logger.info(\"å‰ 3 ç­†è¨“ç·´è³‡æ–™ç¯„ä¾‹ï¼š\")\n",
    "            for i, sample in enumerate(data[:3]):\n",
    "                logger.info(f\"æ¨£æœ¬ {i + 1}:\")\n",
    "                logger.info(f\"Prompt: {sample['prompt']}\")\n",
    "                logger.info(f\"Response: {sample['response']}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"ç„¡æ³•è®€å–è¼¸å‡ºæª”æ¡ˆï¼Œè«‹æª¢æŸ¥è™•ç†éç¨‹æ˜¯å¦æˆåŠŸã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fd32b27-52da-4848-9083-63e5212cc61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-28 16:24:10,861 - æ­£åœ¨åˆå§‹åŒ–æƒ…æ„Ÿåˆ†é¡å™¨ï¼ˆErlangshen-Roberta-110M-Sentimentï¼‰...\n",
      "Device set to use cuda:0\n",
      "2025-03-28 16:24:11,988 - æ­£åœ¨åˆå§‹åŒ–é›¶æ¨£æœ¬åˆ†é¡å™¨...\n",
      "Device set to use cuda:0\n",
      "è®€å–èŠå¤©æª”æ¡ˆ:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-28 16:24:13,838 - æˆåŠŸè®€å–æª”æ¡ˆ: data/claire.txt\n",
      "è®€å–èŠå¤©æª”æ¡ˆ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 170.59it/s]\n",
      "è§£æèŠå¤©æª”æ¡ˆ:   0%|          | 0/1 [00:00<?, ?it/s]2025-03-28 16:24:13,873 - é–‹å§‹è§£æèŠå¤©è¨˜éŒ„ (User: 0, Friend: 2)...\n",
      "\n",
      "è§£æèŠå¤©è¡Œ:   0%|          | 0/26848 [00:00<?, ?it/s]\u001b[A\n",
      "è§£æèŠå¤©è¡Œ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26848/26848 [00:00<00:00, 211445.88it/s]\u001b[A\n",
      "2025-03-28 16:24:14,006 - è§£æå®Œæˆï¼Œç”Ÿæˆ 1253 çµ„å°è©±\n",
      "è§£æèŠå¤©æª”æ¡ˆ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  6.05it/s]\n",
      "2025-03-28 16:24:14,011 - é–‹å§‹é è¨ˆç®— 19809 æ¢è¨Šæ¯çš„åµŒå…¥å‘é‡...\n",
      "2025-03-28 16:24:14,193 - Use pytorch device_name: cuda\n",
      "2025-03-28 16:24:14,194 - Load pretrained SentenceTransformer: intfloat/multilingual-e5-large\n",
      "è¨ˆç®—åµŒå…¥åˆ†ç‰‡:   0%|          | 0/4 [00:00<?, ?it/s]2025-03-28 16:24:17,592 - è™•ç†åˆ†ç‰‡ 1/4 on GPU 0\n",
      "è¨ˆç®—åµŒå…¥åˆ†ç‰‡:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:04<00:12,  4.16s/it]2025-03-28 16:24:21,750 - è™•ç†åˆ†ç‰‡ 2/4 on GPU 1\n",
      "è¨ˆç®—åµŒå…¥åˆ†ç‰‡:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:07<00:07,  3.82s/it]2025-03-28 16:24:25,332 - è™•ç†åˆ†ç‰‡ 3/4 on GPU 0\n",
      "è¨ˆç®—åµŒå…¥åˆ†ç‰‡:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.32s/it]2025-03-28 16:24:28,069 - è™•ç†åˆ†ç‰‡ 4/4 on GPU 1\n",
      "è¨ˆç®—åµŒå…¥åˆ†ç‰‡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.25s/it]\n",
      "2025-03-28 16:24:30,625 - åµŒå…¥è¨ˆç®—å®Œæˆ\n",
      "2025-03-28 16:24:30,628 - é–‹å§‹æ·»åŠ æƒ…ç·’æ¨™ç±¤...\n",
      "2025-03-28 16:24:31,394 - é–‹å§‹å…¨æ‰¹é‡æƒ…æ„Ÿåˆ†é¡...\n",
      "æƒ…æ„Ÿåˆ†é¡æ‰¹æ¬¡è™•ç†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [00:27<00:00,  2.88it/s]\n",
      "çµ„è£æƒ…æ„Ÿæ¨™ç±¤: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [00:00<00:00, 4277.22it/s]\n",
      "2025-03-28 16:24:58,797 - æƒ…æ„Ÿæ¨™ç±¤æ·»åŠ å®Œæˆ\n",
      "2025-03-28 16:24:58,801 - é–‹å§‹æ·»åŠ è©±é¡Œæ¨™ç±¤...\n",
      "2025-03-28 16:24:58,975 - é–‹å§‹å…¨æ‰¹é‡è©±é¡Œåˆ†é¡...\n",
      "è©±é¡Œåˆ†é¡æ‰¹æ¬¡è™•ç†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [12:15<00:00,  9.42s/it]\n",
      "çµ„è£è©±é¡Œæ¨™ç±¤: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1253/1253 [00:02<00:00, 550.88it/s]\n",
      "2025-03-28 16:37:16,385 - è©±é¡Œæ¨™ç±¤æ·»åŠ å®Œæˆ\n",
      "2025-03-28 16:37:16,397 - é–‹å§‹æ··åˆå°è©±...\n",
      "æ··åˆå°è©±: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19809/19809 [00:00<00:00, 909015.96it/s]\n",
      "2025-03-28 16:37:16,426 - æ··åˆå®Œæˆï¼Œç”Ÿæˆ 4999 çµ„å°è©±\n",
      "2025-03-28 16:37:16,428 - é–‹å§‹æ ¼å¼åŒ–è¨“ç·´è³‡æ–™...\n",
      "2025-03-28 16:37:16,430 - é å…ˆæ‰¹é‡è¨ˆç®—æ‰€æœ‰å°è©±è¡Œç‚º...\n",
      "2025-03-28 16:37:16,613 - æ­£åœ¨å° 19809 æ¢è¨Šæ¯é€²è¡Œå°è©±è¡Œç‚ºåˆ†é¡ï¼Œæ‰¹æ¬¡å¤§å°: 256\n",
      "è¡Œç‚ºåˆ†é¡æ‰¹æ¬¡è™•ç†: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 78/78 [14:57<00:00, 11.50s/it]\n",
      "å¾Œè™•ç†å°è©±è¡Œç‚º: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19809/19809 [00:01<00:00, 11292.24it/s]\n",
      "æ ¼å¼åŒ–å°è©±çµ„: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4999/4999 [00:00<00:00, 6905.99it/s]\n",
      "2025-03-28 16:52:16,208 - æ ¼å¼åŒ–å®Œæˆï¼Œç”Ÿæˆ 5130 ç­†è¨“ç·´è³‡æ–™\n",
      "2025-03-28 16:52:16,552 - è¨“ç·´è³‡æ–™å·²å„²å­˜è‡³ output/out.jsonï¼Œç¸½å…± 5130 ç­†è³‡æ–™\n",
      "2025-03-28 16:52:16,620 - å‰ 3 ç­†è¨“ç·´è³‡æ–™ç¯„ä¾‹ï¼š\n",
      "2025-03-28 16:52:16,621 - æ¨£æœ¬ 1:\n",
      "2025-03-28 16:52:16,621 - Prompt: User0: é‡é»æ˜¯\n",
      "2025-03-28 16:52:16,623 - Response: æˆ‘æ²’æœ‰ä¸æƒ³å–\n",
      "2025-03-28 16:52:16,624 - Metadata: {\"context\": [{\"speaker\": \"User0\", \"message\": \"é‡é»æ˜¯\", \"emotion\": \"ä¸­æ€§\", \"act\": \"å¼•å…¥\", \"topic\": \"æ—¥å¸¸\"}], \"response\": {\"speaker\": \"User0\", \"message\": \"æˆ‘æ²’æœ‰ä¸æƒ³å–\", \"emotion\": \"æ‡¶æ•£\", \"act\": \"æ‹’çµ•\", \"topic\": \"æ—¥å¸¸\"}}\n",
      "2025-03-28 16:52:16,625 - æ¨£æœ¬ 2:\n",
      "2025-03-28 16:52:16,626 - Prompt: User2: ä½†æ„Ÿè¦ºé‚£å€‹æ™‚å€™ä¹Ÿæ²’è¾¦æ³•å…¨åˆ°\n",
      "User2: æˆ‘åŸæœ¬é‚„å¾ˆè‡ªè±ª\n",
      "User0: æ‰€ä»¥è¦ºå¾—é€™æ¨£å‡ºå»å¾ˆå°·å°¬\n",
      "User0: å¦‚æœåˆä½µ\n",
      "2025-03-28 16:52:16,627 - Response: æœƒä¸æœƒ\n",
      "2025-03-28 16:52:16,628 - Metadata: {\"context\": [{\"speaker\": \"User2\", \"message\": \"ä½†æ„Ÿè¦ºé‚£å€‹æ™‚å€™ä¹Ÿæ²’è¾¦æ³•å…¨åˆ°\", \"emotion\": \"é–‹å¿ƒ\", \"act\": \"å•å€™\", \"topic\": \"æ—¥å¸¸\"}, {\"speaker\": \"User2\", \"message\": \"æˆ‘åŸæœ¬é‚„å¾ˆè‡ªè±ª\", \"emotion\": \"é–‹å¿ƒ\", \"act\": \"å¼•å…¥\", \"topic\": \"æ—¥å¸¸\"}, {\"speaker\": \"User0\", \"message\": \"æ‰€ä»¥è¦ºå¾—é€™æ¨£å‡ºå»å¾ˆå°·å°¬\", \"emotion\": \"é›£é\", \"act\": \"è£œå……\", \"topic\": \"æ—¥å¸¸\"}, {\"speaker\": \"User0\", \"message\": \"å¦‚æœåˆä½µ\", \"emotion\": \"ä¸­æ€§\", \"act\": \"é–’èŠ\", \"topic\": \"æ—¥å¸¸\"}], \"response\": {\"speaker\": \"User0\", \"message\": \"æœƒä¸æœƒ\", \"emotion\": \"ä¸­æ€§\", \"act\": \"æå•\", \"topic\": \"æ—¥å¸¸\"}}\n",
      "2025-03-28 16:52:16,630 - æ¨£æœ¬ 3:\n",
      "2025-03-28 16:52:16,631 - Prompt: User2: ä½†æ„Ÿè¦ºé‚£å€‹æ™‚å€™ä¹Ÿæ²’è¾¦æ³•å…¨åˆ°\n",
      "User2: æˆ‘åŸæœ¬é‚„å¾ˆè‡ªè±ª\n",
      "User0: æ‰€ä»¥è¦ºå¾—é€™æ¨£å‡ºå»å¾ˆå°·å°¬\n",
      "User0: å¦‚æœåˆä½µ\n",
      "User0: æœƒä¸æœƒ\n",
      "2025-03-28 16:52:16,632 - Response: å¥½ä¸€é»\n",
      "2025-03-28 16:52:16,633 - Metadata: {\"context\": [{\"speaker\": \"User2\", \"message\": \"ä½†æ„Ÿè¦ºé‚£å€‹æ™‚å€™ä¹Ÿæ²’è¾¦æ³•å…¨åˆ°\", \"emotion\": \"é–‹å¿ƒ\", \"act\": \"å•å€™\", \"topic\": \"æ—¥å¸¸\"}, {\"speaker\": \"User2\", \"message\": \"æˆ‘åŸæœ¬é‚„å¾ˆè‡ªè±ª\", \"emotion\": \"é–‹å¿ƒ\", \"act\": \"å¼•å…¥\", \"topic\": \"æ—¥å¸¸\"}, {\"speaker\": \"User0\", \"message\": \"æ‰€ä»¥è¦ºå¾—é€™æ¨£å‡ºå»å¾ˆå°·å°¬\", \"emotion\": \"é›£é\", \"act\": \"è£œå……\", \"topic\": \"æ—¥å¸¸\"}, {\"speaker\": \"User0\", \"message\": \"å¦‚æœåˆä½µ\", \"emotion\": \"ä¸­æ€§\", \"act\": \"é–’èŠ\", \"topic\": \"æ—¥å¸¸\"}, {\"speaker\": \"User0\", \"message\": \"æœƒä¸æœƒ\", \"emotion\": \"ä¸­æ€§\", \"act\": \"æå•\", \"topic\": \"æ—¥å¸¸\"}], \"response\": {\"speaker\": \"User0\", \"message\": \"å¥½ä¸€é»\", \"emotion\": \"ä¸­æ€§\", \"act\": \"è®šç¾\", \"topic\": \"æ—¥å¸¸\"}}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "from typing import List, Tuple\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import logging\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import emoji\n",
    "import os\n",
    "import opencc\n",
    "\n",
    "# è¨­ç½®è¨˜æ†¶é«”ç®¡ç†ç’°å¢ƒè®Šæ•¸\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# è¨­ç½®æ—¥èªŒ\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# åˆå§‹åŒ–å¤š GPU ç’°å¢ƒ\n",
    "device_ids = [0, 1]  # å…©å¼µ A10G\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# å®šç¾©åˆ†é¡æ¨™ç±¤\n",
    "act_labels = [\"æå•\", \"è£œå……\", \"å¼•å…¥\", \"è¡¨é”æƒ…æ„Ÿ\", \"è®šç¾\", \"é“æ­‰\", \"æ„Ÿè¬\", \"è«‹æ±‚\", \"æ‹’çµ•\", \"å»ºè­°\", \"å•å€™\", \"å‘Šåˆ¥\", \"ç¢ºèª\", \"è¡¨é”æ„åœ–\", \"åæ§½\", \"å‚¬ä¿ƒ\", \"å®‰æ…°\", \"æ¥æ¢—\"]\n",
    "topic_labels = [\"æ´»å‹•\", \"æ„Ÿæƒ…\", \"å­¸æ¥­\", \"å¨›æ¨‚\", \"äººéš›\", \"èª²æ¥­å£“åŠ›\", \"ç¤¾åœ˜æ´»å‹•\", \"æ„Ÿæƒ…å…«å¦\", \"ç”Ÿæ´»ç‘£äº‹\", \"å®¿èˆç”Ÿæ´»\", \"è€ƒè©¦\", \"æ—…è¡Œ\", \"ç¾é£Ÿ\", \"ç§‘æŠ€\"]\n",
    "\n",
    "# åˆå§‹åŒ–ç°¡ç¹è½‰æ›å™¨\n",
    "converter_to_simplified = opencc.OpenCC('t2s')\n",
    "converter_to_traditional = opencc.OpenCC('s2t')\n",
    "\n",
    "# å‹•æ…‹æ‰¹æ¬¡å¤§å°\n",
    "def get_dynamic_batch_size(messages: List[str], base_batch_size: int = 256) -> int:\n",
    "    gpu_memory = torch.cuda.memory_reserved(device) / 1024**3\n",
    "    max_memory = 24  # A10G 24GB\n",
    "    if gpu_memory > max_memory * 0.8:\n",
    "        return max(16, base_batch_size // 2)\n",
    "    return base_batch_size\n",
    "\n",
    "# å–®é€²ç¨‹åˆ†ç‰‡è¨ˆç®—åµŒå…¥\n",
    "def compute_chunk(chunk: List[str], gpu_id: int, batch_size: int, embedder: SentenceTransformer) -> torch.Tensor:\n",
    "    torch.cuda.set_device(gpu_id)\n",
    "    embedder.to(f\"cuda:{gpu_id}\")\n",
    "    try:\n",
    "        embeddings = embedder.encode(chunk, convert_to_tensor=True, batch_size=batch_size, show_progress_bar=False)\n",
    "        embeddings = embeddings.to(\"cuda:0\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return embeddings\n",
    "    except torch.cuda.OutOfMemoryError:\n",
    "        logger.warning(f\"GPU {gpu_id} è¨˜æ†¶é«”ä¸è¶³ï¼Œæ¸›åŠæ‰¹æ¬¡å¤§å°é‡è©¦...\")\n",
    "        embeddings = embedder.encode(chunk, convert_to_tensor=True, batch_size=batch_size // 2, show_progress_bar=False)\n",
    "        embeddings = embeddings.to(\"cuda:0\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        return embeddings\n",
    "\n",
    "def precompute_embeddings(messages: List[str], num_gpus: int = 2, chunk_size: int = 5000) -> torch.Tensor:\n",
    "    logger.info(f\"é–‹å§‹é è¨ˆç®— {len(messages)} æ¢è¨Šæ¯çš„åµŒå…¥å‘é‡...\")\n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in messages]\n",
    "    chunks = [simplified_messages[i:i + chunk_size] for i in range(0, len(messages), chunk_size)]\n",
    "    batch_size = get_dynamic_batch_size(messages)\n",
    "    \n",
    "    embedder = SentenceTransformer('intfloat/multilingual-e5-large')\n",
    "    results = []\n",
    "    \n",
    "    for i, chunk in enumerate(tqdm(chunks, desc=\"è¨ˆç®—åµŒå…¥åˆ†ç‰‡\")):\n",
    "        gpu_id = device_ids[i % num_gpus]\n",
    "        logger.info(f\"è™•ç†åˆ†ç‰‡ {i+1}/{len(chunks)} on GPU {gpu_id}\")\n",
    "        chunk_embeddings = compute_chunk(chunk, gpu_id, batch_size, embedder)\n",
    "        results.append(chunk_embeddings)\n",
    "    \n",
    "    embeddings = torch.cat(results, dim=0)\n",
    "    embeddings = embeddings.cpu()\n",
    "    logger.info(\"åµŒå…¥è¨ˆç®—å®Œæˆ\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return embeddings\n",
    "\n",
    "# æ¸…æ´—æ–‡æœ¬\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    patterns = r'(ä¸Šåˆ|ä¸‹åˆ)\\d{1,2}:\\d{2}\\s*|\\[ç…§ç‰‡\\]|\\[å½±ç‰‡\\]|\\[è²¼åœ–\\]|.*å·²æ”¶å›è¨Šæ¯.*|â˜.*|^\\*+$'\n",
    "    return re.sub(patterns, '', text).strip()\n",
    "\n",
    "# é—œéµè©åˆ†é¡ï¼ˆå„ªåŒ–è¡Œç‚ºæ¨™ç±¤ï¼‰\n",
    "def keyword_infer_dialogue_act(message: str, prev_act: str = None) -> str:\n",
    "    message_lower = message.lower()\n",
    "    keywords = {\n",
    "        \"æå•\": ['æœ‰æ²’æœ‰', 'ä»€éº¼', 'æœƒä¸æœƒ', 'ä½ è¦ºå¾—å‘¢', 'æ€éº¼', 'ç‚ºä»€éº¼', 'å—', 'å“ªå€‹', 'å¯ä¸å¯ä»¥', 'åˆ°äº†æ²’', 'å¹¾é»', 'åˆ°åº•', 'çŸ¥é“', 'å¤šå°‘'],\n",
    "        \"è£œå……\": ['å› ç‚º', 'æ‰€ä»¥', 'é›–ç„¶', 'ä¸é', 'ä½†æ˜¯', 'ç„¶å¾Œ', 'è€Œä¸”', 'çµæœ', 'é‚„æœ‰', 'å¹«', 'å°±', 'å‰›å‰›'],\n",
    "        \"å¼•å…¥\": ['é‡é»æ˜¯', 'ä¸»è¦æ˜¯', 'èªªåˆ°', 'è¬›åˆ°', 'æåˆ°', 'æˆ‘ä¹Ÿä¸çŸ¥é“'],\n",
    "        \"è¡¨é”æƒ…æ„Ÿ\": ['é›£é', 'å°·å°¬', 'è‡ªè±ª', 'éº»ç…©', 'ç·Šå¼µ', 'é–‹å¿ƒ', 'ç¬‘æ­»', 'å¸Œæœ›', 'æ€•', 'è¦ºå¾—', 'ç´¯äº†', 'å†·æ·¡', 'å¥½ç¬‘', 'é åŒ—', 'ä½ å¨˜', 'è¶…è¶•'],\n",
    "        \"è®šç¾\": ['æ¼‚äº®', 'å¸¥', 'å¾ˆæ£’', 'å¾ˆè®š', 'å¾ˆå¯æ„›', 'é©åˆ', 'å¥½æœƒ', 'å¥½ä¸€é»', 'é¡å€¼', 'å¾ˆé«˜'],\n",
    "        \"é“æ­‰\": ['å°ä¸èµ·', 'æŠ±æ­‰', 'ä¸å¥½æ„æ€', 'sorry'],\n",
    "        \"æ„Ÿè¬\": ['è¬è¬', 'æ„Ÿè¬', 'æ„Ÿæ¿€', 'å¤šè¬'],\n",
    "        \"è«‹æ±‚\": ['æ‹œè¨—', 'è«‹', 'å¹«æˆ‘', 'å¯ä»¥å—'],\n",
    "        \"æ‹’çµ•\": ['ä¸è¦', 'ä¸è¡Œ', 'ä¸å¯ä»¥', 'ä¸ok', 'æ²’è¾¦æ³•', 'ä¸æœƒ', 'æˆ‘æ²’'],\n",
    "        \"å»ºè­°\": ['å»ºè­°', 'ä¸å¦‚', 'è¦ä¸è¦', 'ä¸ç„¶', 'ç­‰ç­‰å†', 'ä½ å¯ä»¥'],\n",
    "        \"å•å€™\": ['å—¨', 'å˜¿', 'ä½ å¥½', 'æ—©å®‰'],\n",
    "        \"å‘Šåˆ¥\": ['æ°æ°', 'æ‹œæ‹œ', 'å†è¦‹', 'æ™šå®‰', 'è§£æ•£', 'å‡ºé–€', 'ä½ åˆªæ‰å•Š'],\n",
    "        \"ç¢ºèª\": ['çœŸçš„å‡çš„', 'ç¢ºå®š', 'ç¢ºèª', 'å¥½å•¦'],\n",
    "        \"è¡¨é”æ„åœ–\": ['æˆ‘è¦', 'æˆ‘æƒ³', 'æˆ‘æœƒ'],\n",
    "        \"åæ§½\": ['ç¬‘æ­»', 'æ€éº¼å¯èƒ½', 'è¶…é†œ', 'é è…°', 'ä½ å¿«é»'],\n",
    "        \"å‚¬ä¿ƒ\": ['å¿«é»', 'æœ€å¥½å¿«é»', 'è¶•ç·Š', 'ä¾›ä¸‰ä¿¡'],\n",
    "        \"å®‰æ…°\": ['ä¸æœƒå§', 'åˆæ²’å·®', 'æ²’äº‹çš„'],\n",
    "        \"æ¥æ¢—\": ['è¶…å¥½ç¬‘', 'å“ˆå“ˆå“ˆ', 'æˆ‘ä¹Ÿæ˜¯å§', 'å¾Œä¾†ä¹Ÿæ›']\n",
    "    }\n",
    "    for act, kws in keywords.items():\n",
    "        if any(kw in message_lower for kw in kws):\n",
    "            return act\n",
    "    # æ ¹æ“šä¸Šä¸‹æ–‡èª¿æ•´\n",
    "    if prev_act == \"æå•\" and 'å—' not in message_lower:\n",
    "        return \"å›ç­”\"\n",
    "    if 'è¦ºå¾—' in message_lower or 'å¾ˆ' in message_lower:\n",
    "        return \"è¡¨é”æƒ…æ„Ÿ\"\n",
    "    if 'çœŸçš„' in message_lower and 'å‡çš„' in message_lower:\n",
    "        return \"ç¢ºèª\"\n",
    "    return \"é–’èŠ\"  # é»˜èªè¡Œç‚º\n",
    "\n",
    "# åˆå§‹åŒ–åˆ†é¡å™¨\n",
    "logger.info(\"æ­£åœ¨åˆå§‹åŒ–æƒ…æ„Ÿåˆ†é¡å™¨ï¼ˆErlangshen-Roberta-110M-Sentimentï¼‰...\")\n",
    "sentiment_classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"IDEA-CCNL/Erlangshen-Roberta-110M-Sentiment\",\n",
    "    device=device_ids[0],\n",
    "    truncation=True,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "logger.info(\"æ­£åœ¨åˆå§‹åŒ–é›¶æ¨£æœ¬åˆ†é¡å™¨...\")\n",
    "zero_shot_classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    device=device_ids[0],\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# å°è©±è¡Œç‚ºåˆ†é¡\n",
    "def infer_dialogue_act(messages: List[str], prev_acts: List[str] = None, embeddings: torch.Tensor = None) -> List[str]:\n",
    "    if prev_acts is None:\n",
    "        prev_acts = [None] * len(messages)\n",
    "    \n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in messages]\n",
    "    dataset = Dataset.from_dict({\"text\": simplified_messages})\n",
    "    batch_size = get_dynamic_batch_size(messages)\n",
    "    logger.info(f\"æ­£åœ¨å° {len(messages)} æ¢è¨Šæ¯é€²è¡Œå°è©±è¡Œç‚ºåˆ†é¡ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}\")\n",
    "    \n",
    "    num_batches = (len(messages) + batch_size - 1) // batch_size\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(messages), batch_size), total=num_batches, desc=\"è¡Œç‚ºåˆ†é¡æ‰¹æ¬¡è™•ç†\"):\n",
    "        batch = simplified_messages[i:i + batch_size]\n",
    "        batch_results = zero_shot_classifier(batch, act_labels, multi_label=False, batch_size=len(batch))\n",
    "        results.extend(batch_results)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    predicted_acts = []\n",
    "    for i, (message, prev_act, result) in tqdm(enumerate(zip(messages, prev_acts, results)), total=len(messages), desc=\"å¾Œè™•ç†å°è©±è¡Œç‚º\"):\n",
    "        predicted_act = result['labels'][0]\n",
    "        confidence = result['scores'][0]\n",
    "        \n",
    "        if confidence < 0.1 and embeddings is not None and i > 0:\n",
    "            similarity = util.cos_sim(embeddings[i], embeddings[i-1]).item()\n",
    "            if similarity > 0.7 and prev_acts[i-1]:\n",
    "                predicted_act = prev_acts[i-1]\n",
    "            else:\n",
    "                predicted_act = keyword_infer_dialogue_act(message, prev_act)\n",
    "        elif prev_act == \"æå•\" and predicted_act not in [\"æå•\", \"è£œå……\", \"å›ç­”\"]:\n",
    "            predicted_act = \"å›ç­”\"\n",
    "        \n",
    "        if not predicted_act:\n",
    "            predicted_act = \"é–’èŠ\"\n",
    "        \n",
    "        predicted_acts.append(predicted_act)\n",
    "    return predicted_acts\n",
    "\n",
    "# è§£æèŠå¤©è¨˜éŒ„\n",
    "def parse_chat_log(chat_log: str, user_id: str, friend_id: str, time_threshold: int = 10) -> List[List[Tuple[str, str, int]]]:\n",
    "    lines = [line.strip() for line in chat_log.split('\\n') if line.strip() and \"å„²å­˜æ—¥æœŸ\" not in line and not re.match(r'\\d{4}/\\d{2}/\\d{2}', line)]\n",
    "    conversation_groups = []\n",
    "    current_group = []\n",
    "    prev_time = None\n",
    "    \n",
    "    logger.info(f\"é–‹å§‹è§£æèŠå¤©è¨˜éŒ„ (User: {user_id}, Friend: {friend_id})...\")\n",
    "    for line in tqdm(lines, desc=\"è§£æèŠå¤©è¡Œ\"):\n",
    "        match = re.match(r'(ä¸Šåˆ|ä¸‹åˆ)(\\d{1,2}):(\\d{2})\\s*([A-B])\\s+(.+)', line)\n",
    "        if match:\n",
    "            period, hour, minute, speaker, message = match.groups()\n",
    "            cleaned_message = clean_text(message)\n",
    "            if cleaned_message and len(cleaned_message) >= 3:\n",
    "                current_time = int(hour) * 60 + int(minute) + (12 * 60 if period == \"ä¸‹åˆ\" else 0)\n",
    "                if prev_time and abs(current_time - prev_time) > time_threshold:\n",
    "                    conversation_groups.append(current_group)\n",
    "                    current_group = []\n",
    "                role = f\"User{user_id}\" if speaker == \"B\" else f\"User{friend_id}\"\n",
    "                current_group.append((role, cleaned_message, current_time))\n",
    "                prev_time = current_time\n",
    "    \n",
    "    if current_group:\n",
    "        conversation_groups.append(current_group)\n",
    "    logger.info(f\"è§£æå®Œæˆï¼Œç”Ÿæˆ {len(conversation_groups)} çµ„å°è©±\")\n",
    "    return conversation_groups\n",
    "\n",
    "# æ·»åŠ æƒ…ç·’æ¨™ç±¤ï¼ˆå„ªåŒ–ï¼‰\n",
    "def add_emotion_labels(conversation_groups: List[List[Tuple[str, str, int]]], embeddings: torch.Tensor) -> List[List[Tuple[str, str, int, str]]]:\n",
    "    all_messages = [emoji.demojize(msg) for group in conversation_groups for _, msg, _ in group]\n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in all_messages]\n",
    "    batch_size = get_dynamic_batch_size(all_messages)\n",
    "    logger.info(\"é–‹å§‹å…¨æ‰¹é‡æƒ…æ„Ÿåˆ†é¡...\")\n",
    "    dataset = Dataset.from_dict({\"text\": simplified_messages})\n",
    "    num_batches = (len(all_messages) + batch_size - 1) // batch_size\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(all_messages), batch_size), total=num_batches, desc=\"æƒ…æ„Ÿåˆ†é¡æ‰¹æ¬¡è™•ç†\"):\n",
    "        batch = simplified_messages[i:i + batch_size]\n",
    "        batch_results = sentiment_classifier(batch, batch_size=len(batch), truncation=True, max_length=128)\n",
    "        results.extend(batch_results)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    labeled_groups = [[] for _ in conversation_groups]\n",
    "    emotion_map = {\n",
    "        'Positive': 'é–‹å¿ƒ', 'Negative': 'é›£é', 'Neutral': 'ä¸­æ€§',\n",
    "        ':smiling_face_with_heart-eyes:': 'å–œæ„›', ':pouting_face:': 'é›£é',\n",
    "        ':laughing:': 'é–‹å¿ƒ', ':sob:': 'é›£é', ':angry_face:': 'æ†¤æ€’',\n",
    "        ':face_with_tears_of_joy:': 'é–‹å¿ƒ', ':pleading_face:': 'å–œæ„›',\n",
    "        ':scream:': 'èˆˆå¥®', ':thinking_face:': 'ç„¦æ…®', ':weary_face:': 'ç„¡å¥ˆ',\n",
    "        ':sleeping_face:': 'æ‡¶æ•£'\n",
    "    }\n",
    "    msg_idx = 0\n",
    "    for group_idx, group in enumerate(tqdm(conversation_groups, desc=\"çµ„è£æƒ…æ„Ÿæ¨™ç±¤\")):\n",
    "        for speaker, message, time in group:\n",
    "            sentiment = results[msg_idx]['label']\n",
    "            confidence = results[msg_idx]['score']\n",
    "            emotion = emotion_map.get(sentiment, 'ä¸­æ€§')\n",
    "            # æª¢æŸ¥è¡¨æƒ…ç¬¦è™Ÿ\n",
    "            for emoji_key, emo in emotion_map.items():\n",
    "                if emoji_key in message and emo != 'ä¸­æ€§':\n",
    "                    emotion = emo\n",
    "                    break\n",
    "            # å¢å¼·ä¸Šä¸‹æ–‡æ¨ç†\n",
    "            if confidence < 0.6 and msg_idx > 0:\n",
    "                similarity = util.cos_sim(embeddings[msg_idx], embeddings[msg_idx-1]).item()\n",
    "                if similarity > 0.75 and labeled_groups[group_idx]:\n",
    "                    prev_emotion = labeled_groups[group_idx][-1][3]\n",
    "                    if prev_emotion in ['é–‹å¿ƒ', 'èˆˆå¥®', 'å–œæ„›'] and 'æ„›' in message_lower:\n",
    "                        emotion = 'å–œæ„›'\n",
    "                    elif prev_emotion in ['é›£é', 'ç„¦æ…®', 'ç„¡å¥ˆ'] and 'ä¸' in message_lower:\n",
    "                        emotion = 'é›£é'\n",
    "                    else:\n",
    "                        emotion = prev_emotion\n",
    "                elif 'å—' in message or 'ä»€éº¼' in message or 'æ€éº¼' in message:\n",
    "                    emotion = 'ç„¦æ…®'\n",
    "            # æ›´ç´°åŒ–çš„é—œéµè©å¾Œè™•ç†\n",
    "            message_lower = message.lower()\n",
    "            if 'ç¬‘æ­»' in message_lower or 'å¥½ç¬‘' in message_lower or 'å“ˆå“ˆ' in message_lower:\n",
    "                emotion = 'é–‹å¿ƒ' if 'è¶…' not in message_lower else 'èˆˆå¥®'\n",
    "            elif 'é›£é' in message_lower or 'å°·å°¬' in message_lower or 'æ€•' in message_lower:\n",
    "                emotion = 'é›£é'\n",
    "            elif 'å¾ˆé«˜' in message_lower and 'é¡å€¼' in message_lower:\n",
    "                emotion = 'å–œæ„›'\n",
    "            elif 'ä½ å¨˜' in message_lower or 'é åŒ—' in message_lower or 'è€–ä½ åª½' in message_lower:\n",
    "                emotion = 'é›£é' if 'ç¬‘' not in message_lower else 'é–‹å¿ƒ'\n",
    "            elif 'æ€éº¼è¾¦' in message_lower or 'åˆ°åº•' in message_lower or 'è¶…è¶•' in message_lower:\n",
    "                emotion = 'ç„¦æ…®'\n",
    "            elif 'æˆ‘è¦ç¡äº†' in message_lower or 'åˆæ²’å·®' in message_lower or 'æˆ‘æ²’' in message_lower:\n",
    "                emotion = 'æ‡¶æ•£'\n",
    "            elif 'å»å®œè˜­ç©' in message_lower or 'è¶…å¥½ç¬‘' in message_lower:\n",
    "                emotion = 'èˆˆå¥®' if 'è¶…' in message_lower else 'é–‹å¿ƒ'\n",
    "            elif 'æˆ‘æ²’æº–å‚™' in message_lower or 'æˆ‘ä»–åª½' in message_lower:\n",
    "                emotion = 'ç„¦æ…®' if 'æ€éº¼è¾¦' in message_lower else 'ç„¡å¥ˆ'\n",
    "            elif 'å¸Œæœ›' in message_lower or 'è¦ºå¾—' in message_lower and 'ä¸' not in message_lower:\n",
    "                emotion = 'é–‹å¿ƒ'\n",
    "            elif 'æ„›' in message_lower or 'å–œæ­¡' in message_lower:\n",
    "                emotion = 'å–œæ„›'\n",
    "            elif len(message) < 5 and 'æˆ‘' not in message_lower:\n",
    "                emotion = 'ä¸­æ€§'\n",
    "            labeled_groups[group_idx].append((speaker, message, time, emotion))\n",
    "            msg_idx += 1\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(\"æƒ…æ„Ÿæ¨™ç±¤æ·»åŠ å®Œæˆ\")\n",
    "    return labeled_groups\n",
    "\n",
    "# æ·»åŠ è©±é¡Œæ¨™ç±¤ï¼ˆå„ªåŒ–ï¼‰\n",
    "def add_topic_labels(conversation_groups: List[List[Tuple[str, str, int, str]]], embeddings: torch.Tensor) -> List[List[Tuple[str, str, int, str, str]]]:\n",
    "    all_messages = [msg for group in conversation_groups for _, msg, _, _ in group]\n",
    "    simplified_messages = [converter_to_simplified.convert(msg) for msg in all_messages]\n",
    "    batch_size = get_dynamic_batch_size(all_messages)\n",
    "    logger.info(\"é–‹å§‹å…¨æ‰¹é‡è©±é¡Œåˆ†é¡...\")\n",
    "    dataset = Dataset.from_dict({\"text\": simplified_messages})\n",
    "    num_batches = (len(all_messages) + batch_size - 1) // batch_size\n",
    "    results = []\n",
    "    for i in tqdm(range(0, len(all_messages), batch_size), total=num_batches, desc=\"è©±é¡Œåˆ†é¡æ‰¹æ¬¡è™•ç†\"):\n",
    "        batch = simplified_messages[i:i + batch_size]\n",
    "        batch_results = zero_shot_classifier(batch, topic_labels, multi_label=True, batch_size=len(batch))\n",
    "        results.extend(batch_results)\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    labeled_groups = [[] for _ in conversation_groups]\n",
    "    msg_idx = 0\n",
    "    for group_idx, group in enumerate(tqdm(conversation_groups, desc=\"çµ„è£è©±é¡Œæ¨™ç±¤\")):\n",
    "        for speaker, message, time, emotion in group:\n",
    "            result = results[msg_idx]\n",
    "            predicted_topics = sorted([(label, score) for label, score in zip(result['labels'], result['scores'])], key=lambda x: x[1], reverse=True)[:3]\n",
    "            predicted_topics = [label for label, score in predicted_topics if score > 0.3]\n",
    "            # é—œéµè©å¾Œè™•ç†\n",
    "            message_lower = message.lower()\n",
    "            if 'å¥³æœ‹å‹' in message_lower or 'å–œæ­¡' in message_lower or 'åœ¨ä¸€èµ·' in message_lower or 'å®³ç¾' in message_lower:\n",
    "                predicted_topics.append('æ„Ÿæƒ…å…«å¦')\n",
    "            if 'ç¾¤çµ„' in message_lower or 'ç”·ç”Ÿ' in message_lower or 'å¥³ç”Ÿ' in message_lower or 'å®¤å‹' in message_lower or 'åˆæ‹' in message_lower:\n",
    "                predicted_topics.append('äººéš›')\n",
    "            if 'ç¬‘æ­»' in message_lower or 'å¥½ç¬‘' in message_lower or 'ç…§ç‰‡' in message_lower:\n",
    "                predicted_topics.append('å¨›æ¨‚')\n",
    "            if 'éŠè¦½è»Š' in message_lower or 'å‡ºå»ç©' in message_lower or 'è¨ˆç•«' in message_lower or 'æºœå†°' in message_lower:\n",
    "                predicted_topics.append('ç¤¾åœ˜æ´»å‹•')\n",
    "            if 'å¡«' in message_lower or 'åé¡' in message_lower or 'å®¶æ•™' in message_lower or 'è¨è«–å®Œ' in message_lower:\n",
    "                predicted_topics.append('èª²æ¥­å£“åŠ›')\n",
    "            if 'ç¡äº†' in message_lower or 'æ´—æ¾¡' in message_lower or 'è¡Œæ' in message_lower or 'å‡ºé–€' in message_lower:\n",
    "                predicted_topics.append('ç”Ÿæ´»ç‘£äº‹')\n",
    "            if 'å®¿èˆ' in message_lower or 'åºŠ' in message_lower or 'åµ' in message_lower:\n",
    "                predicted_topics.append('å®¿èˆç”Ÿæ´»')\n",
    "            if 'è€ƒè©¦' in message_lower or 'æˆç¸¾' in message_lower or 'æœŸæœ«' in message_lower:\n",
    "                predicted_topics.append('è€ƒè©¦')\n",
    "            if 'å®œè˜­' in message_lower or 'å°ä¸­' in message_lower or 'æ—…è¡Œ' in message_lower:\n",
    "                predicted_topics.append('æ—…è¡Œ')\n",
    "            if 'åƒ' in message_lower or 'å¥½åƒ' in message_lower or 'é¤å»³' in message_lower:\n",
    "                predicted_topics.append('ç¾é£Ÿ')\n",
    "            if 'æ‰‹æ©Ÿ' in message_lower or 'é›»è…¦' in message_lower or 'app' in message_lower or 'å¸³è™Ÿ' in message_lower:\n",
    "                predicted_topics.append('ç§‘æŠ€')\n",
    "            # ä¸Šä¸‹æ–‡ä¸€è‡´æ€§æª¢æŸ¥\n",
    "            if msg_idx > 0:\n",
    "                similarity = util.cos_sim(embeddings[msg_idx], embeddings[msg_idx-1]).item()\n",
    "                if similarity > 0.75 and labeled_groups[group_idx]:\n",
    "                    prev_topic = labeled_groups[group_idx][-1][4].split(\"+\")[0]\n",
    "                    if prev_topic in predicted_topics or not predicted_topics:\n",
    "                        topic_str = prev_topic\n",
    "                    else:\n",
    "                        topic_str = \"+\".join(sorted(set(predicted_topics))) if predicted_topics else \"æ—¥å¸¸\"\n",
    "                else:\n",
    "                    topic_str = \"+\".join(sorted(set(predicted_topics))) if predicted_topics else \"æ—¥å¸¸\"\n",
    "            else:\n",
    "                topic_str = \"+\".join(sorted(set(predicted_topics))) if predicted_topics else \"æ—¥å¸¸\"\n",
    "            labeled_groups[group_idx].append((speaker, message, time, emotion, topic_str))\n",
    "            msg_idx += 1\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    logger.info(\"è©±é¡Œæ¨™ç±¤æ·»åŠ å®Œæˆ\")\n",
    "    return labeled_groups\n",
    "\n",
    "# å„ªåŒ–æ ¼å¼åŒ–è¨“ç·´æ•¸æ“š\n",
    "def format_training_data(conversation_groups: List[List[Tuple[str, str, int, str, str]]], embeddings: torch.Tensor, max_turns: int = 5, similarity_threshold: float = 0.85) -> List[dict]:\n",
    "    training_data = []\n",
    "    msg_idx = 0\n",
    "    \n",
    "    logger.info(\"é–‹å§‹æ ¼å¼åŒ–è¨“ç·´è³‡æ–™...\")\n",
    "    all_messages = [item[1] for group in conversation_groups for item in group]\n",
    "    logger.info(\"é å…ˆæ‰¹é‡è¨ˆç®—æ‰€æœ‰å°è©±è¡Œç‚º...\")\n",
    "    all_dialogue_acts = infer_dialogue_act(all_messages, embeddings=embeddings)\n",
    "    act_idx = 0\n",
    "    \n",
    "    for group in tqdm(conversation_groups, desc=\"æ ¼å¼åŒ–å°è©±çµ„\"):\n",
    "        messages = [item[1] for item in group]\n",
    "        dialogue_acts = all_dialogue_acts[act_idx:act_idx + len(messages)]\n",
    "        act_idx += len(messages)\n",
    "        \n",
    "        if len(messages) > 1:\n",
    "            curr_embeddings = embeddings[msg_idx:msg_idx + len(messages) - 1]\n",
    "            next_embeddings = embeddings[msg_idx + 1:msg_idx + len(messages)]\n",
    "            similarities = util.cos_sim(curr_embeddings, next_embeddings).diagonal().cpu().numpy()\n",
    "        else:\n",
    "            similarities = np.array([])\n",
    "        \n",
    "        for i in range(len(group) - 1):\n",
    "            context = group[max(0, i - max_turns + 1):i + 1]\n",
    "            next_msg = group[i + 1]\n",
    "            similarity = similarities[i] if i < len(similarities) else 0.0\n",
    "            \n",
    "            if similarity >= similarity_threshold and len(next_msg[1]) >= 3:\n",
    "                prompt_text = \"\\n\".join(f\"{s}: {m}\" for s, m, _, _, _ in context)\n",
    "                response_text = next_msg[1]\n",
    "                metadata = {\n",
    "                    \"context\": [{\"speaker\": s, \"message\": m, \"emotion\": e, \"act\": dialogue_acts[j], \"topic\": t} \n",
    "                                for j, (s, m, _, e, t) in enumerate(context)],\n",
    "                    \"response\": {\"speaker\": next_msg[0], \"message\": next_msg[1], \"emotion\": next_msg[3], \n",
    "                                \"act\": dialogue_acts[i + 1], \"topic\": next_msg[4]}\n",
    "                }\n",
    "                training_data.append({\"prompt\": prompt_text, \"response\": response_text, \"metadata\": metadata})\n",
    "        msg_idx += len(group)\n",
    "    \n",
    "    logger.info(f\"æ ¼å¼åŒ–å®Œæˆï¼Œç”Ÿæˆ {len(training_data)} ç­†è¨“ç·´è³‡æ–™\")\n",
    "    torch.cuda.empty_cache()\n",
    "    return training_data\n",
    "\n",
    "# æ··åˆå¤šäººç¾¤èŠ\n",
    "def mix_conversations(chat_logs: List[str], friend_ids: List[str], time_threshold: int = 10, max_turns: int = 5, similarity_threshold: float = 0.85) -> List[dict]:\n",
    "    all_groups = []\n",
    "    for user_id, (chat_log, friend_id) in enumerate(tqdm(zip(chat_logs, friend_ids), desc=\"è§£æèŠå¤©æª”æ¡ˆ\", total=len(chat_logs))):\n",
    "        groups = parse_chat_log(chat_log, str(user_id), friend_id, time_threshold)\n",
    "        all_groups.extend(groups)\n",
    "    \n",
    "    all_messages = [msg for group in all_groups for _, msg, _ in group]\n",
    "    embeddings = precompute_embeddings(all_messages, num_gpus=2, chunk_size=5000)\n",
    "    \n",
    "    logger.info(\"é–‹å§‹æ·»åŠ æƒ…ç·’æ¨™ç±¤...\")\n",
    "    all_groups = add_emotion_labels(all_groups, embeddings)\n",
    "    logger.info(\"é–‹å§‹æ·»åŠ è©±é¡Œæ¨™ç±¤...\")\n",
    "    all_groups = add_topic_labels(all_groups, embeddings)\n",
    "    \n",
    "    logger.info(\"é–‹å§‹æ··åˆå°è©±...\")\n",
    "    all_conversations = [item for group in all_groups for item in group]\n",
    "    all_conversations.sort(key=lambda x: x[2])\n",
    "    \n",
    "    mixed_groups = []\n",
    "    current_group = []\n",
    "    prev_time = None\n",
    "    prev_topic = None\n",
    "    for item in tqdm(all_conversations, desc=\"æ··åˆå°è©±\"):\n",
    "        speaker, message, time, emotion, topic = item\n",
    "        if prev_time and (abs(time - prev_time) > time_threshold or topic.split(\"+\")[0] != prev_topic):\n",
    "            if current_group:\n",
    "                mixed_groups.append(current_group)\n",
    "            current_group = []\n",
    "        current_group.append((speaker, message, time, emotion, topic))\n",
    "        prev_time = time\n",
    "        prev_topic = topic.split(\"+\")[0]\n",
    "    if current_group:\n",
    "        mixed_groups.append(current_group)\n",
    "    \n",
    "    logger.info(f\"æ··åˆå®Œæˆï¼Œç”Ÿæˆ {len(mixed_groups)} çµ„å°è©±\")\n",
    "    return format_training_data(mixed_groups, embeddings, max_turns, similarity_threshold)\n",
    "\n",
    "# ä¸»å‡½æ•¸\n",
    "def process_chat_to_training_data(chat_files: List[str], friend_ids: List[str], output_file: str, time_threshold: int = 10, max_turns: int = 5, similarity_threshold: float = 0.85):\n",
    "    chat_logs = []\n",
    "    for chat_file in tqdm(chat_files, desc=\"è®€å–èŠå¤©æª”æ¡ˆ\"):\n",
    "        try:\n",
    "            with open(chat_file, 'r', encoding='utf-8') as f:\n",
    "                chat_logs.append(f.read())\n",
    "            logger.info(f\"æˆåŠŸè®€å–æª”æ¡ˆ: {chat_file}\")\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"æ‰¾ä¸åˆ°æª”æ¡ˆ: {chat_file}\")\n",
    "            return\n",
    "    \n",
    "    training_data = mix_conversations(chat_logs, friend_ids, time_threshold, max_turns, similarity_threshold)\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(training_data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    logger.info(f\"è¨“ç·´è³‡æ–™å·²å„²å­˜è‡³ {output_file}ï¼Œç¸½å…± {len(training_data)} ç­†è³‡æ–™\")\n",
    "\n",
    "# æ¸¬è©¦ç”¨\n",
    "if __name__ == \"__main__\":\n",
    "    chat_files = [\"data/claire.txt\"]\n",
    "    friend_ids = [\"2\"]\n",
    "    output_file = \"output/out.json\"\n",
    "    \n",
    "    process_chat_to_training_data(chat_files, friend_ids, output_file)\n",
    "    \n",
    "    try:\n",
    "        with open(output_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            logger.info(\"å‰ 3 ç­†è¨“ç·´è³‡æ–™ç¯„ä¾‹ï¼š\")\n",
    "            for i, sample in enumerate(data[:3]):\n",
    "                logger.info(f\"æ¨£æœ¬ {i + 1}:\")\n",
    "                logger.info(f\"Prompt: {sample['prompt']}\")\n",
    "                logger.info(f\"Response: {sample['response']}\")\n",
    "                logger.info(f\"Metadata: {json.dumps(sample['metadata'], ensure_ascii=False)}\")\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"ç„¡æ³•è®€å–è¼¸å‡ºæª”æ¡ˆï¼Œè«‹æª¢æŸ¥è™•ç†éç¨‹æ˜¯å¦æˆåŠŸã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b19a5f3-dde4-49e2-9b71-e5db478d6921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
